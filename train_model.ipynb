{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from collections import defaultdict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make supervised learning dataset from csv files\n",
    "#TODO build dataloader\n",
    "solar_data = np.genfromtxt('data\\\\training_data\\\\train_solar.csv', delimiter=',', skip_header=1)\n",
    "wind_data = np.genfromtxt('data\\\\training_data\\\\train_wind.csv', delimiter=',',skip_header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17544, 11), (17544, 11))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solar_data.shape, wind_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_sliding_window(data, window_size, inputs_cols_indices, label_col_index):\n",
    "    \"\"\"\n",
    "    data: numpy array including data\n",
    "    window_size: size of window\n",
    "    inputs_cols_indices: col indices to include\n",
    "    \"\"\"\n",
    "\n",
    "    # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
    "    inputs = np.zeros((len(data) - window_size, window_size, len(inputs_cols_indices)))\n",
    "    labels = np.zeros(len(data) - window_size)\n",
    "\n",
    "    for i in range(window_size, len(data)):\n",
    "        inputs[i - window_size] = data[i - window_size : i, inputs_cols_indices]\n",
    "        labels[i - window_size] = data[i, label_col_index]\n",
    "    inputs = inputs.reshape(-1, window_size, len(inputs_cols_indices))\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    print(inputs.shape, labels.shape)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_sliding_window_24h(data, window_size, inputs_cols_indices, label_col_index, forecast_horizon=24):\n",
    "    \"\"\"\n",
    "    data: numpy array including data\n",
    "    window_size: size of window\n",
    "    inputs_cols_indices: col indices to include\n",
    "    label_col_index: index of the label column in data\n",
    "    forecast_horizon: number of time steps ahead to predict\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the number of instances based on the available data minus the window size and forecast horizon\n",
    "    num_instances = len(data) - window_size - forecast_horizon + 1\n",
    "\n",
    "    # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
    "    inputs = np.zeros((num_instances, window_size, len(inputs_cols_indices)))\n",
    "    labels = np.zeros(num_instances)\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        inputs[i] = data[i:i + window_size, inputs_cols_indices]\n",
    "        labels[i] = data[i + window_size + forecast_horizon - 1, label_col_index]  # Label is forecast_horizon steps ahead\n",
    "\n",
    "    print(inputs.shape, labels.shape)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17519, 25, 11) (17519, 1)\n",
      "(17519, 25, 11) (17519, 1)\n"
     ]
    }
   ],
   "source": [
    "solar_X, solar_y = move_sliding_window(solar_data, WINDOW_SIZE, range(11), 0)\n",
    "wind_X, wind_y = move_sliding_window(wind_data, WINDOW_SIZE, range(11), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.89679607]\n",
      " [-0.90517156]\n",
      " [-0.91285846]\n",
      " [-0.91991477]\n",
      " [-0.92639337]\n",
      " [-0.93234255]\n",
      " [-0.93780638]\n",
      " [-0.94115568]\n",
      " [-0.93104768]\n",
      " [-0.90421752]]\n",
      "[[-0.96459556]\n",
      " [-0.96776113]\n",
      " [-0.97061679]\n",
      " [-0.97319736]\n",
      " [-0.97553301]\n",
      " [-0.97765001]\n",
      " [-0.9795713 ]\n",
      " [-0.98025675]\n",
      " [-0.96751979]\n",
      " [-0.93005719]\n",
      " [-0.87482354]\n",
      " [-0.8187356 ]\n",
      " [-0.77384916]\n",
      " [-0.75200172]\n",
      " [-0.75831208]\n",
      " [-0.77751507]\n",
      " [-0.79586692]\n",
      " [-0.81266325]\n",
      " [-0.82804247]\n",
      " [-0.84212966]\n",
      " [-0.85503797]\n",
      " [-0.86686994]\n",
      " [-0.87771858]\n",
      " [-0.88766837]\n",
      " [-0.89679607]]\n"
     ]
    }
   ],
   "source": [
    "#sliding window for predicitng 24 ahread instead of 1\n",
    "solar_y_24 = np.roll(solar_y, -24) #shift 24 hours ahead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -1.         -1.         -1.         -0.39492153  0.21077271\n",
      "   0.          1.          1.          1.          1.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.39191435  0.21077271\n",
      "   1.          1.          1.          1.          1.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.39321545  0.21077271\n",
      "   2.          1.          1.          1.          1.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.39499721  0.21077271\n",
      "   3.          1.          1.          1.          1.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.39659511  0.21077271\n",
      "   4.          1.          1.          1.          1.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.39866141  0.21077271\n",
      "   5.          1.          1.          1.          1.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.40071957  0.21077271\n",
      "   6.          1.          1.          1.          1.        ]\n",
      " [-1.         -0.99999988 -0.99999955 -1.         -0.40273931  0.21077271\n",
      "   7.          1.          1.          1.          1.        ]\n",
      " [-0.99853643 -0.99972145 -0.99931288 -0.99889496 -0.40407701  0.213805\n",
      "   8.          1.          1.          1.          1.        ]\n",
      " [-0.98929614 -0.99782193 -0.9954876  -0.9923473  -0.40284649  0.23641761\n",
      "   9.          1.          1.          1.          1.        ]\n",
      " [-0.96981727 -0.99303483 -0.98698748 -0.97767768 -0.39909725  0.28255127\n",
      "  10.          1.          1.          1.          1.        ]\n",
      " [-0.94386172 -0.98444738 -0.97311783 -0.95497492 -0.39420079  0.33970077\n",
      "  11.          1.          1.          1.          1.        ]\n",
      " [-0.91972428 -0.97255544 -0.95463834 -0.92594911 -0.38939845  0.3928102\n",
      "  12.          1.          1.          1.          1.        ]\n",
      " [-0.90233704 -0.9584131  -0.93295393 -0.89033537 -0.38637763  0.42927765\n",
      "  13.          1.          1.          1.          1.        ]\n",
      " [-0.89661751 -0.94385861 -0.91027532 -0.85133526 -0.38565071  0.44081332\n",
      "  14.          1.          1.          1.          1.        ]\n",
      " [-0.90230602 -0.93031973 -0.88873051 -0.81249776 -0.38785276  0.42824321\n",
      "  15.          1.          1.          1.          1.        ]\n",
      " [-0.91235789 -0.91851156 -0.86984705 -0.77819059 -0.39309527  0.40597748\n",
      "  16.          1.          1.          1.          1.        ]\n",
      " [-0.92138079 -0.90821363 -0.85337878 -0.74827128 -0.39905     0.38588082\n",
      "  17.          1.          1.          1.          1.        ]\n",
      " [-0.92929317 -0.89918317 -0.83893739 -0.7220344  -0.4059161   0.36825763\n",
      "  18.          1.          1.          1.          1.        ]\n",
      " [-0.93626531 -0.89122579 -0.82621209 -0.69891526 -0.41373908  0.35272862\n",
      "  19.          1.          1.          1.          1.        ]\n",
      " [-0.94243516 -0.88418409 -0.81495111 -0.67845648 -0.42138304  0.33898655\n",
      "  20.          1.          1.          1.          1.        ]\n",
      " [-0.94791563 -0.87792917 -0.80494834 -0.66028361 -0.42959775  0.32677992\n",
      "  21.          1.          1.          1.          1.        ]\n",
      " [-0.95280007 -0.87235452 -0.79603345 -0.64408717 -0.43792583  0.31590085\n",
      "  22.          1.          1.          1.          1.        ]\n",
      " [-0.95716628 -0.86737132 -0.78806439 -0.62960911 -0.44595844  0.30617601\n",
      "  23.          1.          1.          1.          1.        ]\n",
      " [-0.96107966 -0.87948857 -0.8074273  -0.66344887 -0.45576854  0.06130266\n",
      "   0.          2.          2.          1.          2.        ]] \n",
      " [-0.96459556]\n"
     ]
    }
   ],
   "source": [
    "print(solar_X[0], '\\n', solar_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, inputs, output):\n",
    "        self.inputs = inputs\n",
    "        self.output = output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.output[idx]\n",
    "\n",
    "solar_dataset = SolarDataset(solar_X, solar_y)\n",
    "wind_dataset = SolarDataset(wind_X, wind_y)\n",
    "dataloader_solar = DataLoader(solar_dataset, batch_size=32, shuffle=False, drop_last=True) \n",
    "dataloader_wind = DataLoader(wind_dataset, batch_size=32, shuffle=False, drop_last=True)  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8735, 25, 11) (8735, 1)\n"
     ]
    }
   ],
   "source": [
    "solar_data_val = np.genfromtxt('data\\\\training_data\\\\val_solar.csv', delimiter=',', skip_header=1)\n",
    "solar_X_val, solar_y_val = move_sliding_window(solar_data_val, WINDOW_SIZE, range(11), 0)\n",
    "solar_dataset_val = SolarDataset(solar_X_val, solar_y_val)\n",
    "dataloader_solar_val = DataLoader(solar_dataset_val, batch_size=32, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731.0\n"
     ]
    }
   ],
   "source": [
    "alpha = 2\n",
    "N_h = solar_data.shape[0] / (alpha * (11 + 1))\n",
    "print(N_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GRU_model import GRUNet\n",
    "\n",
    "INPUT_SIZE = solar_X.shape[2]\n",
    "HIDDEN_SIZE = 16\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = solar_y.shape[1]\n",
    "DROP_PROB = .5\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1\n"
     ]
    }
   ],
   "source": [
    "print(INPUT_SIZE, OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUNet(\n",
       "  (gru): GRU(11, 16, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUNet(input_dim=INPUT_SIZE, hidden_dim=HIDDEN_SIZE, output_dim=OUTPUT_SIZE, n_layers=NUM_LAYERS, drop_prob=DROP_PROB).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, grad_clip_param, apply_gradient_clipping=False):\n",
    "    model.train()\n",
    "    losses_train = []\n",
    "    losses_val = [] \n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    eps = 1e-6\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        for i, (input, target) in enumerate(train_loader):\n",
    "            \n",
    "            \n",
    "            input, target  = input.to(device), target.to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            h = model.init_hidden(input.size(0)).to(device)\n",
    "            \n",
    "            output, hidden = model(input.float(), h)\n",
    "\n",
    "            loss = criterion(output, target.float())\n",
    "            # gradient clipping\n",
    "            loss.backward()\n",
    "            if apply_gradient_clipping:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_param)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(val_loader):\n",
    "                input, target  = input.to(device), target.to(device)\n",
    "                h = model.init_hidden(input.size(0)).to(device)\n",
    "                output, hidden = model(input.float(), h)\n",
    "                    \n",
    "                val_loss = criterion(output, target.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        #print epoch results\n",
    "        print(f'Epoch {epoch}, train loss: {np.mean(train_losses)}, val loss: {np.mean(val_losses)}')\n",
    "\n",
    "                \n",
    "        losses_train.append(np.mean(train_losses))\n",
    "        losses_val.append(np.mean(val_losses))\n",
    "\n",
    "    return model, losses_train, losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.08533979780500384, val loss: 0.2439885282799557\n",
      "Epoch 1, train loss: 0.07134200486723143, val loss: 0.09237526652171202\n",
      "Epoch 2, train loss: 0.05662171646939434, val loss: 0.08348656411673051\n",
      "Epoch 3, train loss: 0.051743480532261556, val loss: 0.07250889696489835\n",
      "Epoch 4, train loss: 0.046805613420642336, val loss: 0.19525775150053531\n",
      "Epoch 5, train loss: 0.05268103304632788, val loss: 0.21655869341296186\n",
      "Epoch 6, train loss: 0.04847570093007251, val loss: 0.2198562948334952\n",
      "Epoch 7, train loss: 0.04137041941059539, val loss: 0.1909649561470467\n",
      "Epoch 8, train loss: 0.04364348617335526, val loss: 0.18427647606564707\n",
      "Epoch 9, train loss: 0.03912882147439018, val loss: 0.16032147365823382\n",
      "Epoch 10, train loss: 0.035902572708064774, val loss: 0.19156538730642161\n",
      "Epoch 11, train loss: 0.03339533959970838, val loss: 0.1944984880685618\n",
      "Epoch 12, train loss: 0.03023722404699753, val loss: 0.06793199655315701\n",
      "Epoch 13, train loss: 0.024273088131541765, val loss: 0.06416346733075283\n",
      "Epoch 14, train loss: 0.023219295467719968, val loss: 0.049288694749360304\n",
      "Epoch 15, train loss: 0.022266020590160213, val loss: 0.06570114847401033\n",
      "Epoch 16, train loss: 0.02112565326489086, val loss: 0.06567208776087842\n",
      "Epoch 17, train loss: 0.020262258380912824, val loss: 0.06102210449042853\n",
      "Epoch 18, train loss: 0.01932936026069882, val loss: 0.06123814230865238\n",
      "Epoch 19, train loss: 0.01877987791691103, val loss: 0.05997255773699555\n",
      "Epoch 20, train loss: 0.017967722192715252, val loss: 0.05294900930226397\n",
      "Epoch 21, train loss: 0.016750560005200123, val loss: 0.03885361238430692\n",
      "Epoch 22, train loss: 0.01580444531585686, val loss: 0.041959537050227966\n",
      "Epoch 23, train loss: 0.015013493958990884, val loss: 0.04279584390229395\n",
      "Epoch 24, train loss: 0.01448915490957864, val loss: 0.0317537355936913\n",
      "Epoch 25, train loss: 0.013733919874966737, val loss: 0.029435831488680217\n",
      "Epoch 26, train loss: 0.013613589975106172, val loss: 0.03119754060432623\n",
      "Epoch 27, train loss: 0.013416919028522682, val loss: 0.04681080371750902\n",
      "Epoch 28, train loss: 0.012201356103301553, val loss: 0.04491760019171873\n",
      "Epoch 29, train loss: 0.01132926174377345, val loss: 0.04658974540678387\n",
      "Epoch 30, train loss: 0.010511577820726304, val loss: 0.03677912899151766\n",
      "Epoch 31, train loss: 0.009456430633802016, val loss: 0.026564587907213374\n",
      "Epoch 32, train loss: 0.008970783630369395, val loss: 0.02431369328234509\n",
      "Epoch 33, train loss: 0.0073615347796451335, val loss: 0.018877780357048752\n",
      "Epoch 34, train loss: 0.008777965594500633, val loss: 0.016432538539768702\n",
      "Epoch 35, train loss: 0.007172244888584775, val loss: 0.012558450188568231\n",
      "Epoch 36, train loss: 0.0066663084331524455, val loss: 0.012262692661833545\n",
      "Epoch 37, train loss: 0.006572153625377305, val loss: 0.006843931924937779\n",
      "Epoch 38, train loss: 0.007239183097457602, val loss: 0.00945267709058124\n",
      "Epoch 39, train loss: 0.005202191903357398, val loss: 0.009577289415199844\n",
      "Epoch 40, train loss: 0.004658154384645938, val loss: 0.00810047032096276\n",
      "Epoch 41, train loss: 0.003989360575593256, val loss: 0.004962088515479563\n",
      "Epoch 42, train loss: 0.003374809989251513, val loss: 0.004747988120738335\n",
      "Epoch 43, train loss: 0.0032030656179638795, val loss: 0.004011103226730484\n",
      "Epoch 44, train loss: 0.0028368849525444374, val loss: 0.0035164127161003895\n",
      "Epoch 45, train loss: 0.0026811368818408887, val loss: 0.0032476942610361113\n",
      "Epoch 46, train loss: 0.0024619200984783295, val loss: 0.0032621347591510344\n",
      "Epoch 47, train loss: 0.002367711281469343, val loss: 0.0044099552368446465\n",
      "Epoch 48, train loss: 0.002107196245823478, val loss: 0.004375192150398695\n",
      "Epoch 49, train loss: 0.002058283563166341, val loss: 0.003142394025952158\n",
      "Epoch 50, train loss: 0.002067398421008351, val loss: 0.0030596347897324664\n",
      "Epoch 51, train loss: 0.001945375964463513, val loss: 0.0037432581638152922\n",
      "Epoch 52, train loss: 0.0019101250766518133, val loss: 0.003637140331460721\n",
      "Epoch 53, train loss: 0.0018024268940656078, val loss: 0.003951839627568595\n",
      "Epoch 54, train loss: 0.001874825566814405, val loss: 0.0028115894788510666\n",
      "Epoch 55, train loss: 0.001981155885291121, val loss: 0.0025828798649907936\n",
      "Epoch 56, train loss: 0.0023134560819735054, val loss: 0.0032509958273294615\n",
      "Epoch 57, train loss: 0.002778074291402877, val loss: 0.005545804431730015\n",
      "Epoch 58, train loss: 0.0019185767908941553, val loss: 0.002994095659460072\n",
      "Epoch 59, train loss: 0.0016731468243581895, val loss: 0.002731448825162421\n",
      "Epoch 60, train loss: 0.0015015104371403946, val loss: 0.0030115231956562424\n",
      "Epoch 61, train loss: 0.0012805262763064672, val loss: 0.002787333004080745\n",
      "Epoch 62, train loss: 0.0012475545604560452, val loss: 0.00365082191541767\n",
      "Epoch 63, train loss: 0.0013212092401072683, val loss: 0.0033708991680963234\n",
      "Epoch 64, train loss: 0.0012869202699793173, val loss: 0.003524417338078766\n",
      "Epoch 65, train loss: 0.0012543444018580073, val loss: 0.0031238822084828023\n",
      "Epoch 66, train loss: 0.0015336318296618491, val loss: 0.004046817687220241\n",
      "Epoch 67, train loss: 0.0011606757628495833, val loss: 0.0020826111427488607\n",
      "Epoch 68, train loss: 0.0011746079159722412, val loss: 0.003987650538714233\n",
      "Epoch 69, train loss: 0.0011521089655039424, val loss: 0.0031986362193278826\n",
      "Epoch 70, train loss: 0.0011675048619601855, val loss: 0.003408032292768707\n",
      "Epoch 71, train loss: 0.0010925892850053066, val loss: 0.0037539771039305113\n",
      "Epoch 72, train loss: 0.0010756493024687112, val loss: 0.0037804252360194154\n",
      "Epoch 73, train loss: 0.001120450114320548, val loss: 0.003218629240644291\n",
      "Epoch 74, train loss: 0.0011218352225160913, val loss: 0.003935736859014416\n",
      "Epoch 75, train loss: 0.0011036409036143256, val loss: 0.003714639535300033\n",
      "Epoch 76, train loss: 0.0010599535520073478, val loss: 0.003923044621093569\n",
      "Epoch 77, train loss: 0.0010315522420276688, val loss: 0.003300930053335512\n",
      "Epoch 78, train loss: 0.001107970169085947, val loss: 0.00410541717444764\n",
      "Epoch 79, train loss: 0.0011464235869468695, val loss: 0.003992080364161452\n",
      "Epoch 80, train loss: 0.0011554407767765905, val loss: 0.003663505868992027\n",
      "Epoch 81, train loss: 0.001131416201015055, val loss: 0.0038898962383534714\n",
      "Epoch 82, train loss: 0.0011277966002859298, val loss: 0.002903434114641106\n",
      "Epoch 83, train loss: 0.0011590390807844297, val loss: 0.003573620966027888\n",
      "Epoch 84, train loss: 0.0010989609485913185, val loss: 0.003552473701293981\n",
      "Epoch 85, train loss: 0.0011354424003058882, val loss: 0.0035350340896965304\n",
      "Epoch 86, train loss: 0.0010564343107796316, val loss: 0.0035492932864626173\n",
      "Epoch 87, train loss: 0.0010676338898674674, val loss: 0.0036173846795945256\n",
      "Epoch 88, train loss: 0.0009387959788273126, val loss: 0.0022288044562682285\n",
      "Epoch 89, train loss: 0.0009865705576443098, val loss: 0.005073098311065812\n",
      "Epoch 90, train loss: 0.0010350859617226811, val loss: 0.0046494359994807485\n",
      "Epoch 91, train loss: 0.0009088659455194827, val loss: 0.0027336628603841424\n",
      "Epoch 92, train loss: 0.0008537023220980843, val loss: 0.0035818356531950378\n",
      "Epoch 93, train loss: 0.0010049682758414182, val loss: 0.0034421930474897687\n",
      "Epoch 94, train loss: 0.0009032918799757298, val loss: 0.002882163691591653\n",
      "Epoch 95, train loss: 0.000833636837355357, val loss: 0.002191047450865239\n",
      "Epoch 96, train loss: 0.0009024334347626395, val loss: 0.0034413751787483573\n",
      "Epoch 97, train loss: 0.000894621226814896, val loss: 0.002711387199804663\n",
      "Epoch 98, train loss: 0.0009290434367195159, val loss: 0.003160540460559038\n",
      "Epoch 99, train loss: 0.0009213441843624521, val loss: 0.002409612079057243\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "grad_clip_param = 2\n",
    "\n",
    "model, losses_train, losses_val = train_model(model, dataloader_solar, dataloader_solar_val,  criterion, optimizer, num_epochs=N_EPOCHS, apply_gradient_clipping=True, grad_clip_param=grad_clip_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_15_window_size.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABZ/klEQVR4nO3deXxU1f3/8dedmew7hCQEAgkQQET2RXCh1bTggkurX7W0IvVrq5VWy1dbaSu0WgtWarFq5av9WfVbt9qqtS64RLEuLLKJArLvkIQA2feZ+/vjZiYJZJmZTDIz4f18POZxJzNn7py5wczHcz7ncwzTNE1EREREQpgt2B0QERER6YgCFhEREQl5ClhEREQk5ClgERERkZCngEVERERCngIWERERCXkKWERERCTkKWARERGRkOcIdgcCweVycfjwYRISEjAMI9jdERERES+Ypkl5eTmZmZnYbO2PofSIgOXw4cNkZWUFuxsiIiLihwMHDtC/f/922/SIgCUhIQGwPnBiYmKQeyMiIiLeKCsrIysry/M93p4eEbC4p4ESExMVsIiIiIQZb9I5lHQrIiIiIc+vgOXRRx8lOzub6OhoJk+ezJo1a9ps+8QTT3DeeeeRkpJCSkoKeXl5p7S/4YYbMAyjxW3GjBn+dE1ERER6IJ8DlhdffJF58+axcOFC1q9fz+jRo5k+fTpFRUWttl+xYgXXXXcdH3zwAStXriQrK4tvfvObHDp0qEW7GTNmcOTIEc/t+eef9+8TiYiISI9jmKZp+vKCyZMnM3HiRB555BHAWlKclZXFj3/8Y+66664OX+90OklJSeGRRx7h+uuvB6wRlpKSEl599VXfPwFW0k5SUhKlpaXKYRERkYAyTZOGhgacTmewuxKW7HY7Doej1TwVX76/fUq6raurY926dcyfP9/zmM1mIy8vj5UrV3p1jqqqKurr6+nVq1eLx1esWEFaWhopKSlccMEF/Pa3v6V3796tnqO2tpba2lrPz2VlZb58DBEREa/U1dVx5MgRqqqqgt2VsBYbG0vfvn2JjIz0+xw+BSzFxcU4nU7S09NbPJ6ens5XX33l1Tl+/vOfk5mZSV5enuexGTNm8K1vfYucnBx27drFL37xCy666CJWrlyJ3W4/5RyLFi3iN7/5jS9dFxER8YnL5WLPnj3Y7XYyMzOJjIxUcVIfmaZJXV0dR48eZc+ePeTm5nZYIK4t3bqsefHixbzwwgusWLGC6Ohoz+PXXnut5/5ZZ53FqFGjGDx4MCtWrODCCy885Tzz589n3rx5np/d67hFREQCpa6uzpP2EBsbG+zuhK2YmBgiIiLYt28fdXV1Lb7/feFTmJOamordbqewsLDF44WFhWRkZLT72iVLlrB48WLeeecdRo0a1W7bQYMGkZqays6dO1t9PioqylNzRbVXRESkK/k7IiBNAnENfTpDZGQk48ePJz8/3/OYy+UiPz+fKVOmtPm63//+99x7770sX76cCRMmdPg+Bw8e5NixY/Tt29eX7omIiEgP5XPIM2/ePJ544gmefvpptm7dyi233EJlZSVz5swB4Prrr2+RlHv//fdz99138+STT5KdnU1BQQEFBQVUVFQAUFFRwZ133smqVavYu3cv+fn5XH755QwZMoTp06cH6GOKiIhIOPM5YLnmmmtYsmQJCxYsYMyYMWzcuJHly5d7EnH379/PkSNHPO0fe+wx6urquOqqq+jbt6/ntmTJEsBa7rRp0yYuu+wyhg4dyo033sj48eP56KOPiIqKCtDHFBEREX9kZ2ezdOnSYHfD9zosoUh1WEREJNBqamrYs2cPOTk5fieKBsvXvvY1xowZE5BA4+jRo8TFxXUq8bita9lldVhOOw118N6vwVkH0+8Dh0Z8REQk/JmmidPpxOHoOAzo06dPN/SoY0p9bpcJqx6Fz56Ahppgd0ZERILMNE2q6hq6/ebLZMgNN9zAhx9+yEMPPeTZn++pp57CMAzeeustxo8fT1RUFB9//DG7du3i8ssvJz09nfj4eCZOnMh7773X4nwnTwkZhsFf/vIXrrzySmJjY8nNzeW1114L1CVuk0ZY2mOLaLrvbAheP0REJCRU1zsZseDtbn/fLfdMJzbSu6/shx56iO3btzNy5EjuueceADZv3gzAXXfdxZIlSxg0aBApKSkcOHCAiy++mPvuu4+oqCieeeYZZs6cybZt2xgwYECb7/Gb3/yG3//+9zzwwAM8/PDDzJo1i3379p1SxT6QNMLSHpsNjMZKu8664PZFRETEC0lJSURGRhIbG0tGRgYZGRmeqvH33HMP3/jGNxg8eDC9evVi9OjR/PCHP2TkyJHk5uZy7733Mnjw4A5HTG644Qauu+46hgwZwu9+9zsqKipYs2ZNl34ujbB0xB4JDdXgqg92T0REJMhiIuxsuaf7S27ERJy6TY0/Tq6FVlFRwa9//WveeOMNjhw5QkNDA9XV1ezfv7/d8zQvABsXF0diYiJFRUUB6WNbFLB0xB5hBSxOBSwiIqc7wzC8npoJRXFxcS1+vuOOO3j33XdZsmQJQ4YMISYmhquuuoq6uvZnFSIiIlr8bBgGLpcr4P1tLnyvenexN/5SNCUkIiJhIjIyEqfT2WG7Tz75hBtuuIErr7wSsEZc9u7d28W9849yWDriTrzVCIuIiISJ7OxsVq9ezd69eykuLm5z9CM3N5eXX36ZjRs38vnnn/Od73yny0dK/KWApSP2SOuogEVERMLEHXfcgd1uZ8SIEfTp06fNnJQHH3yQlJQUpk6dysyZM5k+fTrjxo3r5t56R5VuO/KncXB8F3z/bRhwdmDPLSIiISucK92GmkBUutUIS0eUwyIiIhJ0Clg6YlcOi4iISLApYOmIkm5FRESCTgFLRzxJt5oSEhERCRYFLB1xTwmp0q2IiEjQKGDpiHJYREREgk4BS0dUh0VERCToFLB0xNa4e4FyWERERIJGAUtH3CMsrobg9kNERKSbZGdns3Tp0mB3owUFLB1R4TgREZGgU8DSEQUsIiIiQaeApSOewnGaEhIRkdD3+OOPk5mZecquy5dffjnf//732bVrF5dffjnp6enEx8czceJE3nvvvSD11nsKWDqiwnEiIuJmmlBX2f03H/Ypvvrqqzl27BgffPCB57Hjx4+zfPlyZs2aRUVFBRdffDH5+fls2LCBGTNmMHPmzDZ3dA4VjmB3IOSpcJyIiLjVV8HvMrv/fX9xGCLjvGqakpLCRRddxHPPPceFF14IwD/+8Q9SU1P5+te/js1mY/To0Z729957L6+88gqvvfYac+fO7ZLuB4JGWDqiwnEiIhJmZs2axT//+U9qa2sBePbZZ7n22mux2WxUVFRwxx13cMYZZ5CcnEx8fDxbt27VCEvYU+E4ERFxi4i1RjuC8b4+mDlzJqZp8sYbbzBx4kQ++ugj/vjHPwJwxx138O6777JkyRKGDBlCTEwMV111FXV1oZ36oIClI56k29D+RYqISDcwDK+nZoIpOjqab33rWzz77LPs3LmTYcOGMW7cOAA++eQTbrjhBq688koAKioq2Lt3bxB76x0FLB3x5LBolZCIiISPWbNmcemll7J582a++93veh7Pzc3l5ZdfZubMmRiGwd13333KiqJQpByWjqgOi4iIhKELLriAXr16sW3bNr7zne94Hn/wwQdJSUlh6tSpzJw5k+nTp3tGX0KZRlg6omXNIiIShmw2G4cPn5pvk52dzfvvv9/isVtvvbXFz6E4RaQRlo54Nj/UlJCIiEiwKGDpiEZYREREgk4BS0c8uzVrWbOIiEiwKGDpiN09JdQNAYvLBaUHu/59REREwowClo50V+G42gp4+lL445mw5z9d+14iIiJhRgFLR7qjcFxtBTx7Nez7xPr54Nquey8REfGJ6cPGg9K6QFxDBSwd6eq9hNzByv5Pmx4rO9Q17yUiIl6LiLD+/ldVVQW5J+HPfQ3d19QfqsPSka7crbl5sBKVBGdeDuufgVIFLCIiwWa320lOTqaoqAiA2NhYDMMIcq/Ci2maVFVVUVRURHJyMna73e9zKWDpSFcta3bWtwxWvvcKVB1rDFiUeCsiEgoyMjIAPEGL+Cc5OdlzLf2lgKUjnimhABeO2/MfK1iJTLCClf7joXCz9VyZAhYRkVBgGAZ9+/YlLS2N+nqVt/BHREREp0ZW3BSwdKSrkm6PbrOOg79uBSsASf2tY/UJqKuCSN+2ExcRka5ht9sD8qUr/lPSbUe6qnBc8XbrmDq06bHoJGvEBZR4KyIi0owClo50VeG44h3WMTW35eNJ/axj6YHAvp+IiEgYU8DSka5Kuj3WRsCS6A5YNMIiIiLipoClI7ZmdVgCVTyophQqCq37vdsYYdGUkIiIiIcClo64VwlhgssZmHMW77SO8RkQndjyuaQs66ilzSIiIh4KWDrinhKCwCXeehJuc099LlEjLCIiIidTwNIRe7MywoHKY2kvYPEk3WqERURExE0BS0dszQOWABWP8yTcDj31ucTGWiylhwKXMyMiIhLmFLB0xGYDo7FYUMBGWNpYIQSQmGkd6yuhpiQw7yciIhLmFLB4I5BLm50NcHy3df/kFUJgVbeN7W3d19JmERERQAGLdzw7NgdgSqhknxX4OKKbVgSdTIm3IiIiLShg8YY9gPsJuaeDeg+xppta495TSNVuRUREAAUs3vFMCQVgWXNbFW6bU7VbERGRFhSweKN5tdvOam3Tw5Op2q2IiEgLCli84clhCUTA4p4SameExVPtVgGLiIgIKGDxTlfksHg1JaQcFhEREVDA4h17gKaEqo5DVbF1v/eQttt5poQOg8vVufcUERHpAfwKWB599FGys7OJjo5m8uTJrFmzps22TzzxBOeddx4pKSmkpKSQl5d3SnvTNFmwYAF9+/YlJiaGvLw8duzY4U/XukagcliONW56mNgPouLbbpfQFzCsKajKo517TxERkR7A54DlxRdfZN68eSxcuJD169czevRopk+fTlFRUavtV6xYwXXXXccHH3zAypUrycrK4pvf/CaHDjXlZ/z+97/nT3/6E8uWLWP16tXExcUxffp0ampq/P9kgRSownHt7SHU4v0iICHDul+mPYVERER8DlgefPBBbrrpJubMmcOIESNYtmwZsbGxPPnkk622f/bZZ/nRj37EmDFjGD58OH/5y19wuVzk5+cD1ujK0qVL+dWvfsXll1/OqFGjeOaZZzh8+DCvvvpqpz5cwAQq6dYdsLSXcOuW1GxPIRERkdOcTwFLXV0d69atIy8vr+kENht5eXmsXLnSq3NUVVVRX19Pr169ANizZw8FBQUtzpmUlMTkyZPbPGdtbS1lZWUtbl0qUDksxY1TQu0taXZL1K7NIiIibj4FLMXFxTidTtLT01s8np6eTkFBgVfn+PnPf05mZqYnQHG/zpdzLlq0iKSkJM8tK6uNEveBEqjCcZ4poXYSbt3cIyyqxSIiItK9q4QWL17MCy+8wCuvvEJ0dLTf55k/fz6lpaWe24EDXbz81+awjp3JYXHWw4k91n2NsIiIiPjE4Uvj1NRU7HY7hYWFLR4vLCwkIyOj3dcuWbKExYsX89577zFq1CjP4+7XFRYW0rdv3xbnHDNmTKvnioqKIioqypeud457hKUzmx+e2Gu9PiIOEjI7bq9qtyIiIh4+jbBERkYyfvx4T8Is4EmgnTJlSpuv+/3vf8+9997L8uXLmTBhQovncnJyyMjIaHHOsrIyVq9e3e45u1UgCsd5Em4Ht73pYXNKuhUREfHwaYQFYN68ecyePZsJEyYwadIkli5dSmVlJXPmzAHg+uuvp1+/fixatAiA+++/nwULFvDcc8+RnZ3tyUuJj48nPj4ewzC4/fbb+e1vf0tubi45OTncfffdZGZmcsUVVwTuk3ZGQAIWd4VbL6aDABIbA5byI9Z0krsPIiIipyGfA5ZrrrmGo0ePsmDBAgoKChgzZgzLly/3JM3u378fW7MRhMcee4y6ujquuuqqFudZuHAhv/71rwH42c9+RmVlJT/4wQ8oKSnh3HPPZfny5Z3KcwkoT+G4TkwJeVOSv7m4Ptb7uuqtoCV5gP/vLSIiEuZ8DlgA5s6dy9y5c1t9bsWKFS1+3rt3b4fnMwyDe+65h3vuucef7nS9QBSOc1e5ba8kf3M2GyRmQsk+a1qoswFLQx00VEN0UufOIyIiEgTaS8gbgSgcV1NqHeNSvX9NoJY2F30FD4+HP45s6oeIiEgY8WuE5bQTiMJxztrGc/mwusmTeNuJZdv7PoXnr20KVEoPapRFRETCjgIWbwSicFxD43SSI9L713hqsfg5wrLlX/DPm5qCJYCG2rbbi4iIhChNCXnDFoBVQu7X+jTC0olaLKsfh7/PtoKVYZc0rTrq7AaOIiIiQaCAxRuBnBJy+BCwuIMMX6vdfvUmvHUnYMKE78M1/wdR8Y39UMAiIiLhRwGLNwKRdOueErL7MCXkzmE5vgfqq71/3b5PrONZV8MlD4LN3vS+DQpYREQk/Chg8UYgljV7km59CFj6DIekAVBXDuue9v51ZYetY98xYBjWfffIjlM5LCIiEn4UsHjD3snCcc4GMF3WfV+mhOwOOG+edf+TpVBf493ryo9Yx8SmvZkCEnSJiIgEiQIWb3Q26bb5qIYvIywAY75j5bKUH4EN/+fda9wjLM03WdSUkIiIhDEFLN7w7NbsZw5L86XEvoywuNufe7t1/+M/drws2TSh3NqvqcUIi6aEREQkjClg8Ya9sVyNv6uEPCMzBtj8KH0z9nuQ0Nda3rzx2fbbVh1vCkoSmk8JBWBptoiISJAoYPFGZ/M/3K9zRDUlwfoiIhrOud26/9GD7U/rlDdOB8X2bjma467/oikhEREJQwpYvGHrZB2WBj+Kxp1s/GyIS7PK9G96oe12ZY0Jt83zV0BTQiIiEtYUsHijs4XjPEXjfEy4bS4iBs65zbr/nyVt98U9wtI8fwUCU/xOREQkSBSweCNQSbe+rhA62YQ5EJsKJftg8yutt/GMsJwcsLinhDTCIiIi4UcBizc6m7Dq9KPKbWsi42D0tdb9A2tab+MZYTl5Ssidh6OARUREwo8CFm90tnBcgx/7CLUlNdc6ntjT+vNtjrAEYMdpERGRIFHA4o1OF44L0AgLQK9B1vF4GwGLp8rtSSMsmhISEZEwpoDFG4HKYQnECEtKjnUs2d/6iI+nyu1JIywOleYXEZHwpYDFG50uHOdOug1AwJKYaQVQrnooO9jyufoaqD7e1K457SUkIiJhTAGLNzpdOK4x0OnMsmY3mx2SB1r3T54Wck8H2aMgJqXlc569hDQlJCIi4UcBizeaF44zTd9f3xDAERaAXo3TQicn3jbfpfnkirqewnEaYRERkfCjgMUb7lVCmOBy+v56T9JtRPvtvNVW4m1ruzS7aUpIRETCmAIWbzRf3eNP4m0gk26hKfG2vRGWk3mmhBSwiIhI+FHA4o3mIyP+jFAEMukWmqaEThlhaaMGC2gvIRERCWsKWLxhax6w+FE8zj2qEYikW2gaYTm+p2VOTVtVbkFTQiIiEtYUsHjDZgPDbt0PhRGWlIGAAfWVUHm06fH2Rlg0JSQiImFMAYu3OjNCEegRFkcUJPW37jefFmpvhEVTQiIiEsYUsHjLncfi8mNKyLNKKEAjLAAp2dbRnXhrmlBeYN1vdYSl2dJsERGRMKOAxVud2bHZPaoRqBEWaJZ4u9s6Vh1r6lurAYv2EhIRkfClgMVbndntuCGAmx+6pZy0UshdgyU2tfXASFNCIiISxhSweMvWiSmVQCfdQlPxOPeUUHs1WEBTQiIiEtYUsHjLk8PSiRGWLpkSOmmEpbUqt6ApIRERCWsKWLwViByWgCbdNgYsVcVQW97xCIt7Ssh0+re9gIiISBApYPFWZwKWQJfmB4hOhNje1v3je7wYYelktV4REZEgUsDiLU8OS2eWNQdwSgiabYK424sclmbBkqaFREQkzChg8VZnCse5XxPIERZouQmip8qtNyMsSrwVEZHwooDFW4FIug34CEuzxFtPlds2RlgMo1nQpREWEREJLwpYvNWZZcGepNsAByzuEZajX0H1Cet+a0Xj3LRSSEREwpQCFm8FonBcoKeE3CMsh9Y3nj8aYlLabu/oxGcQEREJIgUs3rI5rGOnljV3UdKte5oqoa819dMWTQmJiEiYUsDiLfeXvT+bH3bFsmaAuD4QEdf0c2u7NDfn/gwNWtYsIiLhRQGLtzpVOK6Lkm4No2laCNrPX4Fm+wkpYBERkfCigMVboVY4zi0lu+l+WyuE3DQlJCIiYUoBi7c8X/Y+Tgm5nFY5fAhsaX43dx4LtF2DxU1TQiIiEqYUsHjL5ucIS/P2gdz80K35lFBHIyyaEhIRkTClgMVb/haOa17zJNA5LNBUiwW8GGHpxLSWiIhIEClg8Za/heOaBwddEbD4MsKiwnEiIhKmHMHuQNjwt3BcQ7MaLO3VSPFXUhb0zrXyZDoaYdGUkIiIhCkFLN7qbA5LVyTcAtjscMungAn2Dn6dmhISEZEwpYDFW/5OCXmWNHfBdJCbt+fWlJCIiIQp5bB4y9+kW09Z/i4aYfGFZy8hjbCIiEh4UcDiLbufX/buEZmuHGHxlr+fQUREJMgUsHjLMyXkY+G4hhAaYdGUkIiIhCkFLN7yO+m2i3Zq9ofDz5VOIiIiQaaAxVue3Zp9TbptDHBCakpIIywiIhJeFLB4y71k2OfCcaE0JeTeS0gBi4iIhBcFLN7yN2E1lEZYPIXjNCUkIiLhxa+A5dFHHyU7O5vo6GgmT57MmjVr2my7efNmvv3tb5OdnY1hGCxduvSUNr/+9a8xDKPFbfjw4f50revY/C3NH4IjLJoSEhGRMONzwPLiiy8yb948Fi5cyPr16xk9ejTTp0+nqKio1fZVVVUMGjSIxYsXk5GR0eZ5zzzzTI4cOeK5ffzxx752rWuFcuE4b3mmhLSsWUREwovPAcuDDz7ITTfdxJw5cxgxYgTLli0jNjaWJ598stX2EydO5IEHHuDaa68lKqrtUQaHw0FGRobnlpqa6mvXupa/SbfuACcURli0l5CIiIQpnwKWuro61q1bR15eXtMJbDby8vJYuXJlpzqyY8cOMjMzGTRoELNmzWL//v1ttq2traWsrKzFrcv5uw+Pe/rFEQIBi6aEREQkTPkUsBQXF+N0OklPT2/xeHp6OgUFBX53YvLkyTz11FMsX76cxx57jD179nDeeedRXl7eavtFixaRlJTkuWVlZfn93l7zu3Cce/PDiMD2xx+aEhIRkTAVEquELrroIq6++mpGjRrF9OnTefPNNykpKeHvf/97q+3nz59PaWmp53bgwIGu72SnC8eFwAiLpoRERCRM+bRbc2pqKna7ncLCwhaPFxYWtptQ66vk5GSGDh3Kzp07W30+Kiqq3XyYLuH3suZQSrr1M+gSEREJMp9GWCIjIxk/fjz5+fmex1wuF/n5+UyZMiVgnaqoqGDXrl307ds3YOfsNHfhOJePU0Lu4CAURli0l5CIiIQpn0ZYAObNm8fs2bOZMGECkyZNYunSpVRWVjJnzhwArr/+evr168eiRYsAK1F3y5YtnvuHDh1i48aNxMfHM2TIEADuuOMOZs6cycCBAzl8+DALFy7Ebrdz3XXXBepzdl6nR1hCIGDRlJCIiIQpnwOWa665hqNHj7JgwQIKCgoYM2YMy5cv9yTi7t+/H5utaeDm8OHDjB071vPzkiVLWLJkCdOmTWPFihUAHDx4kOuuu45jx47Rp08fzj33XFatWkWfPn06+fECyN5s40DTBMPw7nWeERZNCYmIiPjL54AFYO7cucydO7fV59xBiFt2djamabZ7vhdeeMGfbnQvm/tSmeByNk0RdSSURlg0JSQiImEqJFYJhYXmIyS+FI8LpREWR7NRIhERkTCigMVbzeuo+DKl4m4bEiMsKhwnIiLhSQGLt2zNAxYfVgq5p19CYYTF3izptoNpOhERkVCigMVbNhsYduu+PyMsoRCwNK8Fo2khEREJIwpYfOHP0uaQSrptHrBoWkhERMKHAhZfeHZs9mFKKJRGWJoXr9N+QiIiEkYUsPjCvZQ5XEdYbLam5dmqxSIiImFEAYsv7H4sCw6lzQ9BK4VERCQsKWDxhWfHZl8Clsa2obD5ITQFLJoSEhGRMKKAxRfuWiy+FI5rCLERFu0nJCIiYUgBiy/82YvHk3Qb0X677qIpIRERCUMKWHzhT8ASSkm3oCkhEREJSwpYfOHJYfFyWbPL1TR9pCkhERERvylg8YWvheOatwuZpFs/RolERESCTAGLL3xNum2eJxIqIyzufjQoh0VERMKHAhZf2H1c1tw8TyQUKt2CpoRERCQsKWDxha+F49xBgS3CqjIbCjQlJCIiYShEvkXDhK9l7Z0htkIINCUkIiJhSQGLL3wdYWkIsRos0JT8qxEWEREJIwpYfOFv0m2oJNyC7yudREREQoACFl/4mv/hHmEJlSXNoCkhEREJSwpYfOEZnfCycFwojrA4/NhxWkREJMgUsPjC5usISygm3WovIRERCT8KWHzhcw6LO+k2lKaE3HsJKWAREZHwoYDFF74WjnMHLKE0wuIpHKcpIRERCR8KWHzh6wqbhhAeYdGUkIiIhBEFLL6w+TrC4k66DcGApUHLmkVEJHwoYPGFz3sJhWDSrfYSEhGRMKSAxRfu0YmwTrp1B12aEhIRkfChgMUXPheOC8ERFk/hOI2wiIhI+FDA4gtPwOJt4bgQHGHRlJCIiIQhBSy+6BGF43z8DCIiIiFAAYsvekQOi/YSEhGR8KOAxRd2h3UM68Jx2q1ZRETCjwIWX/hcOC6E67AoYBERkTCigMUXdh93OtaUkIiISEAoYPGFzccpoVBMunX4GHSJiIiEAAUsvvA56TaUp4Q0wiIiIuFDAYsvfC4cF4JJtyocJyIiYUgBiy98LhznHmEJoYBFq4RERCQMKWDxhc+F49wjLJoSEhER6QwFLL7wdUmwZ5VQCI2wuD+D6fJ+pEhERCTIFLD4wl04zuXrlFAIjbA0z6fRtJCIiIQJBSy+8LlwXAhPCYGmhUREJGwoYPFF88Jxptlx+1BMurU5AMO6r5VCIiISJhSw+MJdOA4TXM6O24fiCIthNE0LaUpIRETChAIWXzSfTvGmeFwojrCA9hMSEZGwo4DFF+46LODdl30oFo6DpoBF+wmJiEiYUMDiC1vzgMWXEZYQmhICTQmJiEjYUcDiC5sNDLt1v6OAxTSbAoKQG2HxsQCeiIhIkClg8ZW3+R/NA5rmU0mhwLOfkKaEREQkPChg8ZVnx+YOisc1r3ESakm32k9IRETCjAIWX7mr3Xb0Zd+8xknITQkpYBERkfCigMVXzYvHtcc9wmLYwWbv2j75SlNCIiISZhSw+MqzY3MHAYs7GAi10RVoNiXkxUonERGREKCAxVferrDx7NQcYkuaodkokUZYREQkPChg8ZU7YOmo0m0oj7CocJyIiIQZBSy+8nqEpTGgCbUVQtCscJymhEREJDz4FbA8+uijZGdnEx0dzeTJk1mzZk2bbTdv3sy3v/1tsrOzMQyDpUuXdvqcQeWZTvFyWXOo1WABTQmJiEjY8TlgefHFF5k3bx4LFy5k/fr1jB49munTp1NUVNRq+6qqKgYNGsTixYvJyMgIyDmDyublCEtYTAlpWbOIiIQHnwOWBx98kJtuuok5c+YwYsQIli1bRmxsLE8++WSr7SdOnMgDDzzAtddeS1RU61/evp4zqLzNYQnlpFvtJSQiImHGp4Clrq6OdevWkZeX13QCm428vDxWrlzpVwf8OWdtbS1lZWUtbt3G3gOWNWtKSEREwoxPAUtxcTFOp5P09PQWj6enp1NQUOBXB/w556JFi0hKSvLcsrKy/Hpvv3hdOC6ER1g0JSQiImEmLFcJzZ8/n9LSUs/twIED3ffmNm9L84fwCIumhEREJMw4fGmcmpqK3W6nsLCwxeOFhYVtJtR2xTmjoqLazIfpcr6W5g/FZc2eaS1NCYmISHjwaYQlMjKS8ePHk5+f73nM5XKRn5/PlClT/OpAV5yzS3l2a+4oYGl83hGKU0LuvYQ0wiIiIuHBpxEWgHnz5jF79mwmTJjApEmTWLp0KZWVlcyZMweA66+/nn79+rFo0SLASqrdsmWL5/6hQ4fYuHEj8fHxDBkyxKtzhhSvd2sO4REWTQmJiEiY8Tlgueaaazh69CgLFiygoKCAMWPGsHz5ck/S7P79+7HZmgZuDh8+zNixYz0/L1myhCVLljBt2jRWrFjh1TlDSo8oHOdlLRkREZEQ4XPAAjB37lzmzp3b6nPuIMQtOzsb0zQ7dc6Q4nXhuMbnQzHp1jMlpBwWEREJD2G5SiiovC4cF8pTQu5RIo2wiIhIeFDA4iuvC8e5R1hCMelWAYuIiIQXBSy+8vbLPpRHWDQlJCIiYUYBi6/cOSwNNe23C+URFoeXtWRERERChAIWX/UeZB33/AdcrrbbeUrzh+IIi/YSEhGR8KKAxVdDL4LIBCjZD/s/bbudM4RL86twnIiIhBkFLL6KjIUzr7Dub3y+7XbuYCAU67BolZCIiIQZBSz+GPMd67jlVairbL1NSCfdakpIRETCiwIWfwyYAinZUFcBX73ReptQTrp1ByyaEhIRkTChgMUfhgGjr7Pub3yu9TahPMKivYRERCTMKGDx1+hrrePuFVB66NTnG0I56bbZjtPtrXQSEREJEQpY/JWSDQPPAUzY9OKpz3uWNYfwlBBolEVERMKCApZ21DW4WLX7GM+t3t96A/e00OfPw8kbPIbyCEvzPilgERGRMKCApR3HK+u49vFV/OrVLyivaaUq7IjLwREDxdvh0PqWz7mryIbiCIut2VJrBSwiIhIGFLC0IyMpmqxeMbhM2LC/5NQG0Ylwxkzr/ucnJd96km5DMWCxNdtiQEubRUQk9Clg6cDEgb0AWLv3eOsNxjROC33xj5Zf/p5lzSE4JQRaKSQiImFFAUsHJuZYAcuatgKWnGmQ0BdqSmDPR02Ph/IICzRV4FXAIiIiYUABSwcmZqcAsPFACXUNrSwBttlh6Azr/va3rKNphnbSLTTbT0hTQiIiEvoUsHRgcJ94UmIjqKl3sflwaeuNhl1kHbctt4IVVwPQuGooVEdYPPsJtZJMLCIiEmIUsHTAMAzGe/JYTrTeKOd8a7VQ2UEo/LLlqEXIjrBoPyEREQkfCli8MCnHmhZqM48lIgYGf926v215y7yQUCzND5oSEhGRsKKAxQsTsptWCpknF4hza57H4g5YDBvYHd3QQz9oSkhERMKIAhYvjMxMIjrCxomqenYdrWy90dDp1vHQOig9aN0P1fwV0JSQiIiEFQUsXoh02BiTlQy0U48lIQMyx1r3t75mHUN1OgiaBSxa1iwiIqFPAYuXJmZ3UI8FYGjjaqEt/7KOjhAeYXEnAzcoYBERkdCngMVLTXksbawUAhjWmMdyYq91DIsRFk0JiYhI6FPA4qVxA5KxGbD/eBWFZTWtN8oYBYn9mn4O5REWu5JuRUQkfChg8VJCdARn9E0E2hllMYym5FsI7REWh5Y1i4hI+FDA4gN3Hstn3uSxQIiPsLj3ElLAIiIioU8Biw8mNO4r1G7AknOeVfUWQnuExd03TQmJiEgYUMDiA/cIy9YjZZTXtPFF37zqbSjXYdGUkIiIhBEFLD5IT4xmQK9YXCas31/SdsPhl1rHuN7d0i+/eKaEtKxZRERCX4jWjQ9dE7JT2H+8is/2HGfa0D6tNxp9LWBamyKGKs+UkAIWEREJfRph8ZF7Wmj9/nbqsdjsMPa7kDygm3rlB3dCsKaEREQkDChg8dG4AVbi7cYDJTQ4XUHuTSeoNL+IiIQRBSw+yk2LJyHKQVWdk22F5cHujv80JSQiImFEAYuPbDaDMQOSgQ4Sb0OdpoRERCSMKGDxg3taaP2+dvJYQp0j2joe3QZ1lcHti4iISAcUsPhh3MDGgKW9xNtQN/gCiOkFx3bAP24ElzPYPRIREWmTAhY/jMlKBmDfsSqKK8J0SiUhA77zojXSsv0teOtnYJodv87lhIIv4cjnUPAFFG6Goq+gvrrr+ywiIqct1WHxQ1JMBEPT49leWMH6fSf45pkZwe6Sf7Imwbceh7/Phs/+AskD4ZyftP+at34Onz1x6uOpw+Cm9yEqvmv6KiIipzWNsPjJk8cSzom3ACMuh+n3WfffvRu+fLn99jvfs46xqRCfDnF9rBVHxdvg7fld21cRETltKWDxU1PAEsZ5LG5n/wgm32zdf+WHcHxP6+2qT8CJxud+vBbu2A537oTv/hMwYP0zsPXf3dJlERE5vShg8ZM78XbTwRLqw7mAHIBhwPTfQb8JVl0W9yjKyY58bh1TsiEmpenxnPPgnNus+6/9GMqOdGl3RUTk9KOAxU+DUuNIiomgpt7F1iNlwe5O59nsMCTPun/ws9bbHN5oHfuOOfW5r/8S+o62RmFevRlcYR7EiYhISFHA4iebzWCsu4BcONdjaa7/ROvYZsCywTpmjj31OUckfOsv4IiB3Stg9WMtn3c5vVuFJCIi0goFLJ3gzmNZF+6Jt279x1vH47uhsvjU549stI6ZY1p/fZ+hTQm87/0alp0LfzwLFmXBPb3gD8Pg3QVQvCPAHRcRkZ5OAUsnjB/YAyreNheTYi1PhlNHWaqOw4m91v2+o9s+x4Tvw7CLrVyYgi+gdD/UNk6ZVRTCJw/BIxPg/02H9f8HzoaAfwwREel5VIelE0ZnJWMz4FBJNYVlNaQnRge7S53Xf6K1RPnAGhh2UdPjnoTbnJYJtyczDPj2/4Nd+db0UEwyRCdBVCIcXAMb/gY73oEDq6xb0RaYsahLP5KIiIQ/jbB0QnyUg6HpCUAPGmXJaiOPpaPpoOYiY+GMmZCbB/0nQGouJKRbj33nRfjpFjj/Z1bbz/4fVBQFqvciItJDKWDppPE9YV+h5vpPso6H1recrmkv4dZXiX3h679oXEZdC6se6/g1IiJyWlPA0kmexNueMsLSZ7g1fVNfaU3XuLW3pNkfhgHn/tS6/9lfoKY0MOcVEZEeSQFLJ7kLyH15qIzahh6w47HNBv3GWfcPrrGOVcehZJ91v72EW18Nu9hK8q0tg7V/Ddx5RUSkx1HA0knZvWPpkxBFndPFXz/ZG+zuBIZ7WujgWuvozl/pNchKog0Umw3Ovd26v+rPUF8TuHOLiEiPooClkwzD4M5vWkuBH3xnO9sLy4PcowDIagxYDjSOsAR6Oqi5kVdBYn9ryfPnzwX+/CIi0iMoYAmAqyf05+vD+lDndPE/f/88/PcW6ucuILcLKo81S7gdE/j3ckTC1LnW/U8eUl0WERFplQKWADAMg8XfHkVSTARfHCrlsRW7gt2lzontBb1zrfuH1jZb0hyAFUKtGXc9xPSyCtNt/VfXvIeIiIQ1BSwBkp4YzW8uOxOAP+XvYPPhjle9FJTWUFMfoom67mmh7cuhZL91P5AJt81FxsHkm637H/9Rew6JiMgp/ApYHn30UbKzs4mOjmby5MmsWbOm3fYvvfQSw4cPJzo6mrPOOos333yzxfM33HADhmG0uM2YMcOfrgXV5WMymX5mOg0uk//5++fUNbQ9NbR+/wnO+/37XLXsU6rrQjBocW+E+PmL1rHXYKtibVeZdBNExFrl/Au+6Lr3ERGRsORzwPLiiy8yb948Fi5cyPr16xk9ejTTp0+nqKj1aqWffvop1113HTfeeCMbNmzgiiuu4IorruDLL79s0W7GjBkcOXLEc3v++ef9+0RBZBgG9115Fr3iIvmqoJw/5be+yZ9pmtz7+hbqnSZfHirjF698gRlqowrugKW+0jp2Rf5Kc7G9YMAU6/7+VV37XiIiEnZ8DlgefPBBbrrpJubMmcOIESNYtmwZsbGxPPnkk622f+ihh5gxYwZ33nknZ5xxBvfeey/jxo3jkUceadEuKiqKjIwMzy0lpZ39akJYanwUv71iJACPfbiLTQdLTmnz+qYjbNhfQnSEDbvN4JUNh/i/Vfu6uacdSDsDIhOafu6KFUIn8wQsK7v+vUREJKz4FLDU1dWxbt068vLymk5gs5GXl8fKla1/yaxcubJFe4Dp06ef0n7FihWkpaUxbNgwbrnlFo4dO+ZL10LKxWf15ZJRfXG6TO546fMWBeVq6p0sfusrAH70tSHcNWM4APf8ewvr9h0PSn9bZbM3FZCDrku4bW7A2dZx/yrlsYiISAs+BSzFxcU4nU7S09NbPJ6enk5BQUGrrykoKOiw/YwZM3jmmWfIz8/n/vvv58MPP+Siiy7C6Ww9t6O2tpaysrIWt1Bzz2Vn0jsuku2FFTycv9Pz+FOf7uVQSTUZidHcdN4g/vu8HC4Z1ZcGl8ktf1tPUXkIFU9zTwsB9B3V9e/XbzzYHFB+GEoPdP37iYhI2AiJVULXXnstl112GWeddRZXXHEFr7/+Op999hkrVqxotf2iRYtISkry3LKysrq3w17o3crUUHFFLY++bwUvd04fRkykHcMw+P23R5GbFk9ReS1zn90QOnVcBk61jn2Gd23CrVtkbNPUk/JYRESkGZ8CltTUVOx2O4WFhS0eLywsJCMjo9XXZGRk+NQeYNCgQaSmprJz585Wn58/fz6lpaWe24EDofl/4xed1ZdLm00NPbB8G+W1DYzsl8iVY/t52sVFOVj2vfHERzlYs/c4z63eH8ReNzP4Apj5J/jW4933np5pIeWxiIhIE58ClsjISMaPH09+fr7nMZfLRX5+PlOmTGn1NVOmTGnRHuDdd99tsz3AwYMHOXbsGH379m31+aioKBITE1vcQtU9l48kNd6aGnpxrRVY/eqSEdhsRot2g/vE8/OLrHyWx/+zOzRGWQwDxs/uuvorrWmex9LdnPXgCsEl5iIi4vuU0Lx583jiiSd4+umn2bp1K7fccguVlZXMmTMHgOuvv5758+d72t92220sX76cP/zhD3z11Vf8+te/Zu3atcyda5Vjr6io4M4772TVqlXs3buX/Px8Lr/8coYMGcL06dMD9DGDp1dcpGdqCOCbI9I5e1DvVttePb4/qfGRHCqp5s0vjnRXF0NLVmPAUrQFqk903/s66+HPU+APw2Dd060HLif2whv/A2/coS0ERES6mc8ByzXXXMOSJUtYsGABY8aMYePGjSxfvtyTWLt//36OHGn6sp06dSrPPfccjz/+OKNHj+Yf//gHr776KiNHWl/idrudTZs2cdlllzF06FBuvPFGxo8fz0cffURUVFSAPmZwzRjZl9lTBjKgVyy/vOSMNttFR9i5YWo2AMs+3B16tVm6Q3wf6D3Eun/gs+5736ItcGwHVB6Ff/8EHp8Gez+xnis5AP++DR4eD5/9BT57AnZ/0H19ExERDLMHfCuWlZWRlJREaWlpSE8PeaO0qp6pi/OprHPy1JyJfG1YWrC71P3+dSts+BucOw/yFnbPe659El7/KSRlQU0Z1DZurZB1NhxaB6566+fYVKgqhjGz4Io/d0/fRER6KF++v0NilZA0SYqN4LpJAwBY9mGYb6Lor2BUvD20zjqOugZ+sh4mfB8MGxxYZQUrOdPg+2/Dfz1jtfvqdWio677+iYic5hSwhKDvn5uDw2awavdxNh4oafHcB9uKuOOlz9l9tCI4nesO7oDl0DpoqO2e9zy03jr2Gw9xqXDpH+Hmj2HqT+CGN2D2a1ZC8ICzIT4Dakph94ru6ZuIiChgCUWZyTFcPsZa9vy/jaMslbUNzH/5C+b89TP+se4gP/i/dVTV9dDEz16DIK4POGvh8Ma229VXW0HDRw+2364jteVQtNW637y6b/qZ8M17IfvcpsdsdhhxuXV/8yv+v6eIiPhEAUuIunnaIACWby7gH+sOctFDH/H8mv0YBiREOdhZVME9/94S5F52EcNoux7Lib3wwSL468WweAA8cznk/wb+Mcf/cv5HPgdMSOwPCW3XB/I480rr+NUb3TcCJCJymlPAEqJy0xPIOyMN04Q7Xvqc/cer6Jccw3P/fTb/+73xGAa88NkB/v354WB3tWu4lzcfWN302IHP4H/Phw8Xw75PwFkHCX3BFgHHd0Pxdv/ey52/0nx0pd2+Tbbet7YUdmm1kIhId1DAEsJunjbYc/+q8f156/bzmDK4N1OHpPKjr1nP/eLlLzhwvCpYXew6zRNvXS7Y/aE1mlJTapXvv3Qp/Hg9zNsKg6ZZbbe95d97eQKW8d61t9lgxBXWfU0LiYh0CwUsIWxCdi/+3+wJPHfTZJZcPZrE6AjPc7fnDWXcgGTKaxv48fMhtP9QoPQdBY4YqD4On/4Jnr0a6ith0NesJNgJc6D3YGv6aOgM6zXb3/bvvZon3HrLPS207U2oD6ENK0VEeigFLCHuwjPSmTo49ZTHI+w2Hrp2LAnRDjYeKOEP7/g5HRKq7BHQf4J1/72FVgLu8EvhuhchKr5l26GNFZEPrIKq4769T3lh487QBmSO8f51/SdCYj+oLYNd7/v2niIi4jMFLGEsq1cs9397FGDVbPnLR7uD3KMAG9Bsv6lR18DVT0NE9KntkgdA2plgumDne769x+HG0ZU+wyEqwfvXaVpIRKRbKWAJcxef1ZcfX2CVsv/tG1t5bEUPKjZ31lXWKMaUuXDFMrA72m7rHmXZvty39/A1f6W5FtNC1b6/XkREvKaApQeY942h3HZhLgD3L/+Kh/N3BLlHAdJnGMzbAtPvs0Y02jPsIuu44z1rI0NvHVxrHb1dIdRc/wlWKf+6CtiZ33F7ERHxmwKWHsAwDH76jaHc8c2hAPzh3e08+O7202vzxH7jIba3tdTY25L+LlfTlJA7X8YXhqEiciIi3UQBSw8y94Jc5l80HIA/5e/gV69+SW2DM8i96iY2O+T6OC10fLe1TNoRDWkj/Htfdx7L9rdVRE5EpAspYOlhfjhtMHdfan35Prt6P1cvW9kz67S0xtc8Fnf+St/R1qokf/QbDwmZUFeuInIiIl1IAUsPdOO5Ofz1hokkx0aw6WApl/zpI97dUhjsbnW9wRdYVW+P7YTinR2370zCrZvNBmfMtO5v/bf/5xERkXYpYOmhvj48jTd+ch5jspIpq2ngpmfWsuitrbhcPTivJToRss+x7nszyhKIgAWaApZtb/iW8CsiIl5TwNKD9UuO4e8/nMKcc7IB+N8Pd3PHS5/T0NOq4jbnqXrbQcDSUAcFm6z7/qwQam7gVIhNheoTsPfjzp1LRERapYClh4t02Fg480yWXjMGu83g5Q2H+MkLG6hr6KFBizuPZf9KqC5pu13hl9bmiTEpkJLTufe02WH4xdZ9TQuJiHQJBSyniSvG9uPPs8YRabfx5hcF3Py3ddTU98AVRL0GQeowcDXAyzdBxdHW27mXPvcbby1P7qwzGpc3f/W6tVxaREQCSgHLaWT6mRk8MXsCUQ4b739VxH8/vZaquoZgdyvwLvgV2KNgxzvw2BTY1mx66MQ+ePmH8PYvrJ+zJgfmPXPOh6gkqCiEA6sDc04REfFQwHKamTa0D09/fxJxkXY+3lnMnf/YFOwuBd6Iy+AHH1j7C1Ueheevgdd/CsvnwyMTYNMLgGnVUDn7R4F5T0ckDGvMn9G0kIhIwClgOQ2dPag3z9w4CZsBb2w6wrp9Pu5wHA7Sz4Sb3rf2IQJY+ySs+rOVt5IzzXruv54+defnzjjjMuu49d9wOlUZFhHpBgpYTlPjB/bivyZkAXDfG1t7Zhn/iGhrH6Lr/2Ul1vabAN97BWa/1vmlzK0ZfAFExELpfji8IfDnFxE5jSlgOY3N+8ZQYiLsrN9fwltfFgS7O11n0Nfgto1wU74VVHSVyFjI/YZ1X9NCIiIBpYDlNJaWGM0Pzh8EWLs899ilzt3JMy30mqaFREQCSAHLae4H5w+iT0IU+45V8bdV+4LdnfCX+02wR1rbA3z8ILh64NJxEZEgUMBymouLcjDvG0MB+NP7OyitVmn5TolOhIn/bd3Pvwf+ehEc2xXcPomI9AAKWISrx/cnNy2ekqp6/vyBF5sGSvum/w4uewQiE6yaLMvOhTVPaIpIRKQTFLAIDruNX1x8BgB//WQv81/exIptRcpp8ZdhwLjvwY8+hezzoL4K3rwDXpsb7J6JiIQtw+wB61nLyspISkqitLSUxMTEYHcnLJmmyQ//bx3vbCn0PJYQ7eCC4Wl8Y0Q65+X2ISkmIog9DFMuF6x5HN6eD6bLWlbdlSuVRETCiC/f3wpYxKPB6WLl7mMs/7KAd7YUcrS81vOc3WYwfkAKXxvehwuGpzEsPQEjEHvwnC7e+jmsXga9BsOPVoIjKtg9EhEJOgUs0mkul8mGAydY/mUBH2w7ys6iihbPD+4Tx8zRmVw2OpNBfQJYLbanqimFRyZaew1d8Cs4/85g90hEJOgUsEjAHThexYptRXyw7Sgf7yxukd8ysl8i00dkcE5uKqP6JeGwKzWqVZtegpf/GxwxcOtqSBkY7B6JiASVAhbpUuU19byzuZB/bzrMRzuKcbqa/gnFRzmYnNOLc4akcv7QPgzuE6epIzfThKdnwt6PYNjFcN3zwe6RiEhQKWCRbnO8so63Nxfwn+1HWbn7GCVVLeu49E+JYdrQPkwb2ofJg3orcbfoK1h2Drga4LoXYNhFwe6RiEjQKGCRoHC6TLYcLuOTXcV8vKOYNXuOU+dsuTQ6IzGa3PR4ctMSGJYRz7gBKQxJiz+9RmHeXQCfPATJA+CWTyEqIdg9EhEJCgUsEhKq6hpYtfsYH247yofbj7L3WFWr7XrHRTIppxeTc3pxbm4fhqT18CTe2gp4dDKUHYTUoXD1U5B+ZrB7JSLS7RSwSEgqra5nZ1EFOwrL2V5YwdYjZWw4cIKa+pajMGf1S+Jb4/px2ehMesf30OW/B9fCi9+F8iPgiIaL7odxs62icyIipwkFLBI26hpcbDpYwuo9x1m1+xgrdx2joTGJ12Ez+NqwNC4+K4Pzh/YhtacFL5XF8MrNsPNd6+eR34ZLl1r7EYmInAYUsEjYOl5Zx78/P8w/1x9k08FSz+OGAaP6JTFtWBrThvZhVP8kInrC8mmXC1Y+bG2U6GqAtDPh+29BdFKweyYi0uUUsEiPsKOwnH9tPMwH24rYfLisxXMxEXbGDkhmUk4vJmX34sx+SeG9AunAZ/DiLKuw3OAL4Dsvgd0R7F6JiHQpBSzS4xSV1bBi+1FWbCvi012nLp8G6JMQxZA+8QxJi2doejxjB6QwPCMhfArZHd4If73I2ixxwo1wyR+U0yIiPZoCFunRXC6TnUcrWLPnOGv2HGft3uMcLq1ptW1spJ0xWclMGJjCgN5xREfYiImwE914S4x2kBAdQUK0g9hIe/CXV2993UrGxYQZi+HsW4LbHxGRLqSARU47ZTX17CqqYNfRSnYWVbDlSBkb9p2gvLbB63PYbQZD+sRzxdh+XDE2k75JMV3Y43Z8+jC88yvAsKrhqriciPRQClhEsArZ7SgqZ92+E6zfV0JxRS019c7Gm4uq+gbKa6xb8+0FwJqJOWdwKt8a148LhqeRHBvZfR03TXj9dlj3FETEwZALIC4N4tMgrg8MOFt1W0SkR1DAIuID0zSprndSUlXPRzuO8s/1h1iz57jnecOwasNMHZzKuUNSmZCdQnSEvWs75ayHZ6+G3R+c+pwjBm5dBSnZXdsHEZEupoBFpJP2H6vilQ2HeH3TYXYUVbR4LtJuY8yAZM7O6cXZg3ozdkAKMZFdEMA462HHu1B2CCqKoLII9n4Cx3bA0BnwnRcD/54iIt1IAYtIABWW1fDJzmI+3lnMJzuLKSyrbfF8hN3g7EG9mTEyg2+MSCctIbrrOnN0Gzx2Drjq4drnYfjFXfdeIiJdTAGLSBcxTZO9x6pYtfsYq3cfY9Xu4xSUNa1QMgyYMDCFrw9PIzctgZzUOAb0iiXSEcCl1e/9Gj7+IyQNsKaGIuNaPr9vJez5j7U8uqHGOjobIOd8q5quoxvzcURE2qGARaSbmKbJ7uJK3tlcyPLNBXx+oOSUNjYDsnrF8vVhafxw2qBWVx8dr6zjjS+O0Cs2kulnprdfO6au0to8sfQAnDsP8hZaj7tc8OFi+PD+tl8bnwGTfwDj50BsLx8/rYhIYClgEQmSwyXVvL25gLX7TrC3uJI9xZVU1Tk9z0fabVw1oT+3TBtMVq9YNh8u5alP9vKvzw9T12BtApndO5YffW0IV47r1/b2A1+9AS98B2wRcMun1gqil2+CHe9Yzw+/FJIHQESMdauvhg3PQkWB9XxELIy/AS5cCBFdOIUlItIOBSwiIcI0TYrKa/niYCmPf7Tbs/rIYTPITU9g65GmLQfOzEzkcEk1Jxqr+PZLjuGH0wYxOac3A3vHtlyZZJrw3DWw423oNwGqjsGJPZj2aL4Y9xsqhl3FlMG9WxbCa6iDL/8JKx+Bwi+tx3LOt3JhouK7/FqIiJxMAYtIiFq9+xgPv7+Tj3cWA1bgctFZfblhajbjBiRTVefkudX7+d//7Ka4oim51zCgf0oMg1LjSU+MIsphJ815hJu/nEWEabU7TB9uqr2dzWYOAOflpnLP5SPJST0px8U0Yfty+Od/Q10F9J8Es16CmORuuQYiIm4KWERC3Ib9J/iqoJyvD0sjI+nUKZmaeicvrNnPqxsPs+toBeU1rVfsvcn+Or+MeI6PnCP5Sf1cym1JDEmLZ3dxJXUNLiIdNm792hBu/togohwnLb0+uBb+9i2oKYWMUfC9VyAutSs+bs/hrAd7GG+yKRJiFLCI9CCmaVJcUceuoxXsPlrJiao6ahtc1DW4qG1wEldTRHq/bEb2T+aMvolER9jZW1zJ3f/6ko92WCM5OalxXDW+P0PS4slNi2dg7zjsNgMKvoBnroCqYkgdBpf+0QpaopMhOkn5LW4NdVYy86d/gmEXw2UPQ7T+1ogfKoth53sQ0wtSBjblmp2mFLCICKZp8vqmI9zz+haOlresHRPpsDGkTzxn9UvinOTjzFj/QyKrCk49SVQiZI6BrMnW1FH/CZ1fXVRTBtveguO7wbCBzWYd7ZEwcCpkjgutXaqPbrMSmo983vRYr8Fwzd8gfUTw+nW6cbmgpsT6dxIRA7YurjbdFY5sguf+C8qPtHw8Ls3KJ7twgRXEnEYUsIiIR1lNPS+uOcCWI2XsKCpnZ1EFNfWuFm36G0X8NuIphjkKSDIqiXFWYNDGn4akAZA6BHo33noNhvg+EJtqjc44ok59TX2NtYLpi5esY0Pru2sD0Gc4jL4ORl0DiX1bb1NXaf3xP7weTuy1vsQc0Y23SKitgIrCplt9NQz6Opx1FfQb711A5HLBmsfhvYVWf2NS4Lw7YNVjUHbQWmk18yEY9V8dn6s99TVwYDUUbbFGteL6WNcxrg/YHNZnrauwjg21EJ8OyVkQlWC93jShaKv1f+278q3rMvgC+MY9kNTv1M/0+fOw6s9W/wdOtW5Zk5tymEzTul41Jda1KzsC5YetY1UxuJxWG9MFphNie1u/s7QR0GeYNfJUdtiacjz4GRxaB9UnrFG7mOTG0btEqz5QTSlUl1hHm93aIytjlHVLO8MKavd9Avs+tW7VTVtmYIuwApeYFEjIsG7xGdbPteXWe9aUWMf4dBg6HXK/eeq0Z3UJHNkIFUetfxeGrelIs58xrKP7s2Na910N1s1ZZ00ZRiVYlahPTmTf/g68dAPUV1r/DUUnQck+qG1KvMceBef8BM79acv6SvXV1rWsKLJ+b5Gx1j5jkXFWgHNyLab2NNRaozyJmSHxPwYKWESkTU6XyaET1Ww5UsoXh0rZdNA6ljSuTgIwcBFPDf2MYsbZdjDBvoNxtp1kc7jD87si4iEyFgMTwzQBs/HLtlmQ0jsXss+1/mC6nNYXX3WJ9aXrbmfYrOAiMt4KguyR1pfa0e1wdGvjl4YfkgdaBfRyzrOG5WNSrC9Se5QVNBzeYN0OrIHibQCYg/M4esGDHHQmMji2hqS3fgS73rfOd9bV0He09cUd29s6Z30VVB61vhiqiq3PH53U9KUdlWh9ht0fWsFKewFcW6KTrcCl8pgVUJwsIg6m3Qln32oFcQfWwFs/sz7bKQzrXHWNQYSrvpU2PvSrpsT/13cpA/pPtP7tleyzrsXx3YF/m6gkGDsLJv439B4Ma56wrr3pgpxp8F/PWP8OTNMKqI5+BSsWWQUfARL7wfl3WL/bPR9avztnbRtvZlhBS9oIK3BMPxP6jYOUnKaAxDStEcKNz8Kmv1u/n5he1rXImmgdTRNO7IHje6z/Cag+Yf27zppkBbQJGYG/TnRDwPLoo4/ywAMPUFBQwOjRo3n44YeZNGlSm+1feukl7r77bvbu3Utubi73338/F1/cVFLcNE0WLlzIE088QUlJCeeccw6PPfYYubm5XvVHAYtI55imycET1Ww+XMbWI2VsOVLGlsNlHCqpbtEumXKGGIfIsRUw2DhCjnGEAUYhvY1yUignwnC28Q5w1NaHT2OmsTru6xyJziUuOoJecZGkxEZax7hIYl2VZB5aTubeV0guXtdun+tj06lPH4MrdSiG6cLmrMXmrMVw1mKLiseemI6RkGH937WzHrb8C7a9aQUTXqozovjfqDk8WjmNmnrrT6VhwMiMOH4W8xrnHf5/Xp+rXfEZ1nRbfXVToFN51ArkIuOt/4OOjLNGFcqPnBoQOKIh+zwYkgepuVa+zYHV1nO9h0DGWbD5FevnyATryzCuD+z/1KqMfHzXqX0y7FabxL6QkGkd4/pYSceGrWkEorzACvSOftU01WHYIO1M6D/e+jJMzGw2mlJiTQtGxjblSkUnWUFd4ZfWCFHBF1bNoKhEa3fygVNh4LnW5zCd1ihBfbV1qzpmtS0vsN6/usQawYlJaTp/0RZrZVzBF61f/5RsK5fE/XVouhpvjQF38/uekZfGURebw7om9gjr93N0a8sgKGMUFGyy7o/5rpUn1lq1adOErf+Gd34JJftPfT6hr/W7rK+yPnddpXVN2woOY1Ks6dW0M2D3iqZSBv5KHmgFLhc/ENAVhV0asLz44otcf/31LFu2jMmTJ7N06VJeeukltm3bRlpa2intP/30U84//3wWLVrEpZdeynPPPcf999/P+vXrGTlyJAD3338/ixYt4umnnyYnJ4e7776bL774gi1bthAd3XHSnwIWka5R73RRWdtAeU0DlXUNVNQ0UFxRy+GSGgrKajhcUk1ReS1VdQ1U1TRgqyslqu4EzroarD/vBiYG9djZZ6Zj4v0WBQONAkYae4mggUijnkgaiKCBg2YfPncNpoiUDs8R6bAR7bAR6bBR7zRxOKs4z7WO6cZKcowCko0KUqggyrBGFE6Y8XzhymGTOYgvXINY6xrKMZIAsNsMesdFUtQsH2iysZXp9s/obasg3V5Ob6OCZKOcels0VRG9qI5MoS6qN0ZkHPFUEecqJ8ZZTnRDOc74vlT3P5fq/ufh7DUEw2ajwemizumiwWlS3+DEBGw2A5thYG882gwDR0MFURWHiKw8RC0RHIwbRXGtjeKKWkqr60mKtjOx7B1GfPkHImqsxGsTA9eYWdRP+5VVaBDrO9dmGNgqizCO78GMSsCMSsSMScZ0xGG326zkbG9Vn4CSA9BrUOdr+1SfsAIWL3JVTNOk+TeZYdCyBpFb6UHY/rY1ldhrEGSOxcwYjRlj5WXZfPmsbXG5YPf71qjK9rfBPbV6wa+sKcWOpmHqq+HTR2DLq1aAknO+NSrTe3Drr60stqYEi7ZagVlBY8DnrGvZzh4Fwy+Bsd+1AsDCzdY004E11vWwR1qjMr1yrAAuKgEOrbcC38LN1ueISoSf77PyzgKkSwOWyZMnM3HiRB555BEAXC4XWVlZ/PjHP+auu+46pf0111xDZWUlr7/+uuexs88+mzFjxrBs2TJM0yQzM5P/+Z//4Y477gCgtLSU9PR0nnrqKa699tqAfmAR6XpOl0l5TT2l1datrLqB2ganZ3VTXYOL8toGSqrqOF5p3U5U1VHvNGlwmThd1pe2yzQxMFr8na5zuqitd1Fd76Sm3jqn09W5me0ERwO9HXXY41NJT4ohLSGK9MRo+iZFMzA1jpzecfRLiSHCbqOorIZPdx3jk53FfLrr2CmjUKEkgSrmOl4h2yjk4YYr+NIc5PM5bAY47DYi7TYcditgMmgKCkzTxOmyfm8ul4mzja8U9+/R/XqbzcBhM5qOhoHNBvbGoMwwrH9H9U6TOqeL+sZArsHlwuUCZ+P7dtR397ma9wPAZVp9bd5dmwEOm/U57TYDzKb3cf9bjLAbRDhsRDReEzfTNK0A0zCIirARE2FnoFFEXv0H7I0cwtqos3G6rHO5z9ngdF87Fw6bjehIOzERNqIj7EQ77EQ4bDhsVl8cNgOny6SqzklVXQOVdda/f4fNINJhI8phJ6oxOI+xOcmq383Amm1k1u6mIHoQ6xMvpIx46p0uz+/IfVkMw8Blmp7fpdMFNMthi3ZVMah2K73NE8y+Zb53/3C85Mv3t8OXE9fV1bFu3Trmz2/qsM1mIy8vj5UrV7b6mpUrVzJv3rwWj02fPp1XX30VgD179lBQUEBeXp7n+aSkJCZPnszKlSu9ClhEJLTYbQbJsZEkx3bfRouuxi8Vp2l9ydXWO6lpcFFb76TeaWK3GZ4vXYfdaPoDb7f59H/WaYnRXDG2H1eMtRJaq+oaOFFVz4nGoOtEVT0VNQ3WyFRt47FZ8FZa3UBpVR11ThcuE8+XoWlaO3+fHBy4v+TMxs/mMps+q8uECLuN1PhIesdH0isuiqQYByeq6jlSUs2R0mjuL5tFZ+I5l4knyAw3LtMKTFpq+2K4TCsgrmtzZtO0nmu7QQubieZNLmr86ZhXrwmceGB84w2gpPHmrywiHQOZ3cledYZPAUtxcTFOp5P09PQWj6enp/PVV1+1+pqCgoJW2xcUFHiedz/WVpuT1dbWUlvbNCxbVlbWajsROX3YbAY2DBxAlAPio3z68+a32EgHsZEO+iWHZi2NBqeLylqnlW7ROCpiYH1tuwMlszH4aT5yYhhWYGSNbpjUN7hocLmshTEmmJi4XHhGRey2pmmrk2cuPKkh7tc1C9TcI2kNnsCtKShz2A0i7DbPzT3a0Py93LFm88/lPoc7yDuZaVpBtWE0jeiYjdeqoXHko8Hl8kzB2WxWe9OEhmYjPu4gzjBajtzU1Ds9I4DuvcTszUZKbIb1uZqPLrlMk+q6ptfV1Dub9cUadTQMg7hIO7FRDmIj7cRE2GlwmY01may6THUNLk8fG5wm9U4XNptBZGMw7L6G7mvlvh42w31N3Ne22e/Nc80C8k/Sb93zX3SALVq0iN/85jfB7oaISMhz2G0kxQb5m0YkAHz6V5yamordbqewsLDF44WFhWRktL7kKSMjo9327qMv55w/fz6lpaWe24EDB3z5GCIiIhJmfApYIiMjGT9+PPn5+Z7HXC4X+fn5TJkypdXXTJkypUV7gHfffdfTPicnh4yMjBZtysrKWL16dZvnjIqKIjExscVNREREei6fp4TmzZvH7NmzmTBhApMmTWLp0qVUVlYyZ84cAK6//nr69evHokWLALjtttuYNm0af/jDH7jkkkt44YUXWLt2LY8//jhgzTvefvvt/Pa3vyU3N9ezrDkzM5MrrrgicJ9UREREwpbPAcs111zD0aNHWbBgAQUFBYwZM4bly5d7kmb379+Prdka7alTp/Lcc8/xq1/9il/84hfk5uby6quvemqwAPzsZz+jsrKSH/zgB5SUlHDuueeyfPlyr2qwiIiISM+n0vwiIiISFL58fyt1XEREREKeAhYREREJeQpYREREJOQpYBEREZGQp4BFREREQp4CFhEREQl5ClhEREQk5ClgERERkZAXlrs1n8xd+66srCzIPRERERFvub+3valh2yMClvLycgCysrKC3BMRERHxVXl5OUlJSe226RGl+V0uF4cPHyYhIQHDMAJ67rKyMrKysjhw4IDK/ncxXevuo2vdfXStu4+udfcJ1LU2TZPy8nIyMzNb7EPYmh4xwmKz2ejfv3+XvkdiYqL+A+gmutbdR9e6++hadx9d6+4TiGvd0ciKm5JuRUREJOQpYBEREZGQp4ClA1FRUSxcuJCoqKhgd6XH07XuPrrW3UfXuvvoWnefYFzrHpF0KyIiIj2bRlhEREQk5ClgERERkZCngEVERERCngIWERERCXkKWDrw6KOPkp2dTXR0NJMnT2bNmjXB7lJYW7RoERMnTiQhIYG0tDSuuOIKtm3b1qJNTU0Nt956K7179yY+Pp5vf/vbFBYWBqnHPcfixYsxDIPbb7/d85iudeAcOnSI7373u/Tu3ZuYmBjOOuss1q5d63neNE0WLFhA3759iYmJIS8vjx07dgSxx+HL6XRy9913k5OTQ0xMDIMHD+bee+9tsR+Nrrd//vOf/zBz5kwyMzMxDINXX321xfPeXNfjx48za9YsEhMTSU5O5sYbb6SioqLznTOlTS+88IIZGRlpPvnkk+bmzZvNm266yUxOTjYLCwuD3bWwNX36dPOvf/2r+eWXX5obN240L774YnPAgAFmRUWFp83NN99sZmVlmfn5+ebatWvNs88+25w6dWoQex3+1qxZY2ZnZ5ujRo0yb7vtNs/jutaBcfz4cXPgwIHmDTfcYK5evdrcvXu3+fbbb5s7d+70tFm8eLGZlJRkvvrqq+bnn39uXnbZZWZOTo5ZXV0dxJ6Hp/vuu8/s3bu3+frrr5t79uwxX3rpJTM+Pt586KGHPG10vf3z5ptvmr/85S/Nl19+2QTMV155pcXz3lzXGTNmmKNHjzZXrVplfvTRR+aQIUPM6667rtN9U8DSjkmTJpm33nqr52en02lmZmaaixYtCmKvepaioiITMD/88EPTNE2zpKTEjIiIMF966SVPm61bt5qAuXLlymB1M6yVl5ebubm55rvvvmtOmzbNE7DoWgfOz3/+c/Pcc89t83mXy2VmZGSYDzzwgOexkpISMyoqynz++ee7o4s9yiWXXGJ+//vfb/HYt771LXPWrFmmaep6B8rJAYs313XLli0mYH722WeeNm+99ZZpGIZ56NChTvVHU0JtqKurY926deTl5Xkes9ls5OXlsXLlyiD2rGcpLS0FoFevXgCsW7eO+vr6Ftd9+PDhDBgwQNfdT7feeiuXXHJJi2sKutaB9NprrzFhwgSuvvpq0tLSGDt2LE888YTn+T179lBQUNDiWiclJTF58mRdaz9MnTqV/Px8tm/fDsDnn3/Oxx9/zEUXXQToencVb67rypUrSU5OZsKECZ42eXl52Gw2Vq9e3an37xGbH3aF4uJinE4n6enpLR5PT0/nq6++ClKvehaXy8Xtt9/OOeecw8iRIwEoKCggMjKS5OTkFm3T09MpKCgIQi/D2wsvvMD69ev57LPPTnlO1zpwdu/ezWOPPca8efP4xS9+wWeffcZPfvITIiMjmT17tud6tvb3RNfad3fddRdlZWUMHz4cu92O0+nkvvvuY9asWQC63l3Em+taUFBAWlpai+cdDge9evXq9LVXwCJBc+utt/Lll1/y8ccfB7srPdKBAwe47bbbePfdd4mOjg52d3o0l8vFhAkT+N3vfgfA2LFj+fLLL1m2bBmzZ88Ocu96nr///e88++yzPPfcc5x55pls3LiR22+/nczMTF3vHkxTQm1ITU3FbrefsmKisLCQjIyMIPWq55g7dy6vv/46H3zwAf379/c8npGRQV1dHSUlJS3a67r7bt26dRQVFTFu3DgcDgcOh4MPP/yQP/3pTzgcDtLT03WtA6Rv376MGDGixWNnnHEG+/fvB/BcT/09CYw777yTu+66i2uvvZazzjqL733ve/z0pz9l0aJFgK53V/HmumZkZFBUVNTi+YaGBo4fP97pa6+ApQ2RkZGMHz+e/Px8z2Mul4v8/HymTJkSxJ6FN9M0mTt3Lq+88grvv/8+OTk5LZ4fP348ERERLa77tm3b2L9/v667jy688EK++OILNm7c6LlNmDCBWbNmee7rWgfGOeecc8ry/O3btzNw4EAAcnJyyMjIaHGty8rKWL16ta61H6qqqrDZWn592e12XC4XoOvdVby5rlOmTKGkpIR169Z52rz//vu4XC4mT57cuQ50KmW3h3vhhRfMqKgo86mnnjK3bNli/uAHPzCTk5PNgoKCYHctbN1yyy1mUlKSuWLFCvPIkSOeW1VVlafNzTffbA4YMMB8//33zbVr15pTpkwxp0yZEsRe9xzNVwmZpq51oKxZs8Z0OBzmfffdZ+7YscN89tlnzdjYWPNvf/ubp83ixYvN5ORk81//+pe5adMm8/LLL9cyWz/Nnj3b7Nevn2dZ88svv2ympqaaP/vZzzxtdL39U15ebm7YsMHcsGGDCZgPPviguWHDBnPfvn2maXp3XWfMmGGOHTvWXL16tfnxxx+bubm5WtbcHR5++GFzwIABZmRkpDlp0iRz1apVwe5SWANavf31r3/1tKmurjZ/9KMfmSkpKWZsbKx55ZVXmkeOHAlep3uQkwMWXevA+fe//22OHDnSjIqKMocPH24+/vjjLZ53uVzm3Xffbaanp5tRUVHmhRdeaG7bti1IvQ1vZWVl5m233WYOGDDAjI6ONgcNGmT+8pe/NGtraz1tdL3988EHH7T6N3r27NmmaXp3XY8dO2Zed911Znx8vJmYmGjOmTPHLC8v73TfDNNsVhpQREREJAQph0VERERCngIWERERCXkKWERERCTkKWARERGRkKeARUREREKeAhYREREJeQpYREREJOQpYBEREZGQp4BFREREQp4CFhEREQl5ClhEREQk5ClgERERkZD3/wEvem6AOQV74AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.plot(losses_train, label='train')\n",
    "plt.plot(losses_val, label='val')\n",
    "plt.legend()\n",
    "plt.savefig('loss_15_window_size.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_X_combined = np.concatenate((solar_X, solar_X_val), axis=0)   \n",
    "solar_y_combined = np.concatenate((solar_y, solar_y_val), axis=0)\n",
    "train_indices = [-1] * len(solar_X)\n",
    "val_indices = [0] * len(solar_X_val)\n",
    "test_fold = np.array(train_indices + val_indices)\n",
    "ps = PredefinedSplit(test_fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1458 candidates, totalling 1458 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [55], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(net, params, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cv\u001b[38;5;241m=\u001b[39mps, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Fit the Grid Search with combined data\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m gs\u001b[38;5;241m.\u001b[39mfit(solar_X_combined, solar_y_combined)\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skorch\\regressor.py:82\u001b[0m, in \u001b[0;36mNeuralNetRegressor.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(NeuralNetRegressor, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skorch\\net.py:1317\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m\"\"\"Initialize and fit the module.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \n\u001b[0;32m   1287\u001b[0m \u001b[38;5;124;03mIf the module was already initialized, by calling fit, the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \n\u001b[0;32m   1315\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n\u001b[1;32m-> 1317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skorch\\net.py:903\u001b[0m, in \u001b[0;36mNeuralNet.initialize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_virtual_params()\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_callbacks()\n\u001b[1;32m--> 903\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_criterion()\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_optimizer()\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skorch\\net.py:753\u001b[0m, in \u001b[0;36mNeuralNet._initialize_module\u001b[1;34m(self, reason)\u001b[0m\n\u001b[0;32m    751\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m--> 753\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_compile(module, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, module)\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skorch\\utils.py:202\u001b[0m, in \u001b[0;36mto_device\u001b[1;34m(X, device)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mdistribution\u001b[38;5;241m.\u001b[39mDistribution):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:217\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 217\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\belau\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    module=GRUNet,\n",
    "    module__input_dim=INPUT_SIZE,  # Set the input dimension\n",
    "    module__output_dim=OUTPUT_SIZE,  # Set the output dimension\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    device='cuda'  # Change this to 'cpu' if not using a GPU\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'module__hidden_dim': [16, 32, 64],\n",
    "    'module__n_layers': [1, 2, 3],\n",
    "    'module__drop_prob': [0.1, 0.2, 0.3],\n",
    "    'optimizer__lr': [0.001, 0.01, 0.1],\n",
    "    'optimizer__weight_decay': [0.0001, 0.001],\n",
    "    'max_epochs': [10, 20, 50],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'iterator_train__shuffle': [False],  # Set shuffle to False for time series data\n",
    "}\n",
    "\n",
    "# Using predefined split as before\n",
    "gs = GridSearchCV(net, params, refit=False, cv=ps, scoring='neg_mean_squared_error', verbose=2, error_score='raise')\n",
    "\n",
    "# Fit the Grid Search with combined data\n",
    "gs.fit(solar_X_combined, solar_y_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
