{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install skorch"
      ],
      "metadata": {
        "id": "5cNN0z0FVMF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6552632d-6d2d-44c7-db6e-e8e3a419a2aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0GpkgrKReH1u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import sys\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from skorch import NeuralNetRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import PredefinedSplit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iS4xpBIxePD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dd7cc3-f1c5-4573-81ee-9895d686e924"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/MyDrive/solar_data\"\n",
        "sys.path.insert(0, PATH)"
      ],
      "metadata": {
        "id": "fyLq5mNeeKsj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GRU_model import GRUNet"
      ],
      "metadata": {
        "id": "jjrAhm-PVHcD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sys.path)"
      ],
      "metadata": {
        "id": "A4NVcvVIfnqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65759405-f816-4b43-b0d9-e4556d1be1e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/solar_data', '/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rhNnPWz-eH1v"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OT-I8FVUeH1w"
      },
      "outputs": [],
      "source": [
        "#TODO make supervised learning dataset from csv files\n",
        "#TODO build dataloader\n",
        "solar_data = np.genfromtxt('/content/drive/MyDrive/solar_data/data/training_data/train_solar.csv', delimiter=',', skip_header=1)\n",
        "wind_data = np.genfromtxt('/content/drive/MyDrive/solar_data/data/training_data/train_wind.csv', delimiter=',',skip_header=1)\n",
        "solar_data_val = np.genfromtxt('/content/drive/MyDrive/solar_data/data/training_data/val_solar.csv', delimiter=',', skip_header=1)\n",
        "wind_data_val = np.genfromtxt('/content/drive/MyDrive/solar_data/data/training_data/val_wind.csv', delimiter=',', skip_header=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AZxZyVSHeH1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1f69f0-c486-4365-cf61-0587c1bff4a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17544, 11), (17544, 11))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "solar_data.shape, wind_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-5nHXZbEeH1w"
      },
      "outputs": [],
      "source": [
        "def move_sliding_window(data, window_size, inputs_cols_indices, label_col_index):\n",
        "    \"\"\"\n",
        "    data: numpy array including data\n",
        "    window_size: size of window\n",
        "    inputs_cols_indices: col indices to include\n",
        "    \"\"\"\n",
        "\n",
        "    # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
        "    inputs = np.zeros((len(data) - window_size, window_size, len(inputs_cols_indices)))\n",
        "    labels = np.zeros(len(data) - window_size)\n",
        "\n",
        "    for i in range(window_size, len(data)):\n",
        "        inputs[i - window_size] = data[i - window_size : i, inputs_cols_indices]\n",
        "        labels[i - window_size] = data[i, label_col_index]\n",
        "    inputs = inputs.reshape(-1, window_size, len(inputs_cols_indices))\n",
        "    labels = labels.reshape(-1, 1)\n",
        "    print(inputs.shape, labels.shape)\n",
        "\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24 hours ahead data for wind and solar supply\n",
        "def move_sliding_window_2(data, window_size, inputs_cols_indices, label_col_index, forecast_horizon=24):\n",
        "    \"\"\"\n",
        "    data: numpy array including data\n",
        "    window_size: size of window\n",
        "    inputs_cols_indices: col indices to include\n",
        "    label_col_index: index of the label column in data\n",
        "    forecast_horizon: number of time steps ahead to predict\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the number of instances based on the available data minus the window size and forecast horizon\n",
        "    num_instances = len(data) - window_size - forecast_horizon + 1\n",
        "\n",
        "    # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
        "    inputs = np.zeros((num_instances, window_size, len(inputs_cols_indices)))\n",
        "    labels = np.zeros(num_instances)\n",
        "\n",
        "    for i in range(num_instances):\n",
        "        inputs[i] = data[i:i + window_size, inputs_cols_indices]\n",
        "        labels[i] = data[i + window_size + forecast_horizon - 1, label_col_index]  # Label is forecast_horizon steps ahead\n",
        "    inputs = inputs.reshape(-1, window_size, len(inputs_cols_indices))\n",
        "    labels = labels.reshape(-1, 1)\n",
        "    print(inputs.shape, labels.shape)\n",
        "\n",
        "    return inputs, labels"
      ],
      "metadata": {
        "id": "bup_787cUiDu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataset\n",
        "class SolarDataset(Dataset):\n",
        "    def __init__(self, inputs, output):\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.output[idx]"
      ],
      "metadata": {
        "id": "PPS9ypQBUdIy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_datasets(train_X, train_y, val_X, val_y):\n",
        "  combined_X = np.concatenate((train_X, val_X), axis=0)\n",
        "  combined_y = np.concatenate((train_y, val_y), axis=0)\n",
        "\n",
        "  combined_X = torch.tensor(combined_X, dtype=torch.float32)\n",
        "  combined_y = torch.tensor(combined_y, dtype=torch.float32)\n",
        "\n",
        "  train_indices = [-1] * len(train_X)\n",
        "  val_indices = [0] * len(val_X)\n",
        "\n",
        "  test_fold = np.array(train_indices + val_indices)\n",
        "\n",
        "  return combined_X, combined_y, PredefinedSplit(test_fold)"
      ],
      "metadata": {
        "id": "Pz9gXmF9VSAE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kEO7eSkGeH1w",
        "outputId": "d38d43d5-b1b5-4487-a2b9-af7877fdbd2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17519, 25, 11) (17519, 1)\n",
            "(17519, 25, 11) (17519, 1)\n",
            "(8735, 25, 11) (8735, 1)\n",
            "(8735, 25, 11) (8735, 1)\n"
          ]
        }
      ],
      "source": [
        "solar_X, solar_y = move_sliding_window(solar_data, WINDOW_SIZE, range(11), 0)\n",
        "wind_X, wind_y = move_sliding_window(wind_data, WINDOW_SIZE, range(11), 0)\n",
        "solar_X_val, solar_y_val = move_sliding_window(solar_data_val, WINDOW_SIZE, range(11), 0)\n",
        "wind_X_val, wind_y_val = move_sliding_window(wind_data_val, WINDOW_SIZE, range(11), 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iDOjWa1aeH1x"
      },
      "outputs": [],
      "source": [
        "solar_dataset = SolarDataset(solar_X, solar_y)\n",
        "wind_dataset = SolarDataset(wind_X, wind_y)\n",
        "dataloader_solar = DataLoader(solar_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "dataloader_wind = DataLoader(wind_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ztB10vZYeH1x",
        "outputId": "65db1f1c-b3f9-44d5-8161-05f5a0f882ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8735, 25, 11) (8735, 1)\n"
          ]
        }
      ],
      "source": [
        "solar_dataset_val = SolarDataset(solar_X_val, solar_y_val)\n",
        "dataloader_solar_val = DataLoader(solar_dataset_val, batch_size=32, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wind_dataset_val = SolarDataset(wind_X_val, wind_y_val)\n",
        "dataloader_wind_val = DataLoader(wind_dataset_val, batch_size=32, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4lmmaZNq71q",
        "outputId": "5a8cecde-28bb-49a4-e1c8-e0c1edcfb6c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8735, 25, 11) (8735, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZnnQeHOeH1x",
        "outputId": "26c45ba6-868f-4f78-e5f8-5f0f429e9ede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "731.0\n"
          ]
        }
      ],
      "source": [
        "alpha = 2\n",
        "N_h = solar_data.shape[0] / (alpha * (11 + 1))\n",
        "print(N_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HBTBODIAeH1x"
      },
      "outputs": [],
      "source": [
        "INPUT_SIZE = solar_data.shape[1]\n",
        "HIDDEN_SIZE = 16\n",
        "NUM_LAYERS = 2\n",
        "OUTPUT_SIZE = 1\n",
        "DROP_PROB = .5\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xVOft_-leH1y",
        "outputId": "1444ab7e-2d88-4a06-d185-128950292427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRUNet(\n",
              "  (gru): GRU(11, 16, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GRUNet(input_dim=INPUT_SIZE, hidden_dim=HIDDEN_SIZE, output_dim=OUTPUT_SIZE, n_layers=NUM_LAYERS, drop_prob=DROP_PROB).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZOhWuJIUeH1z"
      },
      "outputs": [],
      "source": [
        "# Initialize PredefinedSplit\n",
        "solar_X_combined, solar_y_combined, ps = combine_datasets(solar_X, solar_y, solar_X_val, solar_y_val)\n",
        "wind_X_combined, wind_y_combined, ps_wind = combine_datasets(wind_X, wind_y, wind_X_val, wind_y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIE731LweH1z"
      },
      "outputs": [],
      "source": [
        "net = NeuralNetRegressor(\n",
        "    module=GRUNet,\n",
        "    module__input_dim=INPUT_SIZE,\n",
        "    module__output_dim=OUTPUT_SIZE,\n",
        "    criterion=torch.nn.MSELoss,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    device=device  # or 'cpu'\n",
        ")\n",
        "\n",
        "# Define the parameter grid\n",
        "params = {\n",
        "    'module__hidden_dim': [16, 32, 64],\n",
        "    'module__n_layers': [1, 2],\n",
        "    'module__drop_prob': [0.5],\n",
        "    'optimizer__lr': [0.001],\n",
        "    'optimizer__weight_decay': [0.0001, 0.001],\n",
        "    'max_epochs': [80],\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'iterator_train__shuffle': [False],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWwoHjXOeH1z",
        "outputId": "1dea1f47-a49f-4e9f-f699-4b91a7abb103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 36 candidates, totalling 36 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0948\u001b[0m        \u001b[32m0.2153\u001b[0m  2.2034\n",
            "      2        \u001b[36m0.0625\u001b[0m        \u001b[32m0.1325\u001b[0m  2.7707\n",
            "      3        \u001b[36m0.0562\u001b[0m        \u001b[32m0.1292\u001b[0m  2.4129\n",
            "      4        \u001b[36m0.0552\u001b[0m        \u001b[32m0.1264\u001b[0m  2.1479\n",
            "      5        \u001b[36m0.0546\u001b[0m        0.1275  2.1692\n",
            "      6        0.0548        0.1270  2.1876\n",
            "      7        0.0554        0.1287  2.1668\n",
            "      8        \u001b[36m0.0546\u001b[0m        0.1278  2.7786\n",
            "      9        \u001b[36m0.0545\u001b[0m        \u001b[32m0.1262\u001b[0m  2.3871\n",
            "     10        \u001b[36m0.0537\u001b[0m        \u001b[32m0.1248\u001b[0m  2.2096\n",
            "     11        \u001b[36m0.0531\u001b[0m        \u001b[32m0.1201\u001b[0m  2.1636\n",
            "     12        \u001b[36m0.0527\u001b[0m        \u001b[32m0.1174\u001b[0m  2.1666\n",
            "     13        \u001b[36m0.0517\u001b[0m        0.1176  2.2172\n",
            "     14        0.0522        0.1176  2.9174\n",
            "     15        0.0522        0.1196  3.1578\n",
            "     16        0.0543        \u001b[32m0.0994\u001b[0m  2.3695\n",
            "     17        \u001b[36m0.0510\u001b[0m        0.1067  2.1755\n",
            "     18        \u001b[36m0.0506\u001b[0m        \u001b[32m0.0791\u001b[0m  2.2104\n",
            "     19        \u001b[36m0.0485\u001b[0m        0.0814  2.1617\n",
            "     20        \u001b[36m0.0471\u001b[0m        \u001b[32m0.0763\u001b[0m  2.7307\n",
            "     21        \u001b[36m0.0454\u001b[0m        \u001b[32m0.0702\u001b[0m  2.4714\n",
            "     22        \u001b[36m0.0404\u001b[0m        \u001b[32m0.0348\u001b[0m  2.1543\n",
            "     23        \u001b[36m0.0235\u001b[0m        0.1106  2.1763\n",
            "     24        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0270\u001b[0m  2.1529\n",
            "     25        \u001b[36m0.0048\u001b[0m        0.0279  2.1421\n",
            "     26        \u001b[36m0.0029\u001b[0m        \u001b[32m0.0099\u001b[0m  2.7336\n",
            "     27        \u001b[36m0.0022\u001b[0m        \u001b[32m0.0052\u001b[0m  2.4742\n",
            "     28        0.0027        0.0296  2.1462\n",
            "     29        0.0039        0.0244  2.1307\n",
            "     30        0.0026        0.0182  2.1372\n",
            "     31        \u001b[36m0.0021\u001b[0m        0.0202  2.2011\n",
            "     32        \u001b[36m0.0020\u001b[0m        0.0255  2.7675\n",
            "     33        0.0021        0.0228  2.4812\n",
            "     34        0.0021        0.0267  2.1826\n",
            "     35        0.0023        0.0330  2.1690\n",
            "     36        0.0032        0.0288  2.1918\n",
            "     37        0.0026        0.0171  2.2051\n",
            "     38        0.0031        0.0340  2.7914\n",
            "     39        0.0029        0.0211  2.4153\n",
            "     40        0.0024        0.0080  2.1579\n",
            "     41        0.0021        0.0184  2.1442\n",
            "     42        0.0025        0.0232  2.1448\n",
            "     43        0.0023        0.0130  2.1479\n",
            "     44        0.0021        0.0109  2.8377\n",
            "     45        0.0023        0.0193  2.4643\n",
            "     46        0.0021        0.0088  2.1545\n",
            "     47        \u001b[36m0.0019\u001b[0m        0.0090  2.1455\n",
            "     48        \u001b[36m0.0019\u001b[0m        0.0108  2.1375\n",
            "     49        0.0032        0.0077  2.1413\n",
            "     50        0.0024        0.0127  2.7588\n",
            "     51        \u001b[36m0.0018\u001b[0m        0.0127  2.3757\n",
            "     52        \u001b[36m0.0017\u001b[0m        0.0072  2.1566\n",
            "     53        0.0019        0.0108  2.1751\n",
            "     54        0.0051        0.1827  2.1473\n",
            "     55        0.0114        0.0126  2.2358\n",
            "     56        0.0020        0.0112  2.7817\n",
            "     57        \u001b[36m0.0013\u001b[0m        \u001b[32m0.0048\u001b[0m  2.3733\n",
            "     58        0.0017        0.0057  2.2137\n",
            "     59        0.0026        0.0590  2.1747\n",
            "     60        0.0023        0.0244  2.1500\n",
            "     61        0.0019        0.0256  2.3103\n",
            "     62        0.0016        0.0210  2.8085\n",
            "     63        0.0017        0.0121  2.2738\n",
            "     64        0.0022        0.0303  2.1663\n",
            "     65        0.0019        \u001b[32m0.0041\u001b[0m  2.1341\n",
            "     66        0.0017        0.0085  2.1441\n",
            "     67        0.0018        0.0104  2.2974\n",
            "     68        0.0017        0.0116  2.8142\n",
            "     69        0.0035        0.0181  2.2869\n",
            "     70        0.0029        0.0127  2.2157\n",
            "     71        0.0028        \u001b[32m0.0025\u001b[0m  2.1595\n",
            "     72        0.0020        \u001b[32m0.0015\u001b[0m  2.1637\n",
            "     73        0.0019        0.0032  2.3313\n",
            "     74        0.0020        0.0042  2.8234\n",
            "     75        0.0023        0.0075  2.2055\n",
            "     76        0.0017        0.0028  2.1610\n",
            "     77        0.0018        0.0102  2.1742\n",
            "     78        0.0018        0.0062  2.1789\n",
            "     79        0.0019        0.0076  2.3914\n",
            "     80        0.0022        0.0172  2.8242\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.1min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0801\u001b[0m        \u001b[32m0.3229\u001b[0m  2.1480\n",
            "      2        0.0820        \u001b[32m0.3056\u001b[0m  2.1755\n",
            "      3        \u001b[36m0.0751\u001b[0m        \u001b[32m0.2010\u001b[0m  2.1739\n",
            "      4        \u001b[36m0.0560\u001b[0m        \u001b[32m0.1478\u001b[0m  2.1355\n",
            "      5        \u001b[36m0.0539\u001b[0m        \u001b[32m0.1453\u001b[0m  2.5364\n",
            "      6        0.0549        \u001b[32m0.1418\u001b[0m  2.6143\n",
            "      7        0.0581        0.1619  2.1697\n",
            "      8        0.0582        0.1738  2.1712\n",
            "      9        0.0578        0.1771  2.1055\n",
            "     10        0.0571        0.1740  2.1321\n",
            "     11        0.0568        0.1770  2.5988\n",
            "     12        0.0557        0.1708  2.6144\n",
            "     13        0.0571        0.1727  2.1291\n",
            "     14        0.0574        0.1719  2.1241\n",
            "     15        0.0574        0.1707  2.1755\n",
            "     16        0.0575        0.1740  2.2694\n",
            "     17        0.0570        0.1675  2.5893\n",
            "     18        0.0567        0.1829  2.5743\n",
            "     19        0.0570        0.1825  2.1421\n",
            "     20        0.0594        0.1820  2.1328\n",
            "     21        0.0565        0.1788  2.1505\n",
            "     22        0.0573        0.1781  2.1539\n",
            "     23        0.0575        0.1771  2.5644\n",
            "     24        0.0576        0.1750  2.6389\n",
            "     25        0.0576        0.1749  2.1637\n",
            "     26        0.0572        0.1702  2.1359\n",
            "     27        0.0565        0.1712  2.1493\n",
            "     28        0.0565        0.1690  2.1653\n",
            "     29        0.0566        0.1704  2.6184\n",
            "     30        0.0575        0.1668  2.5972\n",
            "     31        0.0576        0.1673  2.1416\n",
            "     32        0.0577        0.1687  2.1395\n",
            "     33        0.0579        0.1695  2.1671\n",
            "     34        0.0585        0.1674  2.1485\n",
            "     35        0.0579        0.1700  2.5655\n",
            "     36        0.0582        0.1675  2.5493\n",
            "     37        0.0582        0.1678  2.1375\n",
            "     38        0.0580        0.1659  2.4661\n",
            "     39        0.0596        0.2382  2.7544\n",
            "     40        0.0635        0.1768  2.2423\n",
            "     41        0.0592        0.1743  2.8400\n",
            "     42        0.0590        0.1739  2.3575\n",
            "     43        0.0590        0.1747  2.1353\n",
            "     44        0.0594        0.1767  2.1343\n",
            "     45        0.0595        0.1744  2.1615\n",
            "     46        0.0600        0.1753  2.2297\n",
            "     47        0.0592        0.1686  2.7736\n",
            "     48        0.0582        0.1701  2.2645\n",
            "     49        0.0590        0.1662  2.1387\n",
            "     50        0.0599        0.2411  2.1682\n",
            "     51        0.0645        0.2402  2.1412\n",
            "     52        0.0647        0.2403  2.2596\n",
            "     53        0.0645        0.2421  2.7623\n",
            "     54        0.0651        0.2416  2.3095\n",
            "     55        0.0646        0.2426  2.1418\n",
            "     56        0.0646        0.2428  2.1574\n",
            "     57        0.0647        0.2434  2.1576\n",
            "     58        0.0646        0.2484  2.2772\n",
            "     59        0.0660        0.1673  2.8205\n",
            "     60        0.0599        0.2377  2.2780\n",
            "     61        0.0671        0.1669  2.1367\n",
            "     62        0.0603        0.2396  2.1502\n",
            "     63        0.0649        0.2386  2.1850\n",
            "     64        0.0650        0.2395  2.3111\n",
            "     65        0.0645        0.2377  2.7655\n",
            "     66        0.0643        0.2416  2.2265\n",
            "     67        0.0638        0.2396  2.2093\n",
            "     68        0.0654        0.2402  2.2098\n",
            "     69        0.0641        0.2406  2.3180\n",
            "     70        0.0651        0.2401  2.3391\n",
            "     71        0.0640        0.2422  2.8715\n",
            "     72        0.0640        0.2425  2.1871\n",
            "     73        0.0640        0.2431  2.1593\n",
            "     74        0.0640        0.2436  2.1455\n",
            "     75        0.0634        0.2439  2.1504\n",
            "     76        0.0639        0.2439  2.3819\n",
            "     77        0.0641        0.2433  2.7812\n",
            "     78        0.0641        0.2446  2.1338\n",
            "     79        0.0646        0.2434  2.1770\n",
            "     80        0.0641        0.2439  2.2015\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0755\u001b[0m        \u001b[32m0.2697\u001b[0m  2.3275\n",
            "      2        \u001b[36m0.0668\u001b[0m        0.2927  2.8545\n",
            "      3        \u001b[36m0.0616\u001b[0m        \u001b[32m0.2388\u001b[0m  2.6985\n",
            "      4        0.0671        0.2933  2.3513\n",
            "      5        \u001b[36m0.0585\u001b[0m        0.2958  2.3372\n",
            "      6        0.0601        0.2933  2.3400\n",
            "      7        \u001b[36m0.0578\u001b[0m        0.2778  2.5793\n",
            "      8        \u001b[36m0.0563\u001b[0m        0.2895  3.0954\n",
            "      9        \u001b[36m0.0557\u001b[0m        0.2730  2.3796\n",
            "     10        \u001b[36m0.0509\u001b[0m        \u001b[32m0.2343\u001b[0m  2.3740\n",
            "     11        \u001b[36m0.0502\u001b[0m        \u001b[32m0.1896\u001b[0m  2.3244\n",
            "     12        \u001b[36m0.0481\u001b[0m        \u001b[32m0.1803\u001b[0m  2.3746\n",
            "     13        \u001b[36m0.0475\u001b[0m        \u001b[32m0.1276\u001b[0m  2.9539\n",
            "     14        \u001b[36m0.0447\u001b[0m        \u001b[32m0.1171\u001b[0m  2.6894\n",
            "     15        \u001b[36m0.0424\u001b[0m        0.1342  2.3242\n",
            "     16        \u001b[36m0.0417\u001b[0m        0.1349  2.3382\n",
            "     17        \u001b[36m0.0390\u001b[0m        0.1248  2.3222\n",
            "     18        \u001b[36m0.0389\u001b[0m        0.1340  2.4905\n",
            "     19        \u001b[36m0.0377\u001b[0m        0.1412  3.0584\n",
            "     20        \u001b[36m0.0355\u001b[0m        0.1295  2.3501\n",
            "     21        0.0358        0.1263  2.3044\n",
            "     22        \u001b[36m0.0353\u001b[0m        \u001b[32m0.0873\u001b[0m  2.3446\n",
            "     23        0.0358        0.1044  2.2915\n",
            "     24        0.0360        \u001b[32m0.0818\u001b[0m  2.8652\n",
            "     25        \u001b[36m0.0339\u001b[0m        0.0965  2.6513\n",
            "     26        \u001b[36m0.0316\u001b[0m        \u001b[32m0.0717\u001b[0m  2.3186\n",
            "     27        \u001b[36m0.0291\u001b[0m        \u001b[32m0.0553\u001b[0m  2.3083\n",
            "     28        \u001b[36m0.0275\u001b[0m        \u001b[32m0.0474\u001b[0m  2.3176\n",
            "     29        \u001b[36m0.0263\u001b[0m        \u001b[32m0.0383\u001b[0m  2.5116\n",
            "     30        \u001b[36m0.0252\u001b[0m        \u001b[32m0.0333\u001b[0m  3.0111\n",
            "     31        \u001b[36m0.0242\u001b[0m        \u001b[32m0.0247\u001b[0m  2.3291\n",
            "     32        \u001b[36m0.0219\u001b[0m        \u001b[32m0.0244\u001b[0m  2.3411\n",
            "     33        0.0221        0.0271  2.3115\n",
            "     34        \u001b[36m0.0207\u001b[0m        0.0252  2.3100\n",
            "     35        \u001b[36m0.0201\u001b[0m        \u001b[32m0.0185\u001b[0m  2.8374\n",
            "     36        \u001b[36m0.0190\u001b[0m        \u001b[32m0.0173\u001b[0m  2.7790\n",
            "     37        \u001b[36m0.0188\u001b[0m        \u001b[32m0.0151\u001b[0m  2.3166\n",
            "     38        \u001b[36m0.0181\u001b[0m        \u001b[32m0.0147\u001b[0m  2.3294\n",
            "     39        \u001b[36m0.0179\u001b[0m        \u001b[32m0.0128\u001b[0m  2.3225\n",
            "     40        \u001b[36m0.0176\u001b[0m        \u001b[32m0.0116\u001b[0m  2.4845\n",
            "     41        \u001b[36m0.0167\u001b[0m        0.0132  3.0400\n",
            "     42        0.0175        0.0124  2.3146\n",
            "     43        0.0175        \u001b[32m0.0114\u001b[0m  2.2847\n",
            "     44        \u001b[36m0.0166\u001b[0m        0.0128  2.3142\n",
            "     45        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0112\u001b[0m  2.3289\n",
            "     46        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0100\u001b[0m  2.8679\n",
            "     47        0.0160        0.0109  2.6611\n",
            "     48        0.0161        0.0134  2.4056\n",
            "     49        \u001b[36m0.0154\u001b[0m        0.0118  2.3131\n",
            "     50        0.0162        0.0132  2.3137\n",
            "     51        \u001b[36m0.0151\u001b[0m        0.0123  2.5429\n",
            "     52        0.0153        0.0104  3.0162\n",
            "     53        0.0152        0.0116  2.3236\n",
            "     54        \u001b[36m0.0150\u001b[0m        0.0108  2.3416\n",
            "     55        \u001b[36m0.0148\u001b[0m        0.0118  2.3634\n",
            "     56        0.0151        \u001b[32m0.0094\u001b[0m  2.3255\n",
            "     57        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0089\u001b[0m  3.1507\n",
            "     58        \u001b[36m0.0143\u001b[0m        0.0108  3.3708\n",
            "     59        \u001b[36m0.0137\u001b[0m        0.0106  2.5508\n",
            "     60        0.0141        \u001b[32m0.0079\u001b[0m  2.3735\n",
            "     61        \u001b[36m0.0136\u001b[0m        0.0097  2.3349\n",
            "     62        0.0146        0.0131  2.3184\n",
            "     63        0.0145        0.0111  2.7086\n",
            "     64        0.0145        0.0122  2.7776\n",
            "     65        0.0146        0.0115  2.3417\n",
            "     66        0.0138        0.0123  2.3212\n",
            "     67        \u001b[36m0.0134\u001b[0m        0.0104  2.3303\n",
            "     68        0.0139        0.0118  2.4250\n",
            "     69        0.0143        0.0166  3.0102\n",
            "     70        \u001b[36m0.0130\u001b[0m        0.0142  2.4317\n",
            "     71        0.0142        0.0181  2.3009\n",
            "     72        0.0134        0.0152  2.3487\n",
            "     73        0.0131        0.0154  2.2914\n",
            "     74        0.0134        0.0238  2.7118\n",
            "     75        0.0143        0.0187  2.8301\n",
            "     76        \u001b[36m0.0128\u001b[0m        0.0201  2.3461\n",
            "     77        0.0139        0.0153  2.3368\n",
            "     78        \u001b[36m0.0128\u001b[0m        0.0265  2.2943\n",
            "     79        0.0128        0.0246  2.3760\n",
            "     80        0.0130        0.0245  3.0112\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.4min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0859\u001b[0m        \u001b[32m0.2906\u001b[0m  2.2931\n",
            "      2        \u001b[36m0.0714\u001b[0m        \u001b[32m0.2732\u001b[0m  2.2842\n",
            "      3        \u001b[36m0.0668\u001b[0m        0.2960  2.3128\n",
            "      4        0.0670        0.3040  2.3795\n",
            "      5        \u001b[36m0.0623\u001b[0m        0.2948  2.8211\n",
            "      6        0.0654        0.2999  2.7420\n",
            "      7        0.0679        0.2946  2.3041\n",
            "      8        0.0790        0.3029  2.3273\n",
            "      9        \u001b[36m0.0597\u001b[0m        \u001b[32m0.2720\u001b[0m  2.3343\n",
            "     10        0.0813        0.3111  2.4557\n",
            "     11        \u001b[36m0.0590\u001b[0m        0.2730  3.0425\n",
            "     12        0.0907        0.2959  2.3174\n",
            "     13        0.0960        0.3021  2.3045\n",
            "     14        0.0970        0.3069  2.3301\n",
            "     15        0.0973        0.3047  2.3092\n",
            "     16        0.0980        0.3007  2.7992\n",
            "     17        0.0949        0.2958  2.7527\n",
            "     18        0.0793        \u001b[32m0.2535\u001b[0m  2.2945\n",
            "     19        0.0723        \u001b[32m0.2420\u001b[0m  2.3260\n",
            "     20        0.0711        0.2433  2.3084\n",
            "     21        0.0734        \u001b[32m0.2298\u001b[0m  2.3974\n",
            "     22        0.0706        \u001b[32m0.2273\u001b[0m  3.0569\n",
            "     23        0.0714        \u001b[32m0.2172\u001b[0m  2.3964\n",
            "     24        0.0717        0.2189  2.3176\n",
            "     25        0.0728        \u001b[32m0.1751\u001b[0m  2.3048\n",
            "     26        0.0726        0.1821  2.3034\n",
            "     27        0.0684        0.2052  2.7293\n",
            "     28        0.0701        0.1913  2.8757\n",
            "     29        0.0729        0.2586  2.3093\n",
            "     30        0.0742        0.1830  2.3043\n",
            "     31        0.0719        \u001b[32m0.1613\u001b[0m  2.3411\n",
            "     32        0.0715        0.1708  2.4257\n",
            "     33        0.0752        0.1769  3.0308\n",
            "     34        0.0738        0.3141  2.3727\n",
            "     35        0.0732        0.1913  2.3171\n",
            "     36        0.0712        0.1956  2.3288\n",
            "     37        0.0688        0.3018  2.2870\n",
            "     38        0.0733        0.2921  2.7501\n",
            "     39        0.0756        0.2213  2.7756\n",
            "     40        0.0736        0.2408  2.3361\n",
            "     41        0.0731        0.2809  2.3325\n",
            "     42        0.0763        0.2482  2.3038\n",
            "     43        0.0774        0.2409  2.4019\n",
            "     44        0.0768        0.2386  3.0522\n",
            "     45        0.0740        0.2150  2.4282\n",
            "     46        0.0740        0.2754  2.3304\n",
            "     47        0.0663        0.2813  2.3151\n",
            "     48        0.0693        0.2705  2.3160\n",
            "     49        0.0634        0.2736  2.7101\n",
            "     50        0.0622        0.2615  2.8280\n",
            "     51        0.0671        0.3074  2.3602\n",
            "     52        0.0658        0.3039  2.4266\n",
            "     53        \u001b[36m0.0587\u001b[0m        \u001b[32m0.1441\u001b[0m  2.3558\n",
            "     54        \u001b[36m0.0520\u001b[0m        \u001b[32m0.1418\u001b[0m  2.5046\n",
            "     55        \u001b[36m0.0498\u001b[0m        0.2101  3.0611\n",
            "     56        0.0601        0.2923  2.2977\n",
            "     57        0.0587        0.2482  2.3024\n",
            "     58        0.0533        0.2022  2.3151\n",
            "     59        0.0544        0.1486  2.3374\n",
            "     60        0.0565        0.1892  2.8010\n",
            "     61        0.0520        0.1694  2.7044\n",
            "     62        0.0498        \u001b[32m0.1347\u001b[0m  2.3195\n",
            "     63        0.0500        0.1527  2.2966\n",
            "     64        \u001b[36m0.0483\u001b[0m        \u001b[32m0.1089\u001b[0m  2.3589\n",
            "     65        \u001b[36m0.0467\u001b[0m        0.1244  2.4722\n",
            "     66        0.0481        0.1103  3.0406\n",
            "     67        0.0468        0.1297  2.3297\n",
            "     68        \u001b[36m0.0465\u001b[0m        \u001b[32m0.1025\u001b[0m  2.3050\n",
            "     69        \u001b[36m0.0449\u001b[0m        \u001b[32m0.0893\u001b[0m  2.3204\n",
            "     70        0.0450        0.1172  2.2869\n",
            "     71        0.0465        0.1111  2.7885\n",
            "     72        \u001b[36m0.0442\u001b[0m        0.1282  2.7291\n",
            "     73        \u001b[36m0.0438\u001b[0m        0.1373  2.9091\n",
            "     74        0.0451        0.1153  2.5926\n",
            "     75        0.0449        0.1553  2.3080\n",
            "     76        \u001b[36m0.0424\u001b[0m        0.1600  2.7319\n",
            "     77        \u001b[36m0.0413\u001b[0m        0.1490  2.8617\n",
            "     78        0.0430        0.1461  2.3044\n",
            "     79        \u001b[36m0.0397\u001b[0m        0.1370  2.3308\n",
            "     80        0.0413        0.1378  2.3418\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0695\u001b[0m        \u001b[32m0.1772\u001b[0m  2.2672\n",
            "      2        \u001b[36m0.0574\u001b[0m        0.2018  2.7941\n",
            "      3        \u001b[36m0.0566\u001b[0m        \u001b[32m0.1620\u001b[0m  2.2234\n",
            "      4        \u001b[36m0.0550\u001b[0m        0.2003  2.1385\n",
            "      5        \u001b[36m0.0543\u001b[0m        0.1797  2.1362\n",
            "      6        \u001b[36m0.0523\u001b[0m        0.1782  2.1423\n",
            "      7        0.0526        0.1681  2.3066\n",
            "      8        \u001b[36m0.0517\u001b[0m        \u001b[32m0.1599\u001b[0m  2.8458\n",
            "      9        0.0517        0.1617  2.1487\n",
            "     10        \u001b[36m0.0510\u001b[0m        \u001b[32m0.1400\u001b[0m  2.1121\n",
            "     11        0.0538        0.1569  2.1112\n",
            "     12        0.0515        0.1561  2.1349\n",
            "     13        0.0519        0.1595  2.3102\n",
            "     14        0.0537        0.1679  2.7981\n",
            "     15        0.0546        0.1673  2.1861\n",
            "     16        0.0526        0.1621  2.1673\n",
            "     17        0.0517        0.1483  2.1480\n",
            "     18        \u001b[36m0.0500\u001b[0m        \u001b[32m0.1392\u001b[0m  2.1528\n",
            "     19        \u001b[36m0.0494\u001b[0m        \u001b[32m0.1383\u001b[0m  2.3267\n",
            "     20        0.0526        0.1404  2.8064\n",
            "     21        0.0521        0.1530  2.1957\n",
            "     22        0.0498        0.1501  2.1523\n",
            "     23        0.0499        0.1419  2.1385\n",
            "     24        0.0500        0.1666  2.1203\n",
            "     25        0.0510        \u001b[32m0.1257\u001b[0m  2.3338\n",
            "     26        \u001b[36m0.0486\u001b[0m        \u001b[32m0.1153\u001b[0m  2.7520\n",
            "     27        0.0496        0.1587  2.1236\n",
            "     28        0.0503        0.1264  2.1331\n",
            "     29        0.0497        \u001b[32m0.1149\u001b[0m  2.1925\n",
            "     30        \u001b[36m0.0483\u001b[0m        0.1177  2.1818\n",
            "     31        \u001b[36m0.0479\u001b[0m        0.1527  2.4447\n",
            "     32        \u001b[36m0.0430\u001b[0m        \u001b[32m0.0946\u001b[0m  2.7058\n",
            "     33        \u001b[36m0.0255\u001b[0m        \u001b[32m0.0541\u001b[0m  2.1446\n",
            "     34        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0375\u001b[0m  2.1442\n",
            "     35        \u001b[36m0.0073\u001b[0m        \u001b[32m0.0287\u001b[0m  2.1552\n",
            "     36        \u001b[36m0.0067\u001b[0m        \u001b[32m0.0235\u001b[0m  2.1507\n",
            "     37        \u001b[36m0.0066\u001b[0m        \u001b[32m0.0204\u001b[0m  2.4444\n",
            "     38        \u001b[36m0.0054\u001b[0m        \u001b[32m0.0198\u001b[0m  2.7493\n",
            "     39        \u001b[36m0.0047\u001b[0m        \u001b[32m0.0149\u001b[0m  2.1258\n",
            "     40        \u001b[36m0.0046\u001b[0m        \u001b[32m0.0085\u001b[0m  2.1186\n",
            "     41        \u001b[36m0.0039\u001b[0m        \u001b[32m0.0067\u001b[0m  2.1601\n",
            "     42        \u001b[36m0.0039\u001b[0m        \u001b[32m0.0061\u001b[0m  2.1509\n",
            "     43        \u001b[36m0.0033\u001b[0m        \u001b[32m0.0053\u001b[0m  2.4466\n",
            "     44        \u001b[36m0.0029\u001b[0m        \u001b[32m0.0053\u001b[0m  2.7417\n",
            "     45        0.0032        0.0091  2.1231\n",
            "     46        \u001b[36m0.0023\u001b[0m        0.0146  2.1377\n",
            "     47        0.0029        0.0159  2.1350\n",
            "     48        0.0027        0.0124  2.1739\n",
            "     49        0.0027        0.0073  2.4247\n",
            "     50        0.0028        0.0111  2.7612\n",
            "     51        0.0031        0.0067  2.1473\n",
            "     52        0.0034        0.0069  2.1627\n",
            "     53        0.0026        \u001b[32m0.0050\u001b[0m  2.1300\n",
            "     54        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0031\u001b[0m  2.7130\n",
            "     55        0.0020        0.0044  2.5876\n",
            "     56        0.0022        0.0040  2.6439\n",
            "     57        0.0021        0.0040  2.1343\n",
            "     58        0.0026        0.0059  2.1360\n",
            "     59        0.0021        0.0035  2.1435\n",
            "     60        \u001b[36m0.0019\u001b[0m        0.0037  2.2057\n",
            "     61        \u001b[36m0.0019\u001b[0m        0.0046  2.6208\n",
            "     62        0.0024        0.0081  2.5755\n",
            "     63        0.0020        0.0040  2.1560\n",
            "     64        0.0035        0.0086  2.1574\n",
            "     65        0.0030        0.0038  2.1281\n",
            "     66        0.0020        0.0047  2.1532\n",
            "     67        \u001b[36m0.0019\u001b[0m        0.0067  2.6019\n",
            "     68        \u001b[36m0.0018\u001b[0m        0.0077  2.5418\n",
            "     69        0.0024        0.0074  2.1542\n",
            "     70        \u001b[36m0.0017\u001b[0m        0.0062  2.1490\n",
            "     71        0.0019        0.0092  2.1514\n",
            "     72        0.0023        0.0108  2.1184\n",
            "     73        0.0017        0.0069  2.6374\n",
            "     74        \u001b[36m0.0015\u001b[0m        0.0058  2.5471\n",
            "     75        0.0015        0.0100  2.0910\n",
            "     76        0.0017        0.0075  2.1504\n",
            "     77        0.0016        0.0095  2.1607\n",
            "     78        0.0022        0.0139  2.1354\n",
            "     79        0.0022        0.0150  2.6091\n",
            "     80        \u001b[36m0.0015\u001b[0m        0.0044  2.5497\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.1min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0749\u001b[0m        \u001b[32m0.2268\u001b[0m  2.1339\n",
            "      2        \u001b[36m0.0569\u001b[0m        \u001b[32m0.1551\u001b[0m  2.1695\n",
            "      3        \u001b[36m0.0563\u001b[0m        0.1568  2.1380\n",
            "      4        \u001b[36m0.0544\u001b[0m        0.1667  2.1448\n",
            "      5        0.0550        0.1996  2.7573\n",
            "      6        \u001b[36m0.0543\u001b[0m        0.1611  2.3541\n",
            "      7        \u001b[36m0.0541\u001b[0m        0.1650  2.1591\n",
            "      8        \u001b[36m0.0537\u001b[0m        0.1730  2.1530\n",
            "      9        0.0551        0.1715  2.1329\n",
            "     10        0.0544        0.1798  2.2467\n",
            "     11        0.0543        0.1648  2.7710\n",
            "     12        0.0552        0.1806  2.3129\n",
            "     13        0.0545        0.1927  2.1404\n",
            "     14        0.0554        0.1767  2.1457\n",
            "     15        0.0552        0.1797  2.5365\n",
            "     16        0.0543        0.1752  2.9351\n",
            "     17        0.0550        0.1811  2.8114\n",
            "     18        0.0554        0.1836  2.1450\n",
            "     19        0.0555        0.1832  2.1554\n",
            "     20        0.0584        0.2586  2.1588\n",
            "     21        0.0607        0.1898  2.1280\n",
            "     22        0.0577        0.2418  2.3528\n",
            "     23        0.0594        0.1922  2.8321\n",
            "     24        0.0579        0.2924  2.1610\n",
            "     25        0.0622        0.2599  2.2012\n",
            "     26        0.0610        0.2584  2.1478\n",
            "     27        0.0613        0.2595  2.1419\n",
            "     28        0.0628        0.2607  2.4075\n",
            "     29        0.0613        0.2603  2.7413\n",
            "     30        0.0608        0.2613  2.1370\n",
            "     31        0.0606        0.2608  2.1716\n",
            "     32        0.0612        0.2589  2.1236\n",
            "     33        0.0606        0.2592  2.1370\n",
            "     34        0.0606        0.2573  2.4248\n",
            "     35        0.0627        0.2549  2.7265\n",
            "     36        0.0610        0.2558  2.1746\n",
            "     37        0.0607        0.2569  2.1459\n",
            "     38        0.0606        0.2563  2.1503\n",
            "     39        0.0606        0.2567  2.1345\n",
            "     40        0.0606        0.2565  2.4833\n",
            "     41        0.0608        0.2554  2.7373\n",
            "     42        0.0623        0.2535  2.1463\n",
            "     43        0.0609        0.2534  2.1301\n",
            "     44        0.0605        0.2542  2.1325\n",
            "     45        0.0608        0.2543  2.2023\n",
            "     46        0.0609        0.2540  2.4613\n",
            "     47        0.0604        0.2486  2.7025\n",
            "     48        0.0608        0.2450  2.1437\n",
            "     49        0.0605        0.2560  2.1664\n",
            "     50        0.0605        0.2567  2.1426\n",
            "     51        0.0608        0.2553  2.1388\n",
            "     52        0.0607        0.2539  2.4617\n",
            "     53        0.0606        0.2543  2.7744\n",
            "     54        0.0609        0.2471  2.1558\n",
            "     55        0.0607        0.2507  2.1749\n",
            "     56        0.0609        0.2446  2.1310\n",
            "     57        0.0606        0.2518  2.1285\n",
            "     58        0.0607        0.2512  2.4646\n",
            "     59        0.0615        0.2452  2.7065\n",
            "     60        0.0603        0.2523  2.1229\n",
            "     61        0.0607        0.2524  2.1680\n",
            "     62        0.0605        0.2549  2.1782\n",
            "     63        0.0604        0.2554  2.1656\n",
            "     64        0.0607        0.2530  2.4030\n",
            "     65        0.0607        0.2526  2.7369\n",
            "     66        0.0604        0.2527  2.1526\n",
            "     67        0.0606        0.2536  2.1738\n",
            "     68        0.0606        0.2535  2.1637\n",
            "     69        0.0606        0.2546  2.1776\n",
            "     70        0.0606        0.2545  2.4881\n",
            "     71        0.0607        0.2527  2.7498\n",
            "     72        0.0611        0.2501  2.1748\n",
            "     73        0.0608        0.2517  2.1332\n",
            "     74        0.0607        0.2526  2.1673\n",
            "     75        0.0610        0.2521  2.1395\n",
            "     76        0.0605        0.2519  2.4644\n",
            "     77        0.0608        0.2500  2.7150\n",
            "     78        0.0607        0.2491  2.1377\n",
            "     79        0.0605        0.2501  2.1225\n",
            "     80        0.0613        0.2487  2.1352\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0726\u001b[0m        \u001b[32m0.2738\u001b[0m  2.3285\n",
            "      2        \u001b[36m0.0549\u001b[0m        0.3023  2.9003\n",
            "      3        \u001b[36m0.0547\u001b[0m        0.2939  2.6867\n",
            "      4        \u001b[36m0.0507\u001b[0m        0.2772  2.3270\n",
            "      5        0.0516        \u001b[32m0.2164\u001b[0m  2.3168\n",
            "      6        \u001b[36m0.0462\u001b[0m        0.2302  2.3113\n",
            "      7        0.0470        \u001b[32m0.1947\u001b[0m  2.5350\n",
            "      8        0.0483        \u001b[32m0.1740\u001b[0m  3.0596\n",
            "      9        0.0471        \u001b[32m0.1524\u001b[0m  2.3188\n",
            "     10        0.0496        0.1571  2.3173\n",
            "     11        \u001b[36m0.0443\u001b[0m        \u001b[32m0.1328\u001b[0m  2.3049\n",
            "     12        \u001b[36m0.0431\u001b[0m        \u001b[32m0.1114\u001b[0m  2.3283\n",
            "     13        \u001b[36m0.0423\u001b[0m        0.1286  2.8211\n",
            "     14        \u001b[36m0.0420\u001b[0m        \u001b[32m0.1090\u001b[0m  2.7313\n",
            "     15        \u001b[36m0.0417\u001b[0m        0.1140  2.3428\n",
            "     16        0.0418        0.1163  2.3270\n",
            "     17        \u001b[36m0.0399\u001b[0m        \u001b[32m0.0943\u001b[0m  2.3491\n",
            "     18        \u001b[36m0.0384\u001b[0m        \u001b[32m0.0880\u001b[0m  2.4581\n",
            "     19        \u001b[36m0.0380\u001b[0m        0.1093  3.1149\n",
            "     20        \u001b[36m0.0360\u001b[0m        0.0929  2.3203\n",
            "     21        0.0362        0.1721  2.2968\n",
            "     22        0.0373        \u001b[32m0.0821\u001b[0m  2.2952\n",
            "     23        \u001b[36m0.0353\u001b[0m        \u001b[32m0.0729\u001b[0m  2.3160\n",
            "     24        \u001b[36m0.0329\u001b[0m        \u001b[32m0.0676\u001b[0m  2.7280\n",
            "     25        \u001b[36m0.0302\u001b[0m        \u001b[32m0.0624\u001b[0m  2.8413\n",
            "     26        \u001b[36m0.0293\u001b[0m        \u001b[32m0.0542\u001b[0m  2.3308\n",
            "     27        \u001b[36m0.0285\u001b[0m        0.0649  2.3346\n",
            "     28        \u001b[36m0.0277\u001b[0m        \u001b[32m0.0463\u001b[0m  2.2966\n",
            "     29        \u001b[36m0.0255\u001b[0m        0.0832  2.4080\n",
            "     30        \u001b[36m0.0253\u001b[0m        \u001b[32m0.0303\u001b[0m  3.0452\n",
            "     31        \u001b[36m0.0228\u001b[0m        \u001b[32m0.0243\u001b[0m  2.4743\n",
            "     32        \u001b[36m0.0225\u001b[0m        0.0294  2.2923\n",
            "     33        \u001b[36m0.0208\u001b[0m        \u001b[32m0.0229\u001b[0m  2.2989\n",
            "     34        0.0211        0.0261  2.2996\n",
            "     35        \u001b[36m0.0207\u001b[0m        \u001b[32m0.0164\u001b[0m  2.7810\n",
            "     36        \u001b[36m0.0193\u001b[0m        \u001b[32m0.0153\u001b[0m  3.3658\n",
            "     37        \u001b[36m0.0185\u001b[0m        0.0163  2.9004\n",
            "     38        \u001b[36m0.0177\u001b[0m        0.0190  2.3251\n",
            "     39        \u001b[36m0.0166\u001b[0m        0.0236  2.3234\n",
            "     40        0.0170        0.0189  2.3482\n",
            "     41        \u001b[36m0.0162\u001b[0m        0.0239  2.7663\n",
            "     42        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0112\u001b[0m  2.8077\n",
            "     43        \u001b[36m0.0154\u001b[0m        0.0232  2.3094\n",
            "     44        0.0156        0.0113  2.3032\n",
            "     45        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0091\u001b[0m  2.3176\n",
            "     46        \u001b[36m0.0146\u001b[0m        0.0101  2.4110\n",
            "     47        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0083\u001b[0m  3.0767\n",
            "     48        \u001b[36m0.0135\u001b[0m        0.0104  2.4510\n",
            "     49        0.0136        0.0099  2.3125\n",
            "     50        0.0136        0.0104  2.3137\n",
            "     51        0.0140        0.0084  2.3352\n",
            "     52        0.0137        0.0092  2.7285\n",
            "     53        \u001b[36m0.0129\u001b[0m        0.0095  2.7651\n",
            "     54        0.0131        \u001b[32m0.0082\u001b[0m  2.3321\n",
            "     55        \u001b[36m0.0121\u001b[0m        \u001b[32m0.0080\u001b[0m  2.3389\n",
            "     56        0.0122        0.0086  2.2871\n",
            "     57        0.0123        \u001b[32m0.0079\u001b[0m  2.4049\n",
            "     58        0.0123        \u001b[32m0.0070\u001b[0m  3.0461\n",
            "     59        \u001b[36m0.0117\u001b[0m        0.0151  2.4423\n",
            "     60        0.0117        0.0081  2.3254\n",
            "     61        0.0119        0.0094  2.3141\n",
            "     62        \u001b[36m0.0108\u001b[0m        0.0084  2.3341\n",
            "     63        0.0119        0.0117  2.7372\n",
            "     64        0.0119        0.0153  2.8851\n",
            "     65        0.0118        0.0132  2.3376\n",
            "     66        0.0124        0.0127  2.3450\n",
            "     67        0.0113        0.0123  2.3312\n",
            "     68        \u001b[36m0.0106\u001b[0m        0.0171  2.3680\n",
            "     69        \u001b[36m0.0105\u001b[0m        0.0237  3.0507\n",
            "     70        0.0113        0.0202  2.5125\n",
            "     71        0.0116        0.0190  2.3520\n",
            "     72        0.0116        0.0267  2.3240\n",
            "     73        0.0116        0.0225  2.3586\n",
            "     74        0.0119        0.0243  2.7353\n",
            "     75        0.0127        0.0251  2.8274\n",
            "     76        0.0117        0.0158  2.3523\n",
            "     77        0.0112        0.0251  2.3280\n",
            "     78        0.0108        0.0219  2.3088\n",
            "     79        0.0110        0.0331  2.4993\n",
            "     80        0.0127        0.0357  3.0573\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.4min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0905\u001b[0m        \u001b[32m0.2974\u001b[0m  2.2912\n",
            "      2        \u001b[36m0.0905\u001b[0m        0.2997  2.3001\n",
            "      3        \u001b[36m0.0887\u001b[0m        0.3147  2.3084\n",
            "      4        0.0908        0.3028  2.3001\n",
            "      5        \u001b[36m0.0836\u001b[0m        \u001b[32m0.2578\u001b[0m  2.8044\n",
            "      6        \u001b[36m0.0753\u001b[0m        \u001b[32m0.2510\u001b[0m  2.7356\n",
            "      7        0.0774        0.2512  2.3291\n",
            "      8        \u001b[36m0.0735\u001b[0m        \u001b[32m0.2375\u001b[0m  2.3124\n",
            "      9        0.0736        \u001b[32m0.2328\u001b[0m  2.3343\n",
            "     10        \u001b[36m0.0723\u001b[0m        \u001b[32m0.2266\u001b[0m  2.4994\n",
            "     11        \u001b[36m0.0713\u001b[0m        \u001b[32m0.2170\u001b[0m  3.0859\n",
            "     12        \u001b[36m0.0702\u001b[0m        \u001b[32m0.2142\u001b[0m  2.2998\n",
            "     13        \u001b[36m0.0695\u001b[0m        0.2598  2.3068\n",
            "     14        \u001b[36m0.0674\u001b[0m        \u001b[32m0.2064\u001b[0m  2.3252\n",
            "     15        \u001b[36m0.0666\u001b[0m        0.2185  2.3362\n",
            "     16        0.0673        \u001b[32m0.2011\u001b[0m  2.7958\n",
            "     17        \u001b[36m0.0655\u001b[0m        0.2065  2.6851\n",
            "     18        \u001b[36m0.0654\u001b[0m        0.2427  2.3182\n",
            "     19        0.0667        0.2210  2.3420\n",
            "     20        0.0685        0.2201  2.2974\n",
            "     21        0.0677        0.2179  2.5034\n",
            "     22        0.0678        0.2182  3.0896\n",
            "     23        0.0677        0.2179  2.3913\n",
            "     24        0.0682        0.2350  2.3062\n",
            "     25        0.0673        0.2194  2.2844\n",
            "     26        0.0704        0.2175  2.3083\n",
            "     27        0.0675        0.2332  2.7229\n",
            "     28        0.0670        0.2246  2.8271\n",
            "     29        0.0696        0.2183  2.3158\n",
            "     30        0.0693        0.2256  2.3053\n",
            "     31        0.0693        0.2122  2.3965\n",
            "     32        0.0686        0.2165  2.4665\n",
            "     33        0.0688        0.2308  3.0548\n",
            "     34        0.0674        0.2117  2.4298\n",
            "     35        0.0678        0.2134  2.3033\n",
            "     36        0.0676        \u001b[32m0.1991\u001b[0m  2.3155\n",
            "     37        0.0694        0.2400  2.3138\n",
            "     38        0.0704        0.2038  2.7096\n",
            "     39        0.0683        0.2155  2.8858\n",
            "     40        0.0686        0.2086  2.2821\n",
            "     41        0.0662        0.2073  2.3121\n",
            "     42        0.0665        \u001b[32m0.1973\u001b[0m  2.3287\n",
            "     43        0.0666        0.2029  2.3504\n",
            "     44        0.0674        0.2056  2.9730\n",
            "     45        \u001b[36m0.0654\u001b[0m        0.2089  2.5428\n",
            "     46        0.0658        0.2024  2.3187\n",
            "     47        0.0670        0.2096  2.3137\n",
            "     48        0.0672        0.2229  2.3293\n",
            "     49        0.0679        0.2013  2.6209\n",
            "     50        0.0670        0.2118  3.4440\n",
            "     51        0.0671        0.2212  3.0759\n",
            "     52        0.0674        0.2059  2.3146\n",
            "     53        0.0680        \u001b[32m0.1959\u001b[0m  2.2912\n",
            "     54        0.0669        0.2067  2.3042\n",
            "     55        0.0671        0.2062  2.6268\n",
            "     56        0.0668        \u001b[32m0.1860\u001b[0m  2.9979\n",
            "     57        0.0683        0.1956  2.3279\n",
            "     58        \u001b[36m0.0649\u001b[0m        0.2242  2.2956\n",
            "     59        0.0660        0.1871  2.3355\n",
            "     60        0.0691        0.2447  2.3300\n",
            "     61        0.0713        0.2683  2.9122\n",
            "     62        0.0703        0.2335  2.6378\n",
            "     63        0.0681        0.1981  2.3175\n",
            "     64        0.0657        0.1961  2.3259\n",
            "     65        0.0661        0.1965  2.3090\n",
            "     66        0.0674        0.2265  2.5648\n",
            "     67        0.0683        0.1991  3.0013\n",
            "     68        0.0662        0.2081  2.3477\n",
            "     69        0.0673        0.1908  2.3227\n",
            "     70        0.0677        0.2236  2.3098\n",
            "     71        0.0684        0.2118  2.3306\n",
            "     72        0.0684        0.2153  2.9210\n",
            "     73        0.0663        0.2190  2.6835\n",
            "     74        0.0669        0.2456  2.3225\n",
            "     75        0.0666        0.2242  2.3170\n",
            "     76        0.0693        0.2518  2.3344\n",
            "     77        0.0672        0.2026  2.5208\n",
            "     78        0.0693        0.2446  3.1114\n",
            "     79        0.0677        0.2081  2.3474\n",
            "     80        0.0705        0.2388  2.3282\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0707\u001b[0m        \u001b[32m0.2310\u001b[0m  2.1560\n",
            "      2        \u001b[36m0.0602\u001b[0m        \u001b[32m0.2225\u001b[0m  2.1399\n",
            "      3        \u001b[36m0.0582\u001b[0m        0.2294  2.5867\n",
            "      4        \u001b[36m0.0566\u001b[0m        0.2321  2.5787\n",
            "      5        \u001b[36m0.0550\u001b[0m        \u001b[32m0.2225\u001b[0m  2.1559\n",
            "      6        \u001b[36m0.0543\u001b[0m        0.2269  2.1159\n",
            "      7        0.0568        0.2751  2.1442\n",
            "      8        0.0580        0.2397  2.1467\n",
            "      9        0.0545        \u001b[32m0.2096\u001b[0m  2.5943\n",
            "     10        \u001b[36m0.0527\u001b[0m        \u001b[32m0.1917\u001b[0m  2.5241\n",
            "     11        \u001b[36m0.0518\u001b[0m        \u001b[32m0.1861\u001b[0m  2.1600\n",
            "     12        \u001b[36m0.0488\u001b[0m        \u001b[32m0.1830\u001b[0m  2.1383\n",
            "     13        \u001b[36m0.0454\u001b[0m        \u001b[32m0.1287\u001b[0m  2.1195\n",
            "     14        \u001b[36m0.0367\u001b[0m        \u001b[32m0.0908\u001b[0m  2.1631\n",
            "     15        \u001b[36m0.0251\u001b[0m        0.1028  2.5687\n",
            "     16        \u001b[36m0.0172\u001b[0m        0.1283  2.5787\n",
            "     17        \u001b[36m0.0150\u001b[0m        0.1277  2.1530\n",
            "     18        \u001b[36m0.0129\u001b[0m        0.1133  2.1370\n",
            "     19        \u001b[36m0.0118\u001b[0m        0.1015  2.1495\n",
            "     20        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0713\u001b[0m  2.1485\n",
            "     21        \u001b[36m0.0072\u001b[0m        0.1007  2.5523\n",
            "     22        \u001b[36m0.0061\u001b[0m        \u001b[32m0.0704\u001b[0m  2.6114\n",
            "     23        0.0066        \u001b[32m0.0506\u001b[0m  2.1311\n",
            "     24        \u001b[36m0.0055\u001b[0m        \u001b[32m0.0340\u001b[0m  2.1604\n",
            "     25        \u001b[36m0.0053\u001b[0m        0.0420  2.1246\n",
            "     26        \u001b[36m0.0052\u001b[0m        0.0358  2.1389\n",
            "     27        0.0055        \u001b[32m0.0276\u001b[0m  2.6586\n",
            "     28        0.0055        0.0285  2.5630\n",
            "     29        \u001b[36m0.0052\u001b[0m        0.0292  2.1817\n",
            "     30        0.0052        \u001b[32m0.0191\u001b[0m  2.1339\n",
            "     31        \u001b[36m0.0050\u001b[0m        0.0253  2.1448\n",
            "     32        \u001b[36m0.0049\u001b[0m        0.0213  2.1595\n",
            "     33        0.0052        \u001b[32m0.0162\u001b[0m  2.6387\n",
            "     34        \u001b[36m0.0044\u001b[0m        \u001b[32m0.0146\u001b[0m  2.5273\n",
            "     35        \u001b[36m0.0040\u001b[0m        0.0176  2.1326\n",
            "     36        0.0046        0.0211  2.1563\n",
            "     37        0.0048        \u001b[32m0.0137\u001b[0m  2.1512\n",
            "     38        0.0045        0.0154  2.1870\n",
            "     39        \u001b[36m0.0039\u001b[0m        0.0256  2.6816\n",
            "     40        0.0045        \u001b[32m0.0111\u001b[0m  2.5821\n",
            "     41        0.0054        0.0131  2.1531\n",
            "     42        0.0046        0.0117  2.1509\n",
            "     43        0.0042        0.0116  2.1496\n",
            "     44        0.0041        0.0126  2.1416\n",
            "     45        \u001b[36m0.0039\u001b[0m        0.0200  2.6340\n",
            "     46        0.0043        0.0159  2.5546\n",
            "     47        0.0043        0.0117  2.1274\n",
            "     48        0.0042        \u001b[32m0.0108\u001b[0m  2.1756\n",
            "     49        \u001b[36m0.0037\u001b[0m        0.0165  2.1497\n",
            "     50        0.0040        0.0148  2.1650\n",
            "     51        0.0044        0.0186  2.6940\n",
            "     52        0.0041        0.0121  2.4650\n",
            "     53        \u001b[36m0.0036\u001b[0m        0.0160  2.1718\n",
            "     54        0.0040        \u001b[32m0.0078\u001b[0m  2.1486\n",
            "     55        0.0037        0.0093  2.1338\n",
            "     56        0.0041        0.0102  2.1091\n",
            "     57        \u001b[36m0.0034\u001b[0m        0.0198  2.6459\n",
            "     58        0.0048        0.0107  2.4682\n",
            "     59        \u001b[36m0.0029\u001b[0m        0.0101  2.1445\n",
            "     60        0.0035        0.0098  2.1330\n",
            "     61        0.0033        0.0090  2.1382\n",
            "     62        0.0031        0.0085  2.1242\n",
            "     63        0.0035        0.0102  2.7538\n",
            "     64        \u001b[36m0.0028\u001b[0m        0.0085  2.3890\n",
            "     65        0.0033        0.0091  2.1456\n",
            "     66        0.0029        0.0079  2.1707\n",
            "     67        0.0029        0.0108  2.1478\n",
            "     68        \u001b[36m0.0028\u001b[0m        0.0113  2.2303\n",
            "     69        0.0034        0.0151  2.8864\n",
            "     70        \u001b[36m0.0027\u001b[0m        0.0081  3.0386\n",
            "     71        \u001b[36m0.0025\u001b[0m        0.0105  2.3318\n",
            "     72        0.0033        0.0094  2.1787\n",
            "     73        0.0027        \u001b[32m0.0077\u001b[0m  2.1321\n",
            "     74        0.0028        0.0096  2.2286\n",
            "     75        0.0033        0.0184  2.8004\n",
            "     76        0.0028        0.0094  2.3277\n",
            "     77        0.0027        0.0100  2.1776\n",
            "     78        0.0032        0.0113  2.1644\n",
            "     79        0.0038        0.0092  2.1766\n",
            "     80        0.0026        \u001b[32m0.0068\u001b[0m  2.3153\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.1min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0889\u001b[0m        \u001b[32m0.2362\u001b[0m  2.7358\n",
            "      2        \u001b[36m0.0847\u001b[0m        0.2408  2.1517\n",
            "      3        0.0851        0.2927  2.1306\n",
            "      4        \u001b[36m0.0794\u001b[0m        0.2821  2.1535\n",
            "      5        \u001b[36m0.0676\u001b[0m        0.2432  2.1374\n",
            "      6        \u001b[36m0.0639\u001b[0m        0.2857  2.3956\n",
            "      7        \u001b[36m0.0627\u001b[0m        0.2855  2.7022\n",
            "      8        \u001b[36m0.0621\u001b[0m        0.2981  2.1791\n",
            "      9        \u001b[36m0.0621\u001b[0m        0.2953  2.1804\n",
            "     10        0.0629        0.2618  2.1565\n",
            "     11        \u001b[36m0.0614\u001b[0m        0.2846  2.1835\n",
            "     12        0.0619        0.2845  2.5265\n",
            "     13        0.0620        0.2849  2.6510\n",
            "     14        0.0621        0.2847  2.1378\n",
            "     15        0.0622        0.2826  2.1309\n",
            "     16        0.0619        0.2805  2.1356\n",
            "     17        0.0619        0.2800  2.1520\n",
            "     18        0.0619        0.2783  2.5030\n",
            "     19        0.0618        0.2681  2.6628\n",
            "     20        0.0620        0.2667  2.1458\n",
            "     21        0.0618        0.2792  2.1586\n",
            "     22        0.0624        0.2795  2.1746\n",
            "     23        0.0622        0.2766  2.1661\n",
            "     24        0.0625        0.2776  2.5206\n",
            "     25        0.0622        0.2785  2.6505\n",
            "     26        0.0622        0.2769  2.1741\n",
            "     27        0.0624        0.2768  2.1503\n",
            "     28        0.0623        0.2752  2.1568\n",
            "     29        0.0622        0.2748  2.1676\n",
            "     30        0.0624        0.2766  2.5513\n",
            "     31        0.0623        0.2753  2.5608\n",
            "     32        0.0623        0.2748  2.1427\n",
            "     33        0.0623        0.2745  2.1474\n",
            "     34        0.0623        0.2736  2.1520\n",
            "     35        0.0624        0.2756  2.1865\n",
            "     36        0.0624        0.2742  2.6264\n",
            "     37        0.0623        0.2742  2.5676\n",
            "     38        0.0624        0.2745  2.1562\n",
            "     39        0.0624        0.2735  2.1872\n",
            "     40        0.0624        0.2737  2.1471\n",
            "     41        0.0624        0.2728  2.1061\n",
            "     42        0.0615        0.2569  2.6393\n",
            "     43        \u001b[36m0.0610\u001b[0m        0.2661  2.4803\n",
            "     44        0.0612        0.2502  2.1414\n",
            "     45        0.0617        0.2578  2.1298\n",
            "     46        0.0619        0.2618  2.1155\n",
            "     47        0.0619        0.2635  2.1609\n",
            "     48        0.0619        0.2630  2.7212\n",
            "     49        0.0618        0.2627  2.4606\n",
            "     50        0.0614        0.2573  2.1257\n",
            "     51        0.0618        0.2579  2.1723\n",
            "     52        0.0615        0.2604  2.1626\n",
            "     53        0.0618        0.2598  2.1506\n",
            "     54        0.0616        0.2601  2.7321\n",
            "     55        0.0619        0.2567  2.4321\n",
            "     56        0.0618        0.2571  2.1547\n",
            "     57        0.0617        0.2601  2.1508\n",
            "     58        0.0621        0.2603  2.1383\n",
            "     59        0.0614        0.2577  2.1496\n",
            "     60        0.0615        0.2649  2.7408\n",
            "     61        0.0618        0.2627  2.4342\n",
            "     62        0.0623        0.2585  2.1494\n",
            "     63        0.0622        0.2648  2.1658\n",
            "     64        0.0620        0.2649  2.1779\n",
            "     65        0.0620        0.2627  2.2049\n",
            "     66        0.0620        0.2642  2.7378\n",
            "     67        0.0621        0.2638  2.3257\n",
            "     68        0.0621        0.2653  2.1515\n",
            "     69        0.0618        0.2656  2.1629\n",
            "     70        0.0624        0.2527  2.1736\n",
            "     71        0.0616        0.2611  2.2608\n",
            "     72        0.0619        0.2669  2.7897\n",
            "     73        0.0622        0.2677  2.2621\n",
            "     74        0.0622        0.2649  2.1913\n",
            "     75        0.0622        0.2663  2.1733\n",
            "     76        0.0623        0.2691  2.1428\n",
            "     77        0.0622        0.2661  2.3783\n",
            "     78        0.0622        0.2684  2.8227\n",
            "     79        0.0622        0.2685  2.1594\n",
            "     80        0.0622        0.2675  2.1622\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0659\u001b[0m        \u001b[32m0.2837\u001b[0m  2.3024\n",
            "      2        \u001b[36m0.0476\u001b[0m        0.3242  2.3300\n",
            "      3        \u001b[36m0.0442\u001b[0m        0.3148  2.8882\n",
            "      4        0.0484        0.3126  2.6729\n",
            "      5        0.0449        0.2844  2.2795\n",
            "      6        0.0453        \u001b[32m0.2342\u001b[0m  2.3113\n",
            "      7        \u001b[36m0.0421\u001b[0m        0.2386  2.3135\n",
            "      8        0.0449        \u001b[32m0.1890\u001b[0m  2.4629\n",
            "      9        0.0424        \u001b[32m0.1798\u001b[0m  3.1864\n",
            "     10        \u001b[36m0.0400\u001b[0m        0.2064  3.2222\n",
            "     11        0.0406        \u001b[32m0.1489\u001b[0m  2.4294\n",
            "     12        \u001b[36m0.0391\u001b[0m        0.1624  2.3665\n",
            "     13        \u001b[36m0.0388\u001b[0m        \u001b[32m0.1391\u001b[0m  2.3123\n",
            "     14        0.0389        \u001b[32m0.1371\u001b[0m  2.8265\n",
            "     15        0.0392        \u001b[32m0.1298\u001b[0m  2.6888\n",
            "     16        0.0405        \u001b[32m0.0998\u001b[0m  2.3200\n",
            "     17        0.0392        0.1488  2.3122\n",
            "     18        0.0405        0.1176  2.3029\n",
            "     19        0.0450        0.1703  2.5341\n",
            "     20        0.0398        0.1177  3.0496\n",
            "     21        0.0412        \u001b[32m0.0980\u001b[0m  2.3148\n",
            "     22        0.0431        0.1263  2.3398\n",
            "     23        0.0400        0.1172  2.3328\n",
            "     24        0.0406        \u001b[32m0.0898\u001b[0m  2.3210\n",
            "     25        \u001b[36m0.0359\u001b[0m        \u001b[32m0.0855\u001b[0m  2.8272\n",
            "     26        0.0391        0.0872  2.7246\n",
            "     27        0.0368        \u001b[32m0.0833\u001b[0m  2.2966\n",
            "     28        0.0361        \u001b[32m0.0713\u001b[0m  2.2918\n",
            "     29        \u001b[36m0.0335\u001b[0m        0.0972  2.3235\n",
            "     30        0.0341        0.0899  2.4777\n",
            "     31        \u001b[36m0.0317\u001b[0m        0.0755  3.0610\n",
            "     32        \u001b[36m0.0306\u001b[0m        \u001b[32m0.0627\u001b[0m  2.3123\n",
            "     33        \u001b[36m0.0291\u001b[0m        \u001b[32m0.0559\u001b[0m  2.3413\n",
            "     34        \u001b[36m0.0272\u001b[0m        \u001b[32m0.0498\u001b[0m  2.3276\n",
            "     35        \u001b[36m0.0266\u001b[0m        0.1234  2.3310\n",
            "     36        0.0282        0.0594  2.7901\n",
            "     37        \u001b[36m0.0253\u001b[0m        \u001b[32m0.0405\u001b[0m  2.7147\n",
            "     38        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0325\u001b[0m  2.3278\n",
            "     39        \u001b[36m0.0221\u001b[0m        0.0354  2.3362\n",
            "     40        \u001b[36m0.0211\u001b[0m        \u001b[32m0.0257\u001b[0m  2.3208\n",
            "     41        \u001b[36m0.0208\u001b[0m        \u001b[32m0.0225\u001b[0m  2.4246\n",
            "     42        \u001b[36m0.0201\u001b[0m        \u001b[32m0.0212\u001b[0m  3.0848\n",
            "     43        0.0207        0.0238  2.3976\n",
            "     44        \u001b[36m0.0186\u001b[0m        0.0222  2.3280\n",
            "     45        0.0187        0.0214  2.3895\n",
            "     46        \u001b[36m0.0184\u001b[0m        0.0251  2.4121\n",
            "     47        0.0192        \u001b[32m0.0211\u001b[0m  2.8881\n",
            "     48        \u001b[36m0.0179\u001b[0m        \u001b[32m0.0184\u001b[0m  2.7285\n",
            "     49        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0173\u001b[0m  2.3281\n",
            "     50        0.0182        0.0181  2.4017\n",
            "     51        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0153\u001b[0m  2.3086\n",
            "     52        \u001b[36m0.0168\u001b[0m        0.0173  2.5062\n",
            "     53        \u001b[36m0.0167\u001b[0m        0.0161  3.0369\n",
            "     54        \u001b[36m0.0166\u001b[0m        0.0184  2.3441\n",
            "     55        0.0166        0.0212  2.3337\n",
            "     56        \u001b[36m0.0165\u001b[0m        0.0189  2.3130\n",
            "     57        0.0174        0.0156  2.3592\n",
            "     58        0.0178        0.0194  2.9037\n",
            "     59        0.0167        0.0211  2.6222\n",
            "     60        \u001b[36m0.0163\u001b[0m        0.0170  2.3565\n",
            "     61        \u001b[36m0.0156\u001b[0m        0.0203  2.3071\n",
            "     62        \u001b[36m0.0156\u001b[0m        0.0229  2.3810\n",
            "     63        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0151\u001b[0m  2.6172\n",
            "     64        0.0164        0.0221  2.9795\n",
            "     65        0.0156        0.0176  2.3425\n",
            "     66        \u001b[36m0.0149\u001b[0m        0.0258  2.3593\n",
            "     67        0.0155        0.0211  2.3473\n",
            "     68        0.0158        \u001b[32m0.0122\u001b[0m  2.3149\n",
            "     69        0.0152        0.0138  2.9838\n",
            "     70        0.0150        0.0192  2.6135\n",
            "     71        0.0152        0.0201  2.3564\n",
            "     72        \u001b[36m0.0146\u001b[0m        0.0203  2.3365\n",
            "     73        0.0148        0.0136  2.3519\n",
            "     74        0.0148        0.0207  2.6902\n",
            "     75        \u001b[36m0.0141\u001b[0m        0.0201  2.9110\n",
            "     76        \u001b[36m0.0135\u001b[0m        0.0202  2.3336\n",
            "     77        0.0140        0.0166  2.3248\n",
            "     78        0.0143        0.0165  2.3574\n",
            "     79        0.0144        0.0173  2.3372\n",
            "     80        \u001b[36m0.0132\u001b[0m        0.0191  2.9666\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.4min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0689\u001b[0m        \u001b[32m0.3424\u001b[0m  2.3474\n",
            "      2        \u001b[36m0.0507\u001b[0m        \u001b[32m0.3258\u001b[0m  2.3181\n",
            "      3        0.0533        \u001b[32m0.3033\u001b[0m  2.3498\n",
            "      4        \u001b[36m0.0477\u001b[0m        \u001b[32m0.2557\u001b[0m  2.3492\n",
            "      5        \u001b[36m0.0424\u001b[0m        0.2677  2.8401\n",
            "      6        0.0444        0.3062  2.6886\n",
            "      7        0.0655        0.3140  2.3398\n",
            "      8        0.0579        0.2968  2.2921\n",
            "      9        0.0546        0.3262  2.3700\n",
            "     10        0.0565        0.3218  2.5814\n",
            "     11        0.0501        0.2700  3.0034\n",
            "     12        0.0516        0.3170  2.3258\n",
            "     13        0.0502        0.3110  2.3558\n",
            "     14        0.0458        0.3046  2.3279\n",
            "     15        0.0479        \u001b[32m0.2498\u001b[0m  2.3303\n",
            "     16        0.0431        0.3161  2.9303\n",
            "     17        0.0561        0.3162  2.6693\n",
            "     18        0.0536        0.3194  2.3443\n",
            "     19        0.0533        0.2973  2.3110\n",
            "     20        0.0587        0.2912  2.2930\n",
            "     21        0.0620        0.3010  2.5212\n",
            "     22        0.0626        0.2980  2.9975\n",
            "     23        0.0614        0.2652  2.9461\n",
            "     24        0.0619        0.2857  2.6970\n",
            "     25        0.0621        0.3166  2.2988\n",
            "     26        0.0632        0.2934  2.5299\n",
            "     27        0.0621        0.3266  3.0143\n",
            "     28        0.0629        0.3278  2.3474\n",
            "     29        0.0643        \u001b[32m0.2403\u001b[0m  2.3284\n",
            "     30        0.0623        0.2531  2.3291\n",
            "     31        0.0607        0.2663  2.3419\n",
            "     32        0.0621        0.2835  2.8991\n",
            "     33        0.0608        0.2857  2.6879\n",
            "     34        0.0611        0.2933  2.3766\n",
            "     35        0.0624        0.3222  2.3412\n",
            "     36        0.0636        0.2668  2.3665\n",
            "     37        0.0612        0.2920  2.6221\n",
            "     38        0.0612        0.3482  2.9281\n",
            "     39        0.0618        0.3039  2.2790\n",
            "     40        0.0610        0.3271  2.3393\n",
            "     41        0.0611        0.3369  2.3539\n",
            "     42        0.0630        0.3194  2.3554\n",
            "     43        0.0605        0.3589  2.9409\n",
            "     44        0.0603        0.2653  2.6478\n",
            "     45        0.0608        0.3327  2.3250\n",
            "     46        0.0607        0.3248  2.3318\n",
            "     47        0.0611        0.2727  2.3382\n",
            "     48        0.0610        0.3323  2.5717\n",
            "     49        0.0608        0.3201  3.0393\n",
            "     50        0.0626        0.3532  2.3273\n",
            "     51        0.0613        0.3202  2.3292\n",
            "     52        0.0628        0.2923  2.3820\n",
            "     53        0.0666        0.2467  2.3763\n",
            "     54        0.0625        0.2534  2.9928\n",
            "     55        0.0625        \u001b[32m0.2022\u001b[0m  2.6870\n",
            "     56        0.0620        0.2671  2.3341\n",
            "     57        0.0633        0.2678  2.3527\n",
            "     58        0.0629        0.2869  2.3548\n",
            "     59        0.0615        0.2732  2.6215\n",
            "     60        0.0634        0.2679  2.9447\n",
            "     61        0.0613        0.2601  2.3354\n",
            "     62        0.0624        0.2493  2.3347\n",
            "     63        0.0626        0.3526  2.3175\n",
            "     64        0.0628        0.2634  2.3522\n",
            "     65        0.0615        0.3414  3.0055\n",
            "     66        0.0625        0.2899  2.6899\n",
            "     67        0.0604        0.3161  2.3427\n",
            "     68        0.0627        0.3324  2.3131\n",
            "     69        0.0621        0.2699  2.3605\n",
            "     70        0.0611        0.3138  2.6106\n",
            "     71        0.0625        0.3698  2.9423\n",
            "     72        0.0609        0.3292  2.3474\n",
            "     73        0.0633        0.2191  2.3652\n",
            "     74        0.0644        0.2631  2.3203\n",
            "     75        0.0639        0.2113  2.3160\n",
            "     76        0.0621        0.3668  2.9888\n",
            "     77        0.0619        0.2956  2.6449\n",
            "     78        0.0608        0.3218  2.3340\n",
            "     79        0.0619        0.3147  2.3293\n",
            "     80        0.0609        0.3364  2.3262\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0955\u001b[0m        \u001b[32m0.2214\u001b[0m  1.2367\n",
            "      2        \u001b[36m0.0846\u001b[0m        \u001b[32m0.1512\u001b[0m  1.4650\n",
            "      3        \u001b[36m0.0718\u001b[0m        \u001b[32m0.1511\u001b[0m  1.5295\n",
            "      4        \u001b[36m0.0675\u001b[0m        \u001b[32m0.1290\u001b[0m  1.2686\n",
            "      5        \u001b[36m0.0656\u001b[0m        \u001b[32m0.1249\u001b[0m  1.1354\n",
            "      6        \u001b[36m0.0649\u001b[0m        0.1294  1.1313\n",
            "      7        0.0676        \u001b[32m0.1247\u001b[0m  1.1404\n",
            "      8        0.0672        \u001b[32m0.1235\u001b[0m  1.1745\n",
            "      9        0.0668        \u001b[32m0.1218\u001b[0m  1.1358\n",
            "     10        0.0674        0.1223  1.1483\n",
            "     11        0.0675        0.1219  1.1407\n",
            "     12        0.0653        0.1243  1.1828\n",
            "     13        0.0685        0.1221  1.5008\n",
            "     14        0.0684        \u001b[32m0.1211\u001b[0m  1.5178\n",
            "     15        0.0684        \u001b[32m0.1202\u001b[0m  1.3269\n",
            "     16        0.0682        0.1228  1.1353\n",
            "     17        0.0688        \u001b[32m0.1201\u001b[0m  1.1652\n",
            "     18        0.0684        \u001b[32m0.1186\u001b[0m  1.1512\n",
            "     19        0.0671        \u001b[32m0.1155\u001b[0m  1.1408\n",
            "     20        0.0657        \u001b[32m0.1043\u001b[0m  1.1359\n",
            "     21        \u001b[36m0.0639\u001b[0m        \u001b[32m0.0965\u001b[0m  1.1228\n",
            "     22        \u001b[36m0.0621\u001b[0m        \u001b[32m0.0910\u001b[0m  1.1342\n",
            "     23        \u001b[36m0.0608\u001b[0m        \u001b[32m0.0861\u001b[0m  1.1436\n",
            "     24        \u001b[36m0.0592\u001b[0m        \u001b[32m0.0801\u001b[0m  1.4472\n",
            "     25        \u001b[36m0.0564\u001b[0m        \u001b[32m0.0740\u001b[0m  1.5086\n",
            "     26        \u001b[36m0.0542\u001b[0m        0.0783  1.4104\n",
            "     27        \u001b[36m0.0535\u001b[0m        \u001b[32m0.0707\u001b[0m  1.1418\n",
            "     28        \u001b[36m0.0507\u001b[0m        \u001b[32m0.0651\u001b[0m  1.1463\n",
            "     29        \u001b[36m0.0475\u001b[0m        0.0745  1.1452\n",
            "     30        0.0495        \u001b[32m0.0555\u001b[0m  1.1487\n",
            "     31        \u001b[36m0.0315\u001b[0m        0.0673  1.1406\n",
            "     32        \u001b[36m0.0262\u001b[0m        \u001b[32m0.0356\u001b[0m  1.1681\n",
            "     33        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0356\u001b[0m  1.1698\n",
            "     34        \u001b[36m0.0090\u001b[0m        0.0734  1.1426\n",
            "     35        0.0090        0.0363  1.3497\n",
            "     36        \u001b[36m0.0066\u001b[0m        0.0388  1.4924\n",
            "     37        \u001b[36m0.0064\u001b[0m        \u001b[32m0.0333\u001b[0m  1.4988\n",
            "     38        \u001b[36m0.0060\u001b[0m        0.0349  1.1243\n",
            "     39        0.0060        \u001b[32m0.0308\u001b[0m  1.1274\n",
            "     40        \u001b[36m0.0055\u001b[0m        \u001b[32m0.0272\u001b[0m  1.1484\n",
            "     41        0.0057        \u001b[32m0.0189\u001b[0m  1.1516\n",
            "     42        \u001b[36m0.0053\u001b[0m        0.0217  1.1566\n",
            "     43        0.0053        \u001b[32m0.0173\u001b[0m  1.1309\n",
            "     44        0.0059        \u001b[32m0.0122\u001b[0m  1.1430\n",
            "     45        \u001b[36m0.0049\u001b[0m        \u001b[32m0.0117\u001b[0m  1.1626\n",
            "     46        \u001b[36m0.0048\u001b[0m        \u001b[32m0.0089\u001b[0m  1.2880\n",
            "     47        0.0052        0.0155  1.4698\n",
            "     48        \u001b[36m0.0043\u001b[0m        0.0155  1.5435\n",
            "     49        0.0064        0.0089  1.1780\n",
            "     50        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0079\u001b[0m  1.1836\n",
            "     51        \u001b[36m0.0033\u001b[0m        \u001b[32m0.0051\u001b[0m  1.1385\n",
            "     52        \u001b[36m0.0025\u001b[0m        0.0059  1.1426\n",
            "     53        \u001b[36m0.0022\u001b[0m        0.0056  1.1444\n",
            "     54        \u001b[36m0.0019\u001b[0m        \u001b[32m0.0047\u001b[0m  1.1589\n",
            "     55        0.0021        0.0069  1.1474\n",
            "     56        \u001b[36m0.0015\u001b[0m        0.0102  1.1425\n",
            "     57        \u001b[36m0.0014\u001b[0m        0.0084  1.3487\n",
            "     58        0.0023        0.0120  1.4654\n",
            "     59        0.0015        0.0077  1.5491\n",
            "     60        \u001b[36m0.0013\u001b[0m        0.0098  1.1762\n",
            "     61        \u001b[36m0.0012\u001b[0m        0.0133  1.1460\n",
            "     62        \u001b[36m0.0012\u001b[0m        0.0105  1.1485\n",
            "     63        \u001b[36m0.0011\u001b[0m        0.0077  1.1428\n",
            "     64        0.0012        0.0059  1.1288\n",
            "     65        0.0014        0.0057  1.1198\n",
            "     66        \u001b[36m0.0010\u001b[0m        0.0076  1.1652\n",
            "     67        0.0011        0.0069  1.1499\n",
            "     68        0.0012        0.0087  1.2849\n",
            "     69        0.0014        0.0156  1.4970\n",
            "     70        0.0013        0.0130  1.6081\n",
            "     71        0.0016        0.0096  1.5605\n",
            "     72        0.0016        0.0235  1.4950\n",
            "     73        0.0019        0.0224  1.3137\n",
            "     74        0.0021        0.0186  1.1335\n",
            "     75        0.0018        0.0155  1.1323\n",
            "     76        0.0014        0.0133  1.1360\n",
            "     77        0.0018        0.0139  1.1454\n",
            "     78        0.0017        0.0172  1.1562\n",
            "     79        0.0015        0.0142  1.3907\n",
            "     80        0.0014        0.0160  1.4984\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.7min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0928\u001b[0m        \u001b[32m0.1478\u001b[0m  1.3583\n",
            "      2        \u001b[36m0.0829\u001b[0m        0.1611  1.1411\n",
            "      3        \u001b[36m0.0758\u001b[0m        \u001b[32m0.1258\u001b[0m  1.1398\n",
            "      4        \u001b[36m0.0711\u001b[0m        0.1267  1.1442\n",
            "      5        \u001b[36m0.0683\u001b[0m        \u001b[32m0.1176\u001b[0m  1.1610\n",
            "      6        \u001b[36m0.0663\u001b[0m        \u001b[32m0.1123\u001b[0m  1.1443\n",
            "      7        \u001b[36m0.0642\u001b[0m        \u001b[32m0.1080\u001b[0m  1.1465\n",
            "      8        \u001b[36m0.0630\u001b[0m        \u001b[32m0.0935\u001b[0m  1.1320\n",
            "      9        \u001b[36m0.0623\u001b[0m        0.1122  1.1499\n",
            "     10        \u001b[36m0.0618\u001b[0m        \u001b[32m0.0855\u001b[0m  1.4264\n",
            "     11        \u001b[36m0.0595\u001b[0m        0.1030  1.4722\n",
            "     12        \u001b[36m0.0587\u001b[0m        0.0856  1.4334\n",
            "     13        0.0601        0.0943  1.1572\n",
            "     14        \u001b[36m0.0569\u001b[0m        0.0910  1.1456\n",
            "     15        0.0595        0.1107  1.1422\n",
            "     16        0.0595        0.0987  1.1467\n",
            "     17        \u001b[36m0.0561\u001b[0m        0.0914  1.1529\n",
            "     18        \u001b[36m0.0546\u001b[0m        0.0889  1.1479\n",
            "     19        0.0559        0.0957  1.1493\n",
            "     20        \u001b[36m0.0497\u001b[0m        0.0929  1.1611\n",
            "     21        \u001b[36m0.0480\u001b[0m        \u001b[32m0.0704\u001b[0m  1.3654\n",
            "     22        0.0482        0.0881  1.5086\n",
            "     23        \u001b[36m0.0467\u001b[0m        0.0900  1.4767\n",
            "     24        0.0469        0.0748  1.1926\n",
            "     25        0.0477        0.0736  1.1674\n",
            "     26        0.0495        0.0924  1.1536\n",
            "     27        0.0506        0.0981  1.1450\n",
            "     28        0.0511        0.0978  1.1399\n",
            "     29        0.0505        0.1035  1.1411\n",
            "     30        0.0495        0.1000  1.1624\n",
            "     31        0.0511        \u001b[32m0.0681\u001b[0m  1.1421\n",
            "     32        0.0493        0.0990  1.3173\n",
            "     33        0.0499        0.0976  1.5071\n",
            "     34        0.0500        0.0938  1.5441\n",
            "     35        0.0497        0.1012  1.1510\n",
            "     36        0.0495        0.0981  1.1629\n",
            "     37        0.0494        0.1001  1.1446\n",
            "     38        0.0496        0.1070  1.1269\n",
            "     39        0.0494        0.1250  1.1461\n",
            "     40        0.0473        0.0845  1.1434\n",
            "     41        0.0476        0.1097  1.1638\n",
            "     42        0.0491        0.1083  1.1447\n",
            "     43        0.0485        0.1088  1.2812\n",
            "     44        0.0474        0.0923  1.4837\n",
            "     45        \u001b[36m0.0463\u001b[0m        0.0907  1.5541\n",
            "     46        \u001b[36m0.0454\u001b[0m        0.0878  1.1495\n",
            "     47        \u001b[36m0.0449\u001b[0m        0.0828  1.1470\n",
            "     48        \u001b[36m0.0438\u001b[0m        0.0889  1.1418\n",
            "     49        \u001b[36m0.0429\u001b[0m        0.0718  1.1480\n",
            "     50        \u001b[36m0.0425\u001b[0m        0.0704  1.1486\n",
            "     51        \u001b[36m0.0418\u001b[0m        \u001b[32m0.0637\u001b[0m  1.1462\n",
            "     52        \u001b[36m0.0411\u001b[0m        0.0702  1.1478\n",
            "     53        \u001b[36m0.0408\u001b[0m        0.0668  1.1557\n",
            "     54        0.0410        0.0744  1.2903\n",
            "     55        \u001b[36m0.0401\u001b[0m        0.0686  1.4675\n",
            "     56        0.0417        0.0769  1.5376\n",
            "     57        0.0438        0.0894  1.1560\n",
            "     58        0.0405        0.0839  1.1403\n",
            "     59        0.0411        0.0731  1.1280\n",
            "     60        0.0403        0.0802  1.1510\n",
            "     61        0.0416        0.0701  1.1205\n",
            "     62        0.0408        0.0765  1.1320\n",
            "     63        0.0426        0.0721  1.1483\n",
            "     64        0.0569        0.2649  1.1475\n",
            "     65        0.0695        0.1428  1.3131\n",
            "     66        0.0766        0.1631  1.4631\n",
            "     67        0.0609        0.1433  1.5542\n",
            "     68        0.0632        0.0905  1.2656\n",
            "     69        0.0572        0.1112  1.1437\n",
            "     70        0.0568        0.0892  1.1223\n",
            "     71        0.0571        0.0678  1.1319\n",
            "     72        0.0554        \u001b[32m0.0608\u001b[0m  1.1446\n",
            "     73        0.0538        0.0632  1.1543\n",
            "     74        0.0538        \u001b[32m0.0598\u001b[0m  1.1802\n",
            "     75        0.0535        \u001b[32m0.0585\u001b[0m  1.1722\n",
            "     76        0.0490        0.0806  1.2017\n",
            "     77        0.0566        0.0757  1.4937\n",
            "     78        0.0527        0.0633  1.5209\n",
            "     79        0.0531        0.0615  1.2406\n",
            "     80        0.0476        0.0713  1.1584\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.6min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0927\u001b[0m        \u001b[32m0.3167\u001b[0m  1.2475\n",
            "      2        0.1124        \u001b[32m0.2743\u001b[0m  1.2371\n",
            "      3        0.1127        \u001b[32m0.1939\u001b[0m  1.2284\n",
            "      4        \u001b[36m0.0870\u001b[0m        \u001b[32m0.1639\u001b[0m  1.2470\n",
            "      5        \u001b[36m0.0643\u001b[0m        \u001b[32m0.1061\u001b[0m  1.2249\n",
            "      6        \u001b[36m0.0600\u001b[0m        0.1634  1.2284\n",
            "      7        \u001b[36m0.0528\u001b[0m        0.2883  1.5299\n",
            "      8        \u001b[36m0.0483\u001b[0m        0.2390  1.6363\n",
            "      9        \u001b[36m0.0457\u001b[0m        0.1712  1.4346\n",
            "     10        \u001b[36m0.0456\u001b[0m        0.1694  1.2204\n",
            "     11        \u001b[36m0.0446\u001b[0m        0.1423  1.2225\n",
            "     12        \u001b[36m0.0435\u001b[0m        0.1066  1.2419\n",
            "     13        0.0435        0.1416  1.2322\n",
            "     14        \u001b[36m0.0416\u001b[0m        0.1613  1.2267\n",
            "     15        \u001b[36m0.0415\u001b[0m        0.1424  1.2285\n",
            "     16        \u001b[36m0.0404\u001b[0m        0.1397  1.2298\n",
            "     17        \u001b[36m0.0397\u001b[0m        0.1392  1.4904\n",
            "     18        \u001b[36m0.0377\u001b[0m        0.1356  1.6170\n",
            "     19        \u001b[36m0.0358\u001b[0m        0.1754  1.6233\n",
            "     20        \u001b[36m0.0326\u001b[0m        0.1348  1.2245\n",
            "     21        \u001b[36m0.0304\u001b[0m        0.1202  1.2263\n",
            "     22        \u001b[36m0.0284\u001b[0m        \u001b[32m0.1008\u001b[0m  1.2327\n",
            "     23        0.0287        \u001b[32m0.0806\u001b[0m  1.2350\n",
            "     24        \u001b[36m0.0274\u001b[0m        0.0956  1.2453\n",
            "     25        \u001b[36m0.0267\u001b[0m        0.1188  1.2073\n",
            "     26        0.0271        0.1023  1.2272\n",
            "     27        \u001b[36m0.0258\u001b[0m        0.1002  1.2789\n",
            "     28        0.0260        0.1108  1.5894\n",
            "     29        \u001b[36m0.0249\u001b[0m        0.1056  1.6470\n",
            "     30        0.0258        0.0934  1.3453\n",
            "     31        \u001b[36m0.0244\u001b[0m        0.1136  1.2610\n",
            "     32        0.0252        0.1057  1.2287\n",
            "     33        0.0253        0.1131  1.2548\n",
            "     34        \u001b[36m0.0243\u001b[0m        0.1041  1.2374\n",
            "     35        0.0248        0.0875  1.2385\n",
            "     36        \u001b[36m0.0237\u001b[0m        0.0934  1.2406\n",
            "     37        \u001b[36m0.0236\u001b[0m        0.0985  1.2397\n",
            "     38        \u001b[36m0.0233\u001b[0m        0.0988  1.5474\n",
            "     39        0.0237        0.1037  1.6371\n",
            "     40        0.0237        0.0923  1.4709\n",
            "     41        0.0234        0.1020  1.2154\n",
            "     42        0.0238        0.1023  1.2190\n",
            "     43        0.0236        0.0960  1.2184\n",
            "     44        0.0238        0.1052  1.2392\n",
            "     45        0.0234        0.0882  1.2450\n",
            "     46        0.0234        0.0951  1.2302\n",
            "     47        0.0233        0.0855  1.2384\n",
            "     48        \u001b[36m0.0229\u001b[0m        \u001b[32m0.0703\u001b[0m  1.3897\n",
            "     49        \u001b[36m0.0226\u001b[0m        0.0944  1.6118\n",
            "     50        0.0234        0.0978  1.6755\n",
            "     51        0.0233        0.0902  1.2264\n",
            "     52        0.0230        0.1099  1.2247\n",
            "     53        0.0229        0.0771  1.2089\n",
            "     54        0.0228        0.0805  1.2505\n",
            "     55        \u001b[36m0.0224\u001b[0m        0.0796  1.2441\n",
            "     56        \u001b[36m0.0218\u001b[0m        0.0853  1.2243\n",
            "     57        0.0223        0.0976  1.2279\n",
            "     58        0.0223        0.0751  1.2233\n",
            "     59        0.0226        0.0824  1.6124\n",
            "     60        0.0220        0.0867  1.6345\n",
            "     61        0.0223        0.0760  1.3858\n",
            "     62        0.0221        0.0769  1.2390\n",
            "     63        0.0219        \u001b[32m0.0652\u001b[0m  1.2249\n",
            "     64        \u001b[36m0.0216\u001b[0m        0.0852  1.2328\n",
            "     65        0.0221        0.0681  1.2503\n",
            "     66        0.0217        0.0745  1.2244\n",
            "     67        \u001b[36m0.0215\u001b[0m        0.0705  1.2274\n",
            "     68        \u001b[36m0.0212\u001b[0m        \u001b[32m0.0583\u001b[0m  1.2235\n",
            "     69        \u001b[36m0.0208\u001b[0m        \u001b[32m0.0536\u001b[0m  1.4945\n",
            "     70        0.0211        0.0602  1.6050\n",
            "     71        0.0210        0.0705  1.5595\n",
            "     72        0.0208        0.0687  1.2337\n",
            "     73        0.0211        0.0583  1.2408\n",
            "     74        \u001b[36m0.0207\u001b[0m        \u001b[32m0.0506\u001b[0m  1.2180\n",
            "     75        \u001b[36m0.0207\u001b[0m        0.0527  1.2124\n",
            "     76        \u001b[36m0.0206\u001b[0m        0.0511  1.2671\n",
            "     77        \u001b[36m0.0199\u001b[0m        \u001b[32m0.0440\u001b[0m  1.2342\n",
            "     78        \u001b[36m0.0198\u001b[0m        0.0446  1.2522\n",
            "     79        0.0203        0.0471  1.3227\n",
            "     80        \u001b[36m0.0198\u001b[0m        0.0454  1.6558\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0931\u001b[0m        \u001b[32m0.3073\u001b[0m  1.6131\n",
            "      2        0.1058        \u001b[32m0.2696\u001b[0m  1.2167\n",
            "      3        0.1099        0.2939  1.2166\n",
            "      4        0.1185        \u001b[32m0.2395\u001b[0m  1.2723\n",
            "      5        0.1210        \u001b[32m0.2143\u001b[0m  1.2466\n",
            "      6        0.1202        0.2214  1.2213\n",
            "      7        0.1173        0.2179  1.2097\n",
            "      8        0.1113        \u001b[32m0.2136\u001b[0m  1.2252\n",
            "      9        0.1097        \u001b[32m0.1791\u001b[0m  1.3190\n",
            "     10        0.1035        \u001b[32m0.1686\u001b[0m  1.6210\n",
            "     11        0.1068        0.1945  1.6700\n",
            "     12        0.1003        \u001b[32m0.1123\u001b[0m  1.3556\n",
            "     13        \u001b[36m0.0898\u001b[0m        \u001b[32m0.0859\u001b[0m  1.2497\n",
            "     14        \u001b[36m0.0801\u001b[0m        0.1556  1.2419\n",
            "     15        0.0802        0.1374  1.2357\n",
            "     16        \u001b[36m0.0664\u001b[0m        \u001b[32m0.0815\u001b[0m  1.2285\n",
            "     17        \u001b[36m0.0586\u001b[0m        \u001b[32m0.0762\u001b[0m  1.2277\n",
            "     18        \u001b[36m0.0568\u001b[0m        \u001b[32m0.0722\u001b[0m  1.2818\n",
            "     19        0.0590        0.0878  1.6257\n",
            "     20        0.0572        \u001b[32m0.0600\u001b[0m  1.8976\n",
            "     21        \u001b[36m0.0550\u001b[0m        0.0825  1.7893\n",
            "     22        0.0563        0.0652  1.6706\n",
            "     23        \u001b[36m0.0548\u001b[0m        0.1247  1.2157\n",
            "     24        0.0588        0.1083  1.2381\n",
            "     25        0.0561        0.0622  1.2427\n",
            "     26        \u001b[36m0.0540\u001b[0m        0.1040  1.2636\n",
            "     27        0.0564        \u001b[32m0.0562\u001b[0m  1.2205\n",
            "     28        \u001b[36m0.0526\u001b[0m        0.0925  1.2274\n",
            "     29        0.0538        0.0738  1.2258\n",
            "     30        0.0534        0.0570  1.2233\n",
            "     31        \u001b[36m0.0524\u001b[0m        0.0889  1.6051\n",
            "     32        0.0530        0.0579  1.6610\n",
            "     33        \u001b[36m0.0513\u001b[0m        0.1133  1.3593\n",
            "     34        0.0538        \u001b[32m0.0532\u001b[0m  1.2135\n",
            "     35        0.0519        0.0558  1.1982\n",
            "     36        \u001b[36m0.0512\u001b[0m        0.1080  1.1986\n",
            "     37        0.0518        0.0620  1.2326\n",
            "     38        \u001b[36m0.0504\u001b[0m        0.0786  1.2363\n",
            "     39        0.0533        0.1065  1.2366\n",
            "     40        0.0521        0.0746  1.2343\n",
            "     41        \u001b[36m0.0493\u001b[0m        0.0722  1.4739\n",
            "     42        0.0545        0.4955  1.6077\n",
            "     43        0.0870        0.1965  1.5644\n",
            "     44        0.0595        0.1940  1.2370\n",
            "     45        0.0605        0.0968  1.2272\n",
            "     46        0.0558        0.1131  1.2267\n",
            "     47        0.0531        0.1582  1.2950\n",
            "     48        0.0519        0.0857  1.2598\n",
            "     49        0.0499        0.0770  1.2993\n",
            "     50        0.0502        0.0738  1.2505\n",
            "     51        \u001b[36m0.0488\u001b[0m        0.0801  1.3939\n",
            "     52        0.0495        0.0746  1.6152\n",
            "     53        \u001b[36m0.0483\u001b[0m        0.0695  1.7329\n",
            "     54        \u001b[36m0.0476\u001b[0m        0.0675  1.2407\n",
            "     55        0.0485        0.0577  1.3524\n",
            "     56        \u001b[36m0.0459\u001b[0m        0.0712  1.2480\n",
            "     57        0.0470        0.0676  1.2267\n",
            "     58        0.0465        0.0643  1.2272\n",
            "     59        \u001b[36m0.0459\u001b[0m        0.0701  1.2297\n",
            "     60        0.0470        0.0612  1.2246\n",
            "     61        \u001b[36m0.0449\u001b[0m        0.0717  1.2599\n",
            "     62        0.0451        0.0605  1.6060\n",
            "     63        0.0453        0.0603  1.6595\n",
            "     64        \u001b[36m0.0443\u001b[0m        0.0626  1.3792\n",
            "     65        \u001b[36m0.0421\u001b[0m        0.0760  1.2202\n",
            "     66        0.0452        0.0603  1.2870\n",
            "     67        \u001b[36m0.0409\u001b[0m        0.0666  1.2339\n",
            "     68        0.0419        0.0661  1.2278\n",
            "     69        0.0422        0.0615  1.2302\n",
            "     70        0.0430        0.0639  1.2837\n",
            "     71        0.0415        0.0663  1.2790\n",
            "     72        0.0418        0.0585  1.5188\n",
            "     73        \u001b[36m0.0405\u001b[0m        0.0659  1.6284\n",
            "     74        \u001b[36m0.0404\u001b[0m        0.0572  1.4432\n",
            "     75        \u001b[36m0.0398\u001b[0m        0.0767  1.2310\n",
            "     76        0.0408        0.0555  1.2388\n",
            "     77        \u001b[36m0.0392\u001b[0m        0.0695  1.2422\n",
            "     78        \u001b[36m0.0383\u001b[0m        0.0701  1.2302\n",
            "     79        0.0394        \u001b[32m0.0521\u001b[0m  1.2029\n",
            "     80        \u001b[36m0.0380\u001b[0m        0.0571  1.2215\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0939\u001b[0m        \u001b[32m0.2592\u001b[0m  1.1481\n",
            "      2        0.0941        0.3005  1.3461\n",
            "      3        0.0973        0.2730  1.4918\n",
            "      4        \u001b[36m0.0918\u001b[0m        \u001b[32m0.1774\u001b[0m  1.5413\n",
            "      5        \u001b[36m0.0734\u001b[0m        \u001b[32m0.1671\u001b[0m  1.1566\n",
            "      6        \u001b[36m0.0710\u001b[0m        \u001b[32m0.1506\u001b[0m  1.1371\n",
            "      7        0.0721        0.1593  1.1269\n",
            "      8        \u001b[36m0.0709\u001b[0m        \u001b[32m0.1503\u001b[0m  1.1512\n",
            "      9        \u001b[36m0.0678\u001b[0m        0.1677  1.1772\n",
            "     10        \u001b[36m0.0677\u001b[0m        0.1682  1.1616\n",
            "     11        0.0701        \u001b[32m0.1487\u001b[0m  1.1580\n",
            "     12        0.0691        \u001b[32m0.1461\u001b[0m  1.1332\n",
            "     13        \u001b[36m0.0645\u001b[0m        0.1487  1.2728\n",
            "     14        0.0677        0.1498  1.4791\n",
            "     15        \u001b[36m0.0634\u001b[0m        0.1549  1.5218\n",
            "     16        \u001b[36m0.0627\u001b[0m        0.1495  1.2491\n",
            "     17        0.0651        0.1607  1.1477\n",
            "     18        0.0652        0.1562  1.1340\n",
            "     19        0.0636        0.1555  1.1430\n",
            "     20        0.0650        0.1638  1.1491\n",
            "     21        0.0665        0.1703  1.1467\n",
            "     22        0.0636        0.1760  1.1397\n",
            "     23        0.0636        0.1647  1.1418\n",
            "     24        \u001b[36m0.0618\u001b[0m        0.1741  1.1669\n",
            "     25        0.0620        0.1776  1.4716\n",
            "     26        0.0632        0.1800  1.4888\n",
            "     27        0.0643        0.1668  1.3540\n",
            "     28        0.0650        0.1518  1.1427\n",
            "     29        0.0647        0.1657  1.1423\n",
            "     30        0.0666        0.1566  1.1352\n",
            "     31        0.0663        0.1576  1.1403\n",
            "     32        0.0625        \u001b[32m0.1459\u001b[0m  1.1443\n",
            "     33        \u001b[36m0.0609\u001b[0m        \u001b[32m0.1331\u001b[0m  1.1379\n",
            "     34        \u001b[36m0.0566\u001b[0m        \u001b[32m0.1324\u001b[0m  1.1369\n",
            "     35        0.0575        \u001b[32m0.1266\u001b[0m  1.1266\n",
            "     36        0.0594        \u001b[32m0.1121\u001b[0m  1.3750\n",
            "     37        0.0571        0.1124  1.4841\n",
            "     38        0.0570        \u001b[32m0.0881\u001b[0m  1.4430\n",
            "     39        \u001b[36m0.0541\u001b[0m        \u001b[32m0.0736\u001b[0m  1.1685\n",
            "     40        \u001b[36m0.0391\u001b[0m        0.0857  1.1570\n",
            "     41        \u001b[36m0.0323\u001b[0m        \u001b[32m0.0537\u001b[0m  1.1488\n",
            "     42        \u001b[36m0.0215\u001b[0m        \u001b[32m0.0393\u001b[0m  1.1282\n",
            "     43        \u001b[36m0.0112\u001b[0m        0.0428  1.1237\n",
            "     44        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0292\u001b[0m  1.1255\n",
            "     45        \u001b[36m0.0081\u001b[0m        \u001b[32m0.0193\u001b[0m  1.1423\n",
            "     46        \u001b[36m0.0072\u001b[0m        \u001b[32m0.0190\u001b[0m  1.1293\n",
            "     47        \u001b[36m0.0066\u001b[0m        \u001b[32m0.0156\u001b[0m  1.3050\n",
            "     48        \u001b[36m0.0057\u001b[0m        \u001b[32m0.0133\u001b[0m  1.4808\n",
            "     49        \u001b[36m0.0056\u001b[0m        0.0159  1.5470\n",
            "     50        \u001b[36m0.0052\u001b[0m        \u001b[32m0.0112\u001b[0m  1.1456\n",
            "     51        \u001b[36m0.0048\u001b[0m        0.0208  1.1496\n",
            "     52        0.0050        0.0201  1.1327\n",
            "     53        0.0051        0.0168  1.1462\n",
            "     54        \u001b[36m0.0042\u001b[0m        0.0153  1.1743\n",
            "     55        \u001b[36m0.0039\u001b[0m        0.0146  1.1713\n",
            "     56        \u001b[36m0.0035\u001b[0m        0.0147  1.1590\n",
            "     57        \u001b[36m0.0031\u001b[0m        0.0136  1.1460\n",
            "     58        \u001b[36m0.0028\u001b[0m        0.0198  1.2869\n",
            "     59        0.0030        0.0195  1.4721\n",
            "     60        \u001b[36m0.0028\u001b[0m        0.0168  1.5184\n",
            "     61        \u001b[36m0.0027\u001b[0m        0.0140  1.2176\n",
            "     62        \u001b[36m0.0024\u001b[0m        \u001b[32m0.0101\u001b[0m  1.1442\n",
            "     63        0.0025        \u001b[32m0.0100\u001b[0m  1.1470\n",
            "     64        \u001b[36m0.0022\u001b[0m        \u001b[32m0.0097\u001b[0m  1.1483\n",
            "     65        \u001b[36m0.0022\u001b[0m        \u001b[32m0.0094\u001b[0m  1.1391\n",
            "     66        0.0022        \u001b[32m0.0087\u001b[0m  1.1247\n",
            "     67        \u001b[36m0.0021\u001b[0m        \u001b[32m0.0085\u001b[0m  1.1418\n",
            "     68        0.0021        0.0086  1.1448\n",
            "     69        0.0022        0.0096  1.2124\n",
            "     70        0.0021        0.0110  1.4528\n",
            "     71        0.0029        0.0094  1.5222\n",
            "     72        0.0023        0.0106  1.3168\n",
            "     73        0.0023        0.0126  1.1431\n",
            "     74        0.0022        0.0116  1.1500\n",
            "     75        0.0021        0.0109  1.1436\n",
            "     76        \u001b[36m0.0020\u001b[0m        0.0108  1.1476\n",
            "     77        0.0020        0.0107  1.1390\n",
            "     78        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0055\u001b[0m  1.1385\n",
            "     79        0.0021        \u001b[32m0.0054\u001b[0m  1.1793\n",
            "     80        \u001b[36m0.0019\u001b[0m        0.0065  1.1474\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.6min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0932\u001b[0m        \u001b[32m0.2372\u001b[0m  1.4807\n",
            "      2        \u001b[36m0.0820\u001b[0m        \u001b[32m0.2080\u001b[0m  1.5276\n",
            "      3        \u001b[36m0.0702\u001b[0m        \u001b[32m0.1739\u001b[0m  1.2089\n",
            "      4        \u001b[36m0.0670\u001b[0m        \u001b[32m0.1587\u001b[0m  1.1322\n",
            "      5        0.0674        0.1644  1.1442\n",
            "      6        \u001b[36m0.0666\u001b[0m        0.1725  1.1463\n",
            "      7        \u001b[36m0.0654\u001b[0m        0.1745  1.1558\n",
            "      8        \u001b[36m0.0643\u001b[0m        0.1811  1.1715\n",
            "      9        \u001b[36m0.0624\u001b[0m        0.1956  1.1442\n",
            "     10        0.0634        0.1941  1.1409\n",
            "     11        0.0635        0.1916  1.2023\n",
            "     12        \u001b[36m0.0619\u001b[0m        0.1883  1.5008\n",
            "     13        0.0628        0.2009  1.4812\n",
            "     14        0.0628        0.1887  1.3097\n",
            "     15        0.0623        0.1907  1.1504\n",
            "     16        0.0645        0.1800  1.1231\n",
            "     17        0.0632        0.1921  1.1536\n",
            "     18        0.0632        0.1834  1.1377\n",
            "     19        0.0625        0.1885  1.1229\n",
            "     20        0.0643        0.1827  1.1430\n",
            "     21        0.0636        0.1822  1.1392\n",
            "     22        0.0643        0.1872  1.1543\n",
            "     23        0.0636        0.1799  1.4531\n",
            "     24        0.0639        0.1796  1.5213\n",
            "     25        0.0633        0.1782  1.4180\n",
            "     26        0.0626        0.1821  1.1553\n",
            "     27        0.0644        0.1768  1.1436\n",
            "     28        0.0638        0.1829  1.1436\n",
            "     29        0.0620        0.1842  1.1466\n",
            "     30        0.0643        0.1792  1.1808\n",
            "     31        0.0630        0.1818  1.1796\n",
            "     32        0.0634        0.1845  1.1926\n",
            "     33        0.0647        0.1846  1.1753\n",
            "     34        0.0640        0.1844  1.4267\n",
            "     35        0.0642        0.1843  1.5162\n",
            "     36        0.0624        0.1851  1.5265\n",
            "     37        0.0630        0.1917  1.2019\n",
            "     38        0.0626        0.1863  1.1611\n",
            "     39        0.0640        0.2313  1.1622\n",
            "     40        0.0651        0.1907  1.2242\n",
            "     41        0.0646        0.1853  1.1717\n",
            "     42        0.0635        0.1843  1.1582\n",
            "     43        0.0660        0.2198  1.1738\n",
            "     44        0.0649        0.1900  1.5082\n",
            "     45        0.0640        0.1940  1.8110\n",
            "     46        0.0652        0.1819  1.7848\n",
            "     47        0.0658        0.2232  1.6488\n",
            "     48        0.0669        0.2135  1.2925\n",
            "     49        0.0634        0.1919  1.1759\n",
            "     50        0.0678        0.1955  1.1722\n",
            "     51        0.0651        0.2066  1.1481\n",
            "     52        0.0646        0.1909  1.1727\n",
            "     53        0.0652        0.1880  1.1682\n",
            "     54        0.0656        0.2035  1.1797\n",
            "     55        0.0662        0.1990  1.2277\n",
            "     56        0.0635        0.1856  1.3180\n",
            "     57        0.0659        0.2202  1.5268\n",
            "     58        0.0668        0.1897  1.5592\n",
            "     59        0.0649        0.2291  1.2532\n",
            "     60        0.0673        0.2307  1.1775\n",
            "     61        0.0684        0.2150  1.1704\n",
            "     62        0.0649        0.1965  1.2210\n",
            "     63        0.0657        0.2559  1.2013\n",
            "     64        0.0698        0.2573  1.1927\n",
            "     65        0.0690        0.2435  1.1831\n",
            "     66        0.0707        0.2423  1.1557\n",
            "     67        0.0691        0.2684  1.3197\n",
            "     68        0.0720        0.2667  1.5352\n",
            "     69        0.0710        0.2680  1.6070\n",
            "     70        0.0711        0.3023  1.1834\n",
            "     71        0.0681        0.2582  1.1812\n",
            "     72        0.0731        0.2648  1.1506\n",
            "     73        0.0725        0.2661  1.1822\n",
            "     74        0.0715        0.2653  1.1495\n",
            "     75        0.0718        0.2650  1.1662\n",
            "     76        0.0725        0.2624  1.1768\n",
            "     77        0.0728        0.2638  1.1802\n",
            "     78        0.0713        0.2656  1.3390\n",
            "     79        0.0722        0.2672  1.5123\n",
            "     80        0.0716        0.2508  1.5981\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.7min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0863\u001b[0m        \u001b[32m0.3008\u001b[0m  1.2505\n",
            "      2        \u001b[36m0.0694\u001b[0m        \u001b[32m0.2791\u001b[0m  1.2569\n",
            "      3        \u001b[36m0.0681\u001b[0m        \u001b[32m0.2527\u001b[0m  1.2559\n",
            "      4        0.0725        0.3152  1.2701\n",
            "      5        0.0736        0.2991  1.2636\n",
            "      6        \u001b[36m0.0668\u001b[0m        0.3017  1.2600\n",
            "      7        0.0728        0.3049  1.2102\n",
            "      8        0.0677        0.2895  1.3738\n",
            "      9        0.0792        0.3003  1.6240\n",
            "     10        0.0750        \u001b[32m0.2307\u001b[0m  1.7181\n",
            "     11        \u001b[36m0.0614\u001b[0m        0.2373  1.3145\n",
            "     12        \u001b[36m0.0614\u001b[0m        0.2314  1.2694\n",
            "     13        0.0650        \u001b[32m0.2063\u001b[0m  1.2604\n",
            "     14        \u001b[36m0.0572\u001b[0m        \u001b[32m0.1557\u001b[0m  1.2888\n",
            "     15        0.0699        0.1605  1.2508\n",
            "     16        0.0691        \u001b[32m0.1133\u001b[0m  1.2721\n",
            "     17        0.0630        \u001b[32m0.1023\u001b[0m  1.2603\n",
            "     18        0.0612        0.1201  1.2479\n",
            "     19        0.0643        0.1303  1.6589\n",
            "     20        \u001b[36m0.0564\u001b[0m        0.1317  1.6574\n",
            "     21        \u001b[36m0.0512\u001b[0m        0.1077  1.4284\n",
            "     22        \u001b[36m0.0484\u001b[0m        \u001b[32m0.0789\u001b[0m  1.2549\n",
            "     23        \u001b[36m0.0438\u001b[0m        0.1045  1.2631\n",
            "     24        \u001b[36m0.0423\u001b[0m        \u001b[32m0.0687\u001b[0m  1.2384\n",
            "     25        \u001b[36m0.0368\u001b[0m        \u001b[32m0.0408\u001b[0m  1.2766\n",
            "     26        \u001b[36m0.0335\u001b[0m        0.0495  1.2509\n",
            "     27        0.0357        0.0639  1.2744\n",
            "     28        \u001b[36m0.0332\u001b[0m        0.0599  1.2551\n",
            "     29        \u001b[36m0.0301\u001b[0m        0.0474  1.5840\n",
            "     30        \u001b[36m0.0292\u001b[0m        0.0577  1.6750\n",
            "     31        \u001b[36m0.0279\u001b[0m        \u001b[32m0.0379\u001b[0m  1.4495\n",
            "     32        0.0288        0.0541  1.2589\n",
            "     33        \u001b[36m0.0271\u001b[0m        0.0586  1.2703\n",
            "     34        0.0271        \u001b[32m0.0360\u001b[0m  1.2454\n",
            "     35        \u001b[36m0.0262\u001b[0m        \u001b[32m0.0348\u001b[0m  1.2424\n",
            "     36        \u001b[36m0.0257\u001b[0m        0.0411  1.2581\n",
            "     37        \u001b[36m0.0252\u001b[0m        0.0505  1.2580\n",
            "     38        \u001b[36m0.0232\u001b[0m        0.0569  1.2404\n",
            "     39        \u001b[36m0.0219\u001b[0m        0.0554  1.4785\n",
            "     40        \u001b[36m0.0213\u001b[0m        0.0491  1.6499\n",
            "     41        \u001b[36m0.0208\u001b[0m        0.0362  1.5448\n",
            "     42        \u001b[36m0.0194\u001b[0m        0.0362  1.2623\n",
            "     43        \u001b[36m0.0181\u001b[0m        \u001b[32m0.0343\u001b[0m  1.2655\n",
            "     44        0.0181        \u001b[32m0.0280\u001b[0m  1.2418\n",
            "     45        \u001b[36m0.0176\u001b[0m        0.0290  1.2147\n",
            "     46        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0232\u001b[0m  1.2546\n",
            "     47        \u001b[36m0.0160\u001b[0m        0.0256  1.2529\n",
            "     48        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0219\u001b[0m  1.2606\n",
            "     49        0.0160        \u001b[32m0.0154\u001b[0m  1.4048\n",
            "     50        \u001b[36m0.0157\u001b[0m        0.0157  1.6426\n",
            "     51        \u001b[36m0.0146\u001b[0m        0.0158  1.6274\n",
            "     52        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0108\u001b[0m  1.2583\n",
            "     53        0.0142        0.0126  1.2577\n",
            "     54        \u001b[36m0.0134\u001b[0m        0.0142  1.2248\n",
            "     55        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0095\u001b[0m  1.2408\n",
            "     56        \u001b[36m0.0120\u001b[0m        0.0126  1.2935\n",
            "     57        0.0120        0.0144  1.2583\n",
            "     58        0.0120        0.0117  1.2362\n",
            "     59        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0088\u001b[0m  1.3105\n",
            "     60        \u001b[36m0.0109\u001b[0m        0.0109  1.6366\n",
            "     61        0.0113        0.0096  1.7040\n",
            "     62        0.0112        0.0093  1.2894\n",
            "     63        \u001b[36m0.0107\u001b[0m        0.0124  1.2335\n",
            "     64        \u001b[36m0.0099\u001b[0m        0.0115  1.2328\n",
            "     65        \u001b[36m0.0092\u001b[0m        0.0227  1.2414\n",
            "     66        0.0106        0.0173  1.2601\n",
            "     67        0.0098        0.0177  1.2339\n",
            "     68        0.0099        0.0134  1.2308\n",
            "     69        0.0095        0.0169  1.2227\n",
            "     70        0.0092        0.0170  1.5700\n",
            "     71        \u001b[36m0.0091\u001b[0m        0.0170  1.6485\n",
            "     72        0.0091        0.0183  1.4379\n",
            "     73        0.0096        0.0162  1.2309\n",
            "     74        0.0091        0.0140  1.2404\n",
            "     75        0.0091        0.0166  1.2381\n",
            "     76        \u001b[36m0.0091\u001b[0m        0.0194  1.2230\n",
            "     77        0.0092        0.0239  1.2313\n",
            "     78        0.0095        0.0181  1.2156\n",
            "     79        \u001b[36m0.0086\u001b[0m        0.0304  1.2421\n",
            "     80        0.0091        0.0162  1.4207\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0818\u001b[0m        \u001b[32m0.3000\u001b[0m  1.6182\n",
            "      2        \u001b[36m0.0699\u001b[0m        0.3182  1.4740\n",
            "      3        0.0707        \u001b[32m0.2810\u001b[0m  1.2322\n",
            "      4        \u001b[36m0.0696\u001b[0m        0.3324  1.2378\n",
            "      5        \u001b[36m0.0684\u001b[0m        0.3122  1.2391\n",
            "      6        0.0685        \u001b[32m0.2675\u001b[0m  1.2591\n",
            "      7        \u001b[36m0.0654\u001b[0m        0.3013  1.2435\n",
            "      8        0.0691        0.3056  1.2752\n",
            "      9        0.0926        0.3346  1.2173\n",
            "     10        0.0816        0.2922  1.4459\n",
            "     11        0.0743        0.3224  1.6583\n",
            "     12        0.0846        0.3246  1.5768\n",
            "     13        0.0840        0.3160  1.2293\n",
            "     14        0.0745        0.3163  1.2232\n",
            "     15        0.0769        0.2748  1.2240\n",
            "     16        \u001b[36m0.0652\u001b[0m        0.2908  1.2395\n",
            "     17        \u001b[36m0.0589\u001b[0m        0.2804  1.2316\n",
            "     18        0.0642        \u001b[32m0.2123\u001b[0m  1.2387\n",
            "     19        0.0623        0.2876  1.2336\n",
            "     20        0.0725        0.2363  1.2885\n",
            "     21        0.0606        \u001b[32m0.1815\u001b[0m  1.6191\n",
            "     22        \u001b[36m0.0495\u001b[0m        0.2611  1.6580\n",
            "     23        0.0610        \u001b[32m0.1502\u001b[0m  1.3178\n",
            "     24        0.0496        0.1901  1.2291\n",
            "     25        0.0572        0.1695  1.2114\n",
            "     26        0.0499        0.1597  1.2351\n",
            "     27        0.0508        \u001b[32m0.1360\u001b[0m  1.2185\n",
            "     28        0.0562        0.1380  1.2498\n",
            "     29        0.0526        \u001b[32m0.1081\u001b[0m  1.2546\n",
            "     30        0.0528        0.1188  1.2439\n",
            "     31        0.0500        0.1088  1.5447\n",
            "     32        0.0505        \u001b[32m0.0965\u001b[0m  1.5926\n",
            "     33        \u001b[36m0.0490\u001b[0m        0.1171  1.5067\n",
            "     34        \u001b[36m0.0463\u001b[0m        \u001b[32m0.0920\u001b[0m  1.2274\n",
            "     35        0.0464        0.2124  1.2378\n",
            "     36        0.0512        \u001b[32m0.0894\u001b[0m  1.2319\n",
            "     37        0.0555        0.2465  1.2288\n",
            "     38        0.0562        0.0929  1.2310\n",
            "     39        0.0496        \u001b[32m0.0455\u001b[0m  1.2292\n",
            "     40        \u001b[36m0.0455\u001b[0m        0.0591  1.2260\n",
            "     41        \u001b[36m0.0436\u001b[0m        0.1219  1.3313\n",
            "     42        \u001b[36m0.0427\u001b[0m        0.1631  1.6375\n",
            "     43        0.0458        0.0651  1.6795\n",
            "     44        \u001b[36m0.0424\u001b[0m        0.0822  1.2542\n",
            "     45        \u001b[36m0.0379\u001b[0m        0.0555  1.2276\n",
            "     46        \u001b[36m0.0364\u001b[0m        0.0911  1.2273\n",
            "     47        \u001b[36m0.0322\u001b[0m        0.0842  1.2343\n",
            "     48        \u001b[36m0.0312\u001b[0m        0.0704  1.2479\n",
            "     49        0.0353        0.0719  1.2302\n",
            "     50        0.0362        0.0578  1.2208\n",
            "     51        0.0348        0.0729  1.2405\n",
            "     52        0.0367        0.0533  1.6260\n",
            "     53        0.0366        0.0495  1.6565\n",
            "     54        0.0317        0.0737  1.3811\n",
            "     55        0.0366        0.0485  1.2304\n",
            "     56        0.0359        0.0542  1.2397\n",
            "     57        0.0321        0.0802  1.2257\n",
            "     58        0.0327        0.0653  1.2404\n",
            "     59        0.0367        0.0618  1.2445\n",
            "     60        0.0327        0.0667  1.4551\n",
            "     61        0.0337        0.0704  1.6635\n",
            "     62        0.0329        0.0660  1.8772\n",
            "     63        0.0347        0.0655  1.6902\n",
            "     64        0.0334        0.0599  1.6040\n",
            "     65        0.0341        0.0737  1.2512\n",
            "     66        0.0345        0.0594  1.2297\n",
            "     67        0.0330        0.0849  1.2280\n",
            "     68        0.0334        0.0678  1.2493\n",
            "     69        0.0341        0.0716  1.2251\n",
            "     70        0.0375        0.0469  1.2522\n",
            "     71        0.0361        0.0521  1.2353\n",
            "     72        0.0363        0.0542  1.2983\n",
            "     73        0.0338        0.0650  1.6256\n",
            "     74        0.0334        0.0632  1.6596\n",
            "     75        0.0330        0.0536  1.3322\n",
            "     76        0.0331        0.0556  1.2345\n",
            "     77        0.0336        0.0673  1.2145\n",
            "     78        0.0348        0.0652  1.2201\n",
            "     79        0.0340        0.0573  1.2260\n",
            "     80        0.0317        0.0607  1.2311\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0882\u001b[0m        \u001b[32m0.2642\u001b[0m  1.1673\n",
            "      2        \u001b[36m0.0770\u001b[0m        \u001b[32m0.1704\u001b[0m  1.1483\n",
            "      3        \u001b[36m0.0671\u001b[0m        \u001b[32m0.1496\u001b[0m  1.4512\n",
            "      4        \u001b[36m0.0670\u001b[0m        0.1589  1.4764\n",
            "      5        0.0688        0.1654  1.4137\n",
            "      6        \u001b[36m0.0632\u001b[0m        0.1690  1.1361\n",
            "      7        0.0640        0.1693  1.1419\n",
            "      8        \u001b[36m0.0621\u001b[0m        0.2085  1.1592\n",
            "      9        0.0659        0.1625  1.1395\n",
            "     10        \u001b[36m0.0605\u001b[0m        0.2038  1.1213\n",
            "     11        0.0634        0.1597  1.1407\n",
            "     12        0.0610        0.1568  1.1491\n",
            "     13        \u001b[36m0.0599\u001b[0m        0.1549  1.1671\n",
            "     14        \u001b[36m0.0593\u001b[0m        0.1664  1.3960\n",
            "     15        0.0615        0.1578  1.4873\n",
            "     16        0.0599        0.1562  1.5043\n",
            "     17        0.0613        0.1728  1.1491\n",
            "     18        \u001b[36m0.0584\u001b[0m        0.1656  1.1408\n",
            "     19        \u001b[36m0.0583\u001b[0m        0.1712  1.1657\n",
            "     20        \u001b[36m0.0572\u001b[0m        0.1829  1.1453\n",
            "     21        0.0577        0.1826  1.1397\n",
            "     22        0.0593        0.2206  1.1419\n",
            "     23        0.0598        0.2000  1.1810\n",
            "     24        0.0584        0.2133  1.1697\n",
            "     25        0.0583        0.2141  1.3215\n",
            "     26        0.0581        0.2461  1.4757\n",
            "     27        0.0613        0.1925  1.5780\n",
            "     28        0.0590        0.1737  1.1649\n",
            "     29        0.0573        0.1712  1.1534\n",
            "     30        0.0581        0.1683  1.1451\n",
            "     31        0.0606        0.1799  1.1696\n",
            "     32        0.0609        0.1638  1.1710\n",
            "     33        0.0584        0.2002  1.1469\n",
            "     34        0.0590        0.1532  1.1378\n",
            "     35        0.0574        0.1814  1.1440\n",
            "     36        0.0589        0.1653  1.2630\n",
            "     37        0.0587        0.1767  1.4948\n",
            "     38        0.0575        0.2176  1.5167\n",
            "     39        0.0582        0.2601  1.1342\n",
            "     40        \u001b[36m0.0547\u001b[0m        0.2551  1.1463\n",
            "     41        0.0648        0.1676  1.1564\n",
            "     42        0.0552        0.1779  1.1476\n",
            "     43        \u001b[36m0.0541\u001b[0m        0.1665  1.1196\n",
            "     44        \u001b[36m0.0523\u001b[0m        0.1710  1.1479\n",
            "     45        \u001b[36m0.0501\u001b[0m        0.2691  1.1449\n",
            "     46        \u001b[36m0.0427\u001b[0m        \u001b[32m0.1384\u001b[0m  1.1468\n",
            "     47        \u001b[36m0.0242\u001b[0m        \u001b[32m0.1349\u001b[0m  1.2702\n",
            "     48        \u001b[36m0.0150\u001b[0m        \u001b[32m0.1048\u001b[0m  1.4719\n",
            "     49        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0649\u001b[0m  1.5387\n",
            "     50        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0502\u001b[0m  1.1217\n",
            "     51        \u001b[36m0.0081\u001b[0m        0.0569  1.1449\n",
            "     52        \u001b[36m0.0075\u001b[0m        \u001b[32m0.0451\u001b[0m  1.1445\n",
            "     53        \u001b[36m0.0070\u001b[0m        \u001b[32m0.0436\u001b[0m  1.1448\n",
            "     54        \u001b[36m0.0065\u001b[0m        \u001b[32m0.0383\u001b[0m  1.1285\n",
            "     55        \u001b[36m0.0061\u001b[0m        0.0426  1.1344\n",
            "     56        \u001b[36m0.0057\u001b[0m        \u001b[32m0.0350\u001b[0m  1.1553\n",
            "     57        \u001b[36m0.0053\u001b[0m        \u001b[32m0.0308\u001b[0m  1.1799\n",
            "     58        \u001b[36m0.0048\u001b[0m        0.0326  1.2570\n",
            "     59        0.0049        \u001b[32m0.0225\u001b[0m  1.4750\n",
            "     60        \u001b[36m0.0043\u001b[0m        \u001b[32m0.0200\u001b[0m  1.5300\n",
            "     61        \u001b[36m0.0041\u001b[0m        \u001b[32m0.0132\u001b[0m  1.2419\n",
            "     62        \u001b[36m0.0038\u001b[0m        0.0192  1.1651\n",
            "     63        0.0040        0.0181  1.1512\n",
            "     64        \u001b[36m0.0037\u001b[0m        0.0258  1.1185\n",
            "     65        0.0039        0.0209  1.1504\n",
            "     66        0.0039        0.0171  1.1571\n",
            "     67        \u001b[36m0.0036\u001b[0m        0.0159  1.1522\n",
            "     68        \u001b[36m0.0034\u001b[0m        0.0180  1.1698\n",
            "     69        \u001b[36m0.0032\u001b[0m        \u001b[32m0.0122\u001b[0m  1.2240\n",
            "     70        \u001b[36m0.0029\u001b[0m        \u001b[32m0.0075\u001b[0m  1.4850\n",
            "     71        0.0033        0.0245  1.5111\n",
            "     72        0.0031        0.0157  1.2320\n",
            "     73        0.0030        0.0178  1.1445\n",
            "     74        0.0034        0.0086  1.1311\n",
            "     75        \u001b[36m0.0026\u001b[0m        0.0088  1.1286\n",
            "     76        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0068\u001b[0m  1.1277\n",
            "     77        0.0026        \u001b[32m0.0061\u001b[0m  1.1503\n",
            "     78        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0048\u001b[0m  1.1451\n",
            "     79        0.0025        \u001b[32m0.0046\u001b[0m  1.1391\n",
            "     80        0.0033        \u001b[32m0.0030\u001b[0m  1.2009\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.6min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0878\u001b[0m        \u001b[32m0.2398\u001b[0m  1.4995\n",
            "      2        0.0917        0.2517  1.5346\n",
            "      3        \u001b[36m0.0728\u001b[0m        \u001b[32m0.1482\u001b[0m  1.1553\n",
            "      4        \u001b[36m0.0644\u001b[0m        0.1901  1.1615\n",
            "      5        0.0654        0.1848  1.1383\n",
            "      6        \u001b[36m0.0626\u001b[0m        0.1828  1.1378\n",
            "      7        0.0632        0.2011  1.1452\n",
            "      8        \u001b[36m0.0620\u001b[0m        0.2030  1.1468\n",
            "      9        \u001b[36m0.0615\u001b[0m        0.1928  1.1498\n",
            "     10        0.0618        0.1831  1.1655\n",
            "     11        \u001b[36m0.0607\u001b[0m        0.1832  1.2749\n",
            "     12        \u001b[36m0.0603\u001b[0m        0.2169  1.4884\n",
            "     13        0.0662        0.1755  1.5469\n",
            "     14        \u001b[36m0.0602\u001b[0m        0.2231  1.1445\n",
            "     15        0.0669        0.2368  1.1424\n",
            "     16        0.0662        0.2186  1.1386\n",
            "     17        0.0648        0.2081  1.1447\n",
            "     18        0.0643        0.1850  1.1550\n",
            "     19        0.0662        0.1769  1.1554\n",
            "     20        0.0653        0.1830  1.1420\n",
            "     21        0.0662        0.2589  1.1415\n",
            "     22        0.0666        0.2559  1.2637\n",
            "     23        0.0675        0.2423  1.4563\n",
            "     24        0.0671        0.2551  1.5345\n",
            "     25        0.0662        0.2605  1.2061\n",
            "     26        0.0674        0.2480  1.1401\n",
            "     27        0.0667        0.2352  1.1425\n",
            "     28        0.0656        0.2468  1.1546\n",
            "     29        0.0642        0.1838  1.1505\n",
            "     30        0.0653        0.2541  1.1602\n",
            "     31        0.0667        0.2528  1.1403\n",
            "     32        0.0657        0.2506  1.1370\n",
            "     33        0.0663        0.2531  1.2412\n",
            "     34        0.0665        0.2338  1.5257\n",
            "     35        0.0658        0.2459  1.5225\n",
            "     36        0.0657        0.1608  1.2695\n",
            "     37        0.0642        \u001b[32m0.1269\u001b[0m  1.1488\n",
            "     38        0.0654        0.2017  1.1467\n",
            "     39        0.0662        0.2333  1.1490\n",
            "     40        0.0675        0.2189  1.1366\n",
            "     41        0.0664        0.2300  1.1331\n",
            "     42        0.0673        \u001b[32m0.0982\u001b[0m  1.1442\n",
            "     43        0.0648        0.2604  1.1314\n",
            "     44        0.0667        0.2473  1.1833\n",
            "     45        0.0646        0.2673  1.4915\n",
            "     46        0.0658        0.2797  1.5028\n",
            "     47        0.0670        0.2727  1.3049\n",
            "     48        0.0665        0.2634  1.1395\n",
            "     49        0.0662        0.2465  1.1530\n",
            "     50        0.0656        0.2434  1.1676\n",
            "     51        0.0653        0.2431  1.1467\n",
            "     52        0.0661        0.2576  1.1422\n",
            "     53        0.0651        0.2479  1.1602\n",
            "     54        0.0660        0.2150  1.1423\n",
            "     55        0.0645        0.2047  1.1432\n",
            "     56        0.0633        0.2155  1.5000\n",
            "     57        0.0640        0.2434  1.4940\n",
            "     58        0.0645        0.2627  1.3182\n",
            "     59        0.0649        0.2638  1.1528\n",
            "     60        0.0657        0.2565  1.1385\n",
            "     61        0.0660        0.2545  1.1765\n",
            "     62        0.0667        0.2461  1.1389\n",
            "     63        0.0661        0.2343  1.1469\n",
            "     64        0.0660        0.2325  1.1362\n",
            "     65        0.0649        0.2194  1.1379\n",
            "     66        0.0659        0.2252  1.1404\n",
            "     67        0.0651        0.2084  1.4916\n",
            "     68        0.0657        0.2495  1.5119\n",
            "     69        0.0665        0.2604  1.3617\n",
            "     70        0.0671        0.2389  1.1490\n",
            "     71        0.0668        0.2519  1.1398\n",
            "     72        0.0667        0.2256  1.1344\n",
            "     73        0.0666        0.2483  1.1424\n",
            "     74        0.0669        0.2452  1.1842\n",
            "     75        0.0663        0.2469  1.1795\n",
            "     76        0.0663        0.2339  1.1323\n",
            "     77        0.0662        0.2491  1.1492\n",
            "     78        0.0653        0.1650  1.4258\n",
            "     79        0.0651        0.2185  1.4908\n",
            "     80        0.0681        0.2539  1.4134\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.7min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0861\u001b[0m        \u001b[32m0.2830\u001b[0m  1.2569\n",
            "      2        \u001b[36m0.0669\u001b[0m        0.3003  1.2569\n",
            "      3        \u001b[36m0.0633\u001b[0m        \u001b[32m0.2772\u001b[0m  1.2799\n",
            "      4        \u001b[36m0.0603\u001b[0m        0.2915  1.2578\n",
            "      5        \u001b[36m0.0558\u001b[0m        \u001b[32m0.2660\u001b[0m  1.2759\n",
            "      6        0.0567        \u001b[32m0.2476\u001b[0m  1.2665\n",
            "      7        0.0571        \u001b[32m0.2337\u001b[0m  1.3476\n",
            "      8        \u001b[36m0.0557\u001b[0m        \u001b[32m0.2023\u001b[0m  1.8271\n",
            "      9        \u001b[36m0.0533\u001b[0m        0.2166  1.8883\n",
            "     10        0.0568        \u001b[32m0.1994\u001b[0m  1.9213\n",
            "     11        0.0533        0.2088  1.4604\n",
            "     12        \u001b[36m0.0526\u001b[0m        \u001b[32m0.1676\u001b[0m  1.2692\n",
            "     13        \u001b[36m0.0488\u001b[0m        \u001b[32m0.1408\u001b[0m  1.2704\n",
            "     14        \u001b[36m0.0472\u001b[0m        0.1471  1.2608\n",
            "     15        \u001b[36m0.0453\u001b[0m        \u001b[32m0.1407\u001b[0m  1.2523\n",
            "     16        0.0468        \u001b[32m0.1070\u001b[0m  1.2881\n",
            "     17        \u001b[36m0.0424\u001b[0m        0.1199  1.2593\n",
            "     18        \u001b[36m0.0424\u001b[0m        0.1568  1.2662\n",
            "     19        0.0434        0.1357  1.5169\n",
            "     20        0.0436        0.1314  1.6133\n",
            "     21        0.0441        0.1227  1.4741\n",
            "     22        \u001b[36m0.0408\u001b[0m        0.1171  1.2616\n",
            "     23        0.0508        0.2409  1.2661\n",
            "     24        0.0618        0.1862  1.2423\n",
            "     25        0.0515        0.1726  1.2647\n",
            "     26        0.0469        \u001b[32m0.1038\u001b[0m  1.2856\n",
            "     27        0.0423        0.1056  1.2343\n",
            "     28        0.0413        \u001b[32m0.1014\u001b[0m  1.2530\n",
            "     29        \u001b[36m0.0395\u001b[0m        \u001b[32m0.0907\u001b[0m  1.5433\n",
            "     30        0.0400        \u001b[32m0.0819\u001b[0m  1.6270\n",
            "     31        \u001b[36m0.0381\u001b[0m        \u001b[32m0.0790\u001b[0m  1.5740\n",
            "     32        0.0388        \u001b[32m0.0657\u001b[0m  1.2603\n",
            "     33        \u001b[36m0.0372\u001b[0m        \u001b[32m0.0498\u001b[0m  1.2568\n",
            "     34        0.0382        \u001b[32m0.0379\u001b[0m  1.2516\n",
            "     35        \u001b[36m0.0329\u001b[0m        0.0389  1.2544\n",
            "     36        \u001b[36m0.0329\u001b[0m        0.0466  1.2582\n",
            "     37        \u001b[36m0.0312\u001b[0m        \u001b[32m0.0371\u001b[0m  1.2568\n",
            "     38        \u001b[36m0.0298\u001b[0m        0.0472  1.2921\n",
            "     39        \u001b[36m0.0287\u001b[0m        \u001b[32m0.0282\u001b[0m  1.4176\n",
            "     40        \u001b[36m0.0261\u001b[0m        \u001b[32m0.0257\u001b[0m  1.6262\n",
            "     41        \u001b[36m0.0236\u001b[0m        0.0454  1.6643\n",
            "     42        \u001b[36m0.0221\u001b[0m        0.0569  1.2882\n",
            "     43        0.0223        0.0423  1.2450\n",
            "     44        \u001b[36m0.0211\u001b[0m        0.0348  1.2540\n",
            "     45        \u001b[36m0.0206\u001b[0m        0.0312  1.2550\n",
            "     46        \u001b[36m0.0201\u001b[0m        0.0397  1.2724\n",
            "     47        \u001b[36m0.0195\u001b[0m        \u001b[32m0.0185\u001b[0m  1.2694\n",
            "     48        \u001b[36m0.0187\u001b[0m        0.0232  1.2752\n",
            "     49        \u001b[36m0.0181\u001b[0m        0.0382  1.4047\n",
            "     50        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0165\u001b[0m  1.6213\n",
            "     51        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0161\u001b[0m  1.6998\n",
            "     52        \u001b[36m0.0169\u001b[0m        0.0173  1.2690\n",
            "     53        \u001b[36m0.0165\u001b[0m        0.0269  1.2814\n",
            "     54        \u001b[36m0.0163\u001b[0m        0.0185  1.2675\n",
            "     55        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0113\u001b[0m  1.2591\n",
            "     56        \u001b[36m0.0156\u001b[0m        0.0129  1.2509\n",
            "     57        \u001b[36m0.0149\u001b[0m        0.0156  1.2651\n",
            "     58        0.0153        \u001b[32m0.0088\u001b[0m  1.2560\n",
            "     59        \u001b[36m0.0147\u001b[0m        0.0105  1.2688\n",
            "     60        0.0147        0.0110  1.6780\n",
            "     61        0.0148        0.0099  1.6586\n",
            "     62        0.0149        0.0097  1.3757\n",
            "     63        \u001b[36m0.0140\u001b[0m        0.0118  1.2726\n",
            "     64        0.0147        0.0095  1.2672\n",
            "     65        0.0141        0.0098  1.2753\n",
            "     66        \u001b[36m0.0138\u001b[0m        0.0121  1.2610\n",
            "     67        0.0140        0.0100  1.2669\n",
            "     68        0.0139        0.0191  1.2622\n",
            "     69        0.0139        0.0102  1.2615\n",
            "     70        \u001b[36m0.0125\u001b[0m        0.0109  1.5947\n",
            "     71        0.0128        0.0101  1.6685\n",
            "     72        \u001b[36m0.0124\u001b[0m        0.0094  1.4466\n",
            "     73        0.0130        0.0119  1.2570\n",
            "     74        0.0134        0.0111  1.2426\n",
            "     75        \u001b[36m0.0124\u001b[0m        0.0094  1.2712\n",
            "     76        0.0125        0.0094  1.2626\n",
            "     77        0.0129        0.0088  1.2609\n",
            "     78        \u001b[36m0.0117\u001b[0m        0.0102  1.2570\n",
            "     79        0.0130        0.0088  1.2594\n",
            "     80        0.0129        \u001b[32m0.0086\u001b[0m  1.5009\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0973\u001b[0m        \u001b[32m0.2763\u001b[0m  1.6448\n",
            "      2        \u001b[36m0.0741\u001b[0m        0.2834  1.4093\n",
            "      3        \u001b[36m0.0675\u001b[0m        0.3010  1.2681\n",
            "      4        \u001b[36m0.0610\u001b[0m        0.3059  1.2520\n",
            "      5        0.0822        \u001b[32m0.2667\u001b[0m  1.2568\n",
            "      6        \u001b[36m0.0610\u001b[0m        \u001b[32m0.2475\u001b[0m  1.2638\n",
            "      7        0.0638        \u001b[32m0.2427\u001b[0m  1.2545\n",
            "      8        0.0617        \u001b[32m0.2153\u001b[0m  1.2584\n",
            "      9        0.0718        0.2385  1.2584\n",
            "     10        0.0679        0.2897  1.5719\n",
            "     11        0.0726        0.2213  1.6275\n",
            "     12        0.0620        \u001b[32m0.2140\u001b[0m  1.4495\n",
            "     13        0.0631        \u001b[32m0.2106\u001b[0m  1.2802\n",
            "     14        \u001b[36m0.0593\u001b[0m        0.2173  1.2612\n",
            "     15        0.0734        \u001b[32m0.2006\u001b[0m  1.2646\n",
            "     16        0.0634        0.2482  1.2743\n",
            "     17        0.0602        0.2243  1.2645\n",
            "     18        0.0597        0.2255  1.2678\n",
            "     19        0.0651        0.2231  1.2659\n",
            "     20        0.0692        \u001b[32m0.1924\u001b[0m  1.5656\n",
            "     21        0.0683        0.2111  1.6389\n",
            "     22        0.0670        \u001b[32m0.1811\u001b[0m  1.4770\n",
            "     23        0.0653        0.2341  1.2699\n",
            "     24        0.0646        \u001b[32m0.1654\u001b[0m  1.2868\n",
            "     25        0.0670        0.2446  1.2833\n",
            "     26        0.0699        0.2000  1.2759\n",
            "     27        0.0716        0.2192  1.2681\n",
            "     28        0.0746        0.2038  1.2723\n",
            "     29        0.0707        0.2066  1.2715\n",
            "     30        0.0706        0.2062  1.5294\n",
            "     31        0.0699        0.1994  1.6556\n",
            "     32        0.0680        0.2675  1.5282\n",
            "     33        0.0658        0.2052  1.2707\n",
            "     34        0.0681        0.2062  1.2696\n",
            "     35        0.0693        0.3089  1.2916\n",
            "     36        0.0694        0.2798  1.2585\n",
            "     37        0.0666        0.2622  1.2585\n",
            "     38        0.0638        0.2655  1.2615\n",
            "     39        0.0644        0.2785  1.2665\n",
            "     40        0.0667        0.2811  1.4479\n",
            "     41        0.0669        0.2744  1.6514\n",
            "     42        0.0651        0.2448  1.6009\n",
            "     43        0.0670        0.2459  1.2671\n",
            "     44        0.0625        0.2787  1.2664\n",
            "     45        0.0649        0.2493  1.2377\n",
            "     46        0.0663        0.2343  1.2926\n",
            "     47        0.0698        0.2984  1.2630\n",
            "     48        0.0765        0.2802  1.2530\n",
            "     49        0.0699        0.2832  1.2630\n",
            "     50        0.0690        0.2532  1.3804\n",
            "     51        0.0674        0.2608  1.6423\n",
            "     52        0.0644        0.2404  1.6465\n",
            "     53        0.0635        0.2512  1.2692\n",
            "     54        0.0636        0.2451  1.2642\n",
            "     55        0.0617        0.2199  1.2654\n",
            "     56        0.0596        0.2074  1.2694\n",
            "     57        0.0595        0.2450  1.3077\n",
            "     58        \u001b[36m0.0576\u001b[0m        0.2123  1.2604\n",
            "     59        0.0943        0.2318  1.2636\n",
            "     60        0.0817        0.2106  1.4184\n",
            "     61        0.0583        0.2244  1.6023\n",
            "     62        \u001b[36m0.0575\u001b[0m        0.1884  1.6993\n",
            "     63        \u001b[36m0.0572\u001b[0m        0.1767  1.2484\n",
            "     64        \u001b[36m0.0561\u001b[0m        0.2398  1.2511\n",
            "     65        0.0586        0.1935  1.2579\n",
            "     66        0.0627        0.1858  1.2548\n",
            "     67        \u001b[36m0.0541\u001b[0m        0.1898  1.2757\n",
            "     68        0.0550        0.1797  1.2651\n",
            "     69        0.0576        \u001b[32m0.1501\u001b[0m  1.2668\n",
            "     70        0.0553        0.2128  1.3237\n",
            "     71        \u001b[36m0.0539\u001b[0m        0.1733  1.6197\n",
            "     72        \u001b[36m0.0535\u001b[0m        0.1510  1.6788\n",
            "     73        0.0544        \u001b[32m0.1411\u001b[0m  1.3347\n",
            "     74        0.0543        0.1806  1.2716\n",
            "     75        \u001b[36m0.0531\u001b[0m        0.1758  1.2913\n",
            "     76        \u001b[36m0.0514\u001b[0m        0.1681  1.2695\n",
            "     77        0.0537        0.1456  1.2523\n",
            "     78        0.0528        0.1766  1.2577\n",
            "     79        0.0536        0.1600  1.2848\n",
            "     80        0.0578        \u001b[32m0.1130\u001b[0m  1.3076\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1728\u001b[0m        \u001b[32m0.1392\u001b[0m  0.8575\n",
            "      2        \u001b[36m0.1147\u001b[0m        \u001b[32m0.1311\u001b[0m  0.8396\n",
            "      3        \u001b[36m0.1082\u001b[0m        \u001b[32m0.1233\u001b[0m  0.8930\n",
            "      4        \u001b[36m0.1013\u001b[0m        \u001b[32m0.1105\u001b[0m  0.8767\n",
            "      5        \u001b[36m0.0938\u001b[0m        \u001b[32m0.0971\u001b[0m  0.6444\n",
            "      6        \u001b[36m0.0878\u001b[0m        \u001b[32m0.0858\u001b[0m  0.6488\n",
            "      7        \u001b[36m0.0819\u001b[0m        0.0929  0.6431\n",
            "      8        \u001b[36m0.0786\u001b[0m        \u001b[32m0.0659\u001b[0m  0.6407\n",
            "      9        \u001b[36m0.0725\u001b[0m        0.0670  0.6436\n",
            "     10        \u001b[36m0.0682\u001b[0m        \u001b[32m0.0613\u001b[0m  0.6466\n",
            "     11        \u001b[36m0.0618\u001b[0m        0.0662  0.6399\n",
            "     12        \u001b[36m0.0585\u001b[0m        0.0705  0.6471\n",
            "     13        \u001b[36m0.0563\u001b[0m        \u001b[32m0.0588\u001b[0m  0.6378\n",
            "     14        \u001b[36m0.0503\u001b[0m        \u001b[32m0.0555\u001b[0m  0.6362\n",
            "     15        \u001b[36m0.0485\u001b[0m        0.0651  0.6387\n",
            "     16        0.0486        0.0676  0.6281\n",
            "     17        0.0497        0.0637  0.6235\n",
            "     18        \u001b[36m0.0477\u001b[0m        0.0645  0.6293\n",
            "     19        \u001b[36m0.0473\u001b[0m        0.0665  0.6729\n",
            "     20        \u001b[36m0.0464\u001b[0m        0.0634  0.7491\n",
            "     21        \u001b[36m0.0446\u001b[0m        0.0571  0.8700\n",
            "     22        0.0511        0.1104  0.8301\n",
            "     23        0.0564        0.0839  0.8774\n",
            "     24        0.0528        0.0721  0.8774\n",
            "     25        0.0499        0.0657  0.6404\n",
            "     26        0.0466        0.0657  0.6396\n",
            "     27        0.0458        0.0642  0.6516\n",
            "     28        0.0454        0.0687  0.6265\n",
            "     29        \u001b[36m0.0446\u001b[0m        0.0644  0.6444\n",
            "     30        \u001b[36m0.0442\u001b[0m        0.0660  0.6382\n",
            "     31        \u001b[36m0.0435\u001b[0m        0.0607  0.6361\n",
            "     32        \u001b[36m0.0431\u001b[0m        0.0576  0.6566\n",
            "     33        0.0434        0.0648  0.6404\n",
            "     34        \u001b[36m0.0418\u001b[0m        0.0705  0.6343\n",
            "     35        0.0430        0.0670  0.6588\n",
            "     36        \u001b[36m0.0410\u001b[0m        0.0606  0.6440\n",
            "     37        \u001b[36m0.0403\u001b[0m        0.0601  0.7982\n",
            "     38        \u001b[36m0.0391\u001b[0m        \u001b[32m0.0516\u001b[0m  0.8614\n",
            "     39        0.0401        0.0580  0.9088\n",
            "     40        \u001b[36m0.0386\u001b[0m        0.0561  1.0341\n",
            "     41        0.0388        0.0595  0.9868\n",
            "     42        \u001b[36m0.0382\u001b[0m        0.0570  0.9526\n",
            "     43        0.0411        \u001b[32m0.0455\u001b[0m  0.8709\n",
            "     44        \u001b[36m0.0372\u001b[0m        0.1401  0.8769\n",
            "     45        0.0461        0.0698  0.7423\n",
            "     46        \u001b[36m0.0356\u001b[0m        0.0553  0.6481\n",
            "     47        \u001b[36m0.0289\u001b[0m        0.0773  0.6403\n",
            "     48        \u001b[36m0.0257\u001b[0m        \u001b[32m0.0449\u001b[0m  0.6466\n",
            "     49        \u001b[36m0.0219\u001b[0m        \u001b[32m0.0431\u001b[0m  0.6475\n",
            "     50        0.0225        \u001b[32m0.0353\u001b[0m  0.6361\n",
            "     51        \u001b[36m0.0206\u001b[0m        \u001b[32m0.0334\u001b[0m  0.6381\n",
            "     52        \u001b[36m0.0190\u001b[0m        \u001b[32m0.0285\u001b[0m  0.6508\n",
            "     53        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0215\u001b[0m  0.6304\n",
            "     54        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0169\u001b[0m  0.6415\n",
            "     55        \u001b[36m0.0146\u001b[0m        \u001b[32m0.0158\u001b[0m  0.6419\n",
            "     56        \u001b[36m0.0114\u001b[0m        0.0184  0.6403\n",
            "     57        \u001b[36m0.0101\u001b[0m        0.0271  0.6393\n",
            "     58        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0135\u001b[0m  0.6403\n",
            "     59        \u001b[36m0.0085\u001b[0m        \u001b[32m0.0101\u001b[0m  0.6434\n",
            "     60        0.0094        0.0249  0.6565\n",
            "     61        0.0086        0.0135  0.8479\n",
            "     62        \u001b[36m0.0070\u001b[0m        0.0137  0.8563\n",
            "     63        \u001b[36m0.0065\u001b[0m        \u001b[32m0.0073\u001b[0m  0.8513\n",
            "     64        \u001b[36m0.0061\u001b[0m        \u001b[32m0.0063\u001b[0m  0.8806\n",
            "     65        \u001b[36m0.0056\u001b[0m        0.0067  0.7557\n",
            "     66        \u001b[36m0.0051\u001b[0m        0.0071  0.6455\n",
            "     67        \u001b[36m0.0046\u001b[0m        0.0063  0.6420\n",
            "     68        \u001b[36m0.0041\u001b[0m        \u001b[32m0.0051\u001b[0m  0.6185\n",
            "     69        \u001b[36m0.0038\u001b[0m        \u001b[32m0.0039\u001b[0m  0.6429\n",
            "     70        \u001b[36m0.0035\u001b[0m        \u001b[32m0.0038\u001b[0m  0.6413\n",
            "     71        \u001b[36m0.0032\u001b[0m        \u001b[32m0.0033\u001b[0m  0.6484\n",
            "     72        \u001b[36m0.0031\u001b[0m        \u001b[32m0.0031\u001b[0m  0.6520\n",
            "     73        \u001b[36m0.0030\u001b[0m        \u001b[32m0.0028\u001b[0m  0.6487\n",
            "     74        \u001b[36m0.0028\u001b[0m        \u001b[32m0.0028\u001b[0m  0.6395\n",
            "     75        \u001b[36m0.0028\u001b[0m        \u001b[32m0.0026\u001b[0m  0.6635\n",
            "     76        \u001b[36m0.0027\u001b[0m        \u001b[32m0.0026\u001b[0m  0.6578\n",
            "     77        0.0027        0.0031  0.6317\n",
            "     78        0.0027        0.0039  0.6497\n",
            "     79        0.0033        0.0086  0.6262\n",
            "     80        0.0032        0.0057  0.6829\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  57.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1583\u001b[0m        \u001b[32m0.1338\u001b[0m  0.8427\n",
            "      2        0.1586        0.1506  0.8584\n",
            "      3        0.1929        \u001b[32m0.1276\u001b[0m  0.8634\n",
            "      4        0.1756        0.1337  0.9013\n",
            "      5        0.1921        0.1287  0.6982\n",
            "      6        0.1904        0.1591  0.6416\n",
            "      7        0.1953        0.1390  0.6418\n",
            "      8        0.1982        0.1346  0.6465\n",
            "      9        0.1931        \u001b[32m0.1250\u001b[0m  0.6613\n",
            "     10        0.1976        \u001b[32m0.1221\u001b[0m  0.6422\n",
            "     11        0.1936        \u001b[32m0.1196\u001b[0m  0.6389\n",
            "     12        0.1812        \u001b[32m0.1104\u001b[0m  0.6433\n",
            "     13        0.1963        0.1120  0.6373\n",
            "     14        0.1961        0.1133  0.6398\n",
            "     15        0.1951        \u001b[32m0.1072\u001b[0m  0.6462\n",
            "     16        0.2068        0.1121  0.6334\n",
            "     17        0.1880        \u001b[32m0.1000\u001b[0m  0.6462\n",
            "     18        0.1910        0.1177  0.6393\n",
            "     19        0.1913        \u001b[32m0.0980\u001b[0m  0.6403\n",
            "     20        0.1908        \u001b[32m0.0956\u001b[0m  0.7164\n",
            "     21        0.1889        0.0990  0.8639\n",
            "     22        0.1888        \u001b[32m0.0952\u001b[0m  0.8609\n",
            "     23        0.1887        0.1016  0.8798\n",
            "     24        0.1891        \u001b[32m0.0946\u001b[0m  0.8833\n",
            "     25        0.1821        0.1068  0.6915\n",
            "     26        0.1863        0.0963  0.6356\n",
            "     27        0.1870        0.1198  0.6443\n",
            "     28        0.1888        \u001b[32m0.0943\u001b[0m  0.6437\n",
            "     29        0.1866        0.1104  0.6400\n",
            "     30        0.1879        \u001b[32m0.0938\u001b[0m  0.6541\n",
            "     31        0.1946        0.1092  0.6380\n",
            "     32        0.1837        \u001b[32m0.0920\u001b[0m  0.6501\n",
            "     33        0.1839        0.1016  0.6552\n",
            "     34        0.1844        \u001b[32m0.0914\u001b[0m  0.6418\n",
            "     35        0.1834        0.1010  0.6475\n",
            "     36        0.1840        0.0915  0.6290\n",
            "     37        0.1833        0.1009  0.6301\n",
            "     38        0.1839        0.0915  0.6416\n",
            "     39        0.1831        0.0997  0.6444\n",
            "     40        0.1837        0.0916  0.7091\n",
            "     41        0.1943        0.1016  0.8463\n",
            "     42        0.1790        \u001b[32m0.0913\u001b[0m  0.8453\n",
            "     43        0.1803        0.0969  0.8518\n",
            "     44        0.1810        0.0937  0.8935\n",
            "     45        0.1805        0.0943  0.6532\n",
            "     46        0.1805        0.1053  0.6509\n",
            "     47        0.1816        0.0921  0.6612\n",
            "     48        0.1804        0.0988  0.6415\n",
            "     49        0.1809        0.0922  0.6467\n",
            "     50        0.1805        0.0920  0.6377\n",
            "     51        0.1822        0.1022  0.6356\n",
            "     52        0.1773        0.0960  0.6465\n",
            "     53        0.1778        0.0973  0.6413\n",
            "     54        0.1781        0.0922  0.6343\n",
            "     55        0.1773        0.0933  0.6463\n",
            "     56        0.1774        0.1053  0.6415\n",
            "     57        0.1784        0.0937  0.6483\n",
            "     58        0.1776        0.0980  0.6427\n",
            "     59        0.1780        \u001b[32m0.0907\u001b[0m  0.6480\n",
            "     60        0.1775        0.0922  0.7552\n",
            "     61        0.1774        0.0945  0.8588\n",
            "     62        0.1774        0.0929  0.8378\n",
            "     63        0.1775        0.0981  0.9123\n",
            "     64        0.1780        0.0911  0.8355\n",
            "     65        0.1773        0.0917  0.6561\n",
            "     66        0.1773        0.0958  0.6516\n",
            "     67        0.1815        0.0951  0.6331\n",
            "     68        0.1858        0.0953  0.6273\n",
            "     69        0.1830        0.0974  0.6342\n",
            "     70        0.1820        0.0920  0.6448\n",
            "     71        0.1814        0.0942  0.6353\n",
            "     72        0.1813        0.0969  0.6354\n",
            "     73        0.1812        0.0935  0.6446\n",
            "     74        0.1714        0.0973  0.6453\n",
            "     75        0.1871        0.1096  0.6554\n",
            "     76        0.1870        0.0961  0.6431\n",
            "     77        0.1850        0.0963  0.6351\n",
            "     78        0.2046        0.0930  0.6437\n",
            "     79        0.1799        0.1203  0.6322\n",
            "     80        0.1838        \u001b[32m0.0897\u001b[0m  0.7888\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  56.0s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1023\u001b[0m        \u001b[32m0.3231\u001b[0m  0.9312\n",
            "      2        0.1441        \u001b[32m0.2249\u001b[0m  0.9355\n",
            "      3        0.1629        \u001b[32m0.1381\u001b[0m  0.9404\n",
            "      4        0.1543        \u001b[32m0.1213\u001b[0m  0.7523\n",
            "      5        0.1645        \u001b[32m0.1048\u001b[0m  0.6805\n",
            "      6        0.1341        0.1102  0.6640\n",
            "      7        0.1180        \u001b[32m0.0963\u001b[0m  0.6883\n",
            "      8        0.1027        0.0995  0.7204\n",
            "      9        \u001b[36m0.0935\u001b[0m        0.1012  0.6857\n",
            "     10        \u001b[36m0.0802\u001b[0m        0.1422  0.6871\n",
            "     11        \u001b[36m0.0744\u001b[0m        0.1450  0.6903\n",
            "     12        \u001b[36m0.0679\u001b[0m        0.1352  0.6631\n",
            "     13        \u001b[36m0.0612\u001b[0m        0.1019  0.6813\n",
            "     14        \u001b[36m0.0610\u001b[0m        0.0987  0.6855\n",
            "     15        \u001b[36m0.0563\u001b[0m        0.0979  0.6699\n",
            "     16        \u001b[36m0.0552\u001b[0m        \u001b[32m0.0734\u001b[0m  0.6802\n",
            "     17        \u001b[36m0.0519\u001b[0m        \u001b[32m0.0626\u001b[0m  0.6872\n",
            "     18        \u001b[36m0.0484\u001b[0m        \u001b[32m0.0596\u001b[0m  0.7345\n",
            "     19        \u001b[36m0.0455\u001b[0m        \u001b[32m0.0470\u001b[0m  0.8983\n",
            "     20        \u001b[36m0.0419\u001b[0m        \u001b[32m0.0404\u001b[0m  0.9150\n",
            "     21        \u001b[36m0.0384\u001b[0m        0.0470  0.9313\n",
            "     22        \u001b[36m0.0367\u001b[0m        \u001b[32m0.0389\u001b[0m  0.8920\n",
            "     23        \u001b[36m0.0344\u001b[0m        \u001b[32m0.0373\u001b[0m  0.6852\n",
            "     24        \u001b[36m0.0334\u001b[0m        0.0381  0.6725\n",
            "     25        \u001b[36m0.0318\u001b[0m        \u001b[32m0.0369\u001b[0m  0.6836\n",
            "     26        \u001b[36m0.0299\u001b[0m        0.0510  0.7115\n",
            "     27        \u001b[36m0.0289\u001b[0m        0.0403  0.6983\n",
            "     28        \u001b[36m0.0274\u001b[0m        0.0383  0.6848\n",
            "     29        \u001b[36m0.0265\u001b[0m        0.0477  0.7014\n",
            "     30        0.0266        \u001b[32m0.0320\u001b[0m  0.6725\n",
            "     31        0.0267        0.0344  0.6754\n",
            "     32        \u001b[36m0.0255\u001b[0m        0.0581  0.6850\n",
            "     33        0.0258        0.0348  0.6741\n",
            "     34        \u001b[36m0.0252\u001b[0m        0.0324  0.6841\n",
            "     35        \u001b[36m0.0242\u001b[0m        0.0389  0.6829\n",
            "     36        0.0248        0.0370  0.6966\n",
            "     37        0.0245        0.0327  0.8676\n",
            "     38        0.0242        0.0364  0.9316\n",
            "     39        0.0242        0.0397  0.9251\n",
            "     40        \u001b[36m0.0231\u001b[0m        0.0572  0.9978\n",
            "     41        0.0261        0.0374  0.8197\n",
            "     42        0.0264        0.0435  0.6787\n",
            "     43        0.0239        0.0425  0.6958\n",
            "     44        0.0235        0.0448  0.6864\n",
            "     45        0.0239        0.0436  0.6694\n",
            "     46        0.0232        0.0556  0.6911\n",
            "     47        0.0260        \u001b[32m0.0294\u001b[0m  0.6867\n",
            "     48        0.0242        0.0298  0.6784\n",
            "     49        0.0235        0.0335  0.7174\n",
            "     50        0.0236        0.0360  0.6882\n",
            "     51        0.0241        0.0316  0.6879\n",
            "     52        0.0236        0.0374  0.7057\n",
            "     53        \u001b[36m0.0226\u001b[0m        0.0350  0.7001\n",
            "     54        \u001b[36m0.0220\u001b[0m        0.0367  0.6752\n",
            "     55        0.0221        0.0405  0.7684\n",
            "     56        0.0223        0.0381  0.9170\n",
            "     57        \u001b[36m0.0215\u001b[0m        \u001b[32m0.0273\u001b[0m  0.9155\n",
            "     58        0.0224        \u001b[32m0.0263\u001b[0m  0.9316\n",
            "     59        0.0219        \u001b[32m0.0252\u001b[0m  0.9565\n",
            "     60        \u001b[36m0.0202\u001b[0m        0.0252  0.6804\n",
            "     61        \u001b[36m0.0189\u001b[0m        0.0252  0.6753\n",
            "     62        \u001b[36m0.0184\u001b[0m        0.0289  0.6913\n",
            "     63        \u001b[36m0.0179\u001b[0m        0.0277  0.6818\n",
            "     64        \u001b[36m0.0172\u001b[0m        0.0317  0.6851\n",
            "     65        \u001b[36m0.0168\u001b[0m        0.0308  0.6965\n",
            "     66        \u001b[36m0.0162\u001b[0m        0.0263  0.6815\n",
            "     67        \u001b[36m0.0159\u001b[0m        0.0258  0.7083\n",
            "     68        0.0169        \u001b[32m0.0200\u001b[0m  0.6820\n",
            "     69        \u001b[36m0.0157\u001b[0m        0.0290  0.6895\n",
            "     70        \u001b[36m0.0151\u001b[0m        0.0223  0.6871\n",
            "     71        \u001b[36m0.0149\u001b[0m        0.0258  0.6873\n",
            "     72        0.0155        0.0281  0.6802\n",
            "     73        \u001b[36m0.0138\u001b[0m        0.0299  0.7000\n",
            "     74        0.0149        0.0238  0.8279\n",
            "     75        \u001b[36m0.0134\u001b[0m        0.0348  0.9346\n",
            "     76        0.0141        0.0231  0.9082\n",
            "     77        \u001b[36m0.0132\u001b[0m        0.0295  0.9638\n",
            "     78        0.0137        0.0203  0.8083\n",
            "     79        \u001b[36m0.0123\u001b[0m        0.0305  0.6851\n",
            "     80        0.0128        0.0249  0.6795\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.0min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1026\u001b[0m        \u001b[32m0.2224\u001b[0m  0.6908\n",
            "      2        0.1384        \u001b[32m0.1908\u001b[0m  0.6772\n",
            "      3        0.1424        0.2497  0.6964\n",
            "      4        0.1813        0.2115  0.6818\n",
            "      5        0.2115        \u001b[32m0.1450\u001b[0m  0.6802\n",
            "      6        0.1936        \u001b[32m0.1306\u001b[0m  0.6883\n",
            "      7        0.1774        \u001b[32m0.1291\u001b[0m  0.6841\n",
            "      8        0.1826        \u001b[32m0.1122\u001b[0m  0.6617\n",
            "      9        0.1479        \u001b[32m0.1036\u001b[0m  0.7081\n",
            "     10        0.1191        \u001b[32m0.0975\u001b[0m  0.6799\n",
            "     11        \u001b[36m0.0950\u001b[0m        \u001b[32m0.0884\u001b[0m  0.6707\n",
            "     12        \u001b[36m0.0818\u001b[0m        \u001b[32m0.0852\u001b[0m  0.7742\n",
            "     13        \u001b[36m0.0733\u001b[0m        \u001b[32m0.0804\u001b[0m  0.9205\n",
            "     14        \u001b[36m0.0681\u001b[0m        0.0881  0.9001\n",
            "     15        0.0687        0.0816  0.9478\n",
            "     16        \u001b[36m0.0632\u001b[0m        \u001b[32m0.0798\u001b[0m  0.9269\n",
            "     17        \u001b[36m0.0607\u001b[0m        \u001b[32m0.0796\u001b[0m  0.6843\n",
            "     18        \u001b[36m0.0606\u001b[0m        0.1032  0.6829\n",
            "     19        \u001b[36m0.0592\u001b[0m        \u001b[32m0.0750\u001b[0m  0.6783\n",
            "     20        \u001b[36m0.0570\u001b[0m        \u001b[32m0.0709\u001b[0m  0.6819\n",
            "     21        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0617\u001b[0m  0.6740\n",
            "     22        0.0535        0.0640  0.6728\n",
            "     23        \u001b[36m0.0507\u001b[0m        0.0624  0.6788\n",
            "     24        \u001b[36m0.0501\u001b[0m        \u001b[32m0.0602\u001b[0m  0.6769\n",
            "     25        0.0502        0.0851  0.6791\n",
            "     26        \u001b[36m0.0494\u001b[0m        0.0666  0.6858\n",
            "     27        \u001b[36m0.0442\u001b[0m        0.0887  0.6816\n",
            "     28        0.0447        \u001b[32m0.0545\u001b[0m  0.6943\n",
            "     29        \u001b[36m0.0415\u001b[0m        0.0608  0.6931\n",
            "     30        \u001b[36m0.0391\u001b[0m        0.0922  0.6782\n",
            "     31        0.0428        0.0898  0.7973\n",
            "     32        0.0409        \u001b[32m0.0538\u001b[0m  0.9066\n",
            "     33        \u001b[36m0.0372\u001b[0m        0.0633  0.8984\n",
            "     34        \u001b[36m0.0350\u001b[0m        0.2072  0.9530\n",
            "     35        0.0355        0.0739  0.8601\n",
            "     36        0.0389        0.0715  0.6996\n",
            "     37        0.0363        \u001b[32m0.0460\u001b[0m  0.6695\n",
            "     38        0.0368        0.0481  0.6912\n",
            "     39        \u001b[36m0.0341\u001b[0m        0.0565  0.6891\n",
            "     40        0.0358        0.0720  0.6888\n",
            "     41        0.0350        0.0569  0.6975\n",
            "     42        \u001b[36m0.0338\u001b[0m        0.0676  0.6994\n",
            "     43        0.0358        0.0599  0.6838\n",
            "     44        0.0343        0.0547  0.7007\n",
            "     45        \u001b[36m0.0335\u001b[0m        0.0845  0.7033\n",
            "     46        0.0391        0.0734  0.6960\n",
            "     47        0.0352        0.0717  0.6939\n",
            "     48        0.0336        0.0689  0.7001\n",
            "     49        0.0343        0.0657  0.6929\n",
            "     50        0.0336        0.0614  0.9273\n",
            "     51        \u001b[36m0.0326\u001b[0m        0.1370  0.9370\n",
            "     52        0.0328        0.0803  0.9488\n",
            "     53        0.0344        \u001b[32m0.0396\u001b[0m  1.0380\n",
            "     54        0.0335        0.0591  0.7812\n",
            "     55        0.0340        0.0602  0.7110\n",
            "     56        0.0337        0.0629  0.6763\n",
            "     57        \u001b[36m0.0323\u001b[0m        0.0699  0.6880\n",
            "     58        0.0324        0.0638  0.6982\n",
            "     59        \u001b[36m0.0318\u001b[0m        0.1132  0.6782\n",
            "     60        0.0322        0.0495  0.6964\n",
            "     61        0.0343        0.0500  0.7056\n",
            "     62        0.0323        0.0807  0.6745\n",
            "     63        0.0337        0.0416  0.6842\n",
            "     64        0.0338        \u001b[32m0.0379\u001b[0m  0.6957\n",
            "     65        0.0341        0.0471  0.6849\n",
            "     66        0.0332        0.0680  0.6930\n",
            "     67        0.0335        0.0950  0.6869\n",
            "     68        0.0339        0.0998  0.8527\n",
            "     69        0.0337        0.0871  0.9285\n",
            "     70        0.0337        0.0833  0.9177\n",
            "     71        0.0324        0.0670  0.9696\n",
            "     72        \u001b[36m0.0308\u001b[0m        0.1263  0.8828\n",
            "     73        0.0339        0.1268  0.6904\n",
            "     74        \u001b[36m0.0305\u001b[0m        0.0744  0.6893\n",
            "     75        0.0331        0.0627  0.6902\n",
            "     76        0.0331        0.0496  0.6767\n",
            "     77        0.0328        0.0526  0.6741\n",
            "     78        0.0324        0.0762  0.6849\n",
            "     79        0.0324        0.0901  0.6952\n",
            "     80        0.0326        0.0704  0.6925\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  59.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0893\u001b[0m        \u001b[32m0.1742\u001b[0m  0.6591\n",
            "      2        0.1122        \u001b[32m0.1312\u001b[0m  0.6717\n",
            "      3        0.1093        0.1828  0.6486\n",
            "      4        0.1078        0.1333  0.6576\n",
            "      5        0.0950        \u001b[32m0.1026\u001b[0m  0.6582\n",
            "      6        \u001b[36m0.0821\u001b[0m        \u001b[32m0.0915\u001b[0m  0.6421\n",
            "      7        \u001b[36m0.0775\u001b[0m        \u001b[32m0.0814\u001b[0m  0.8436\n",
            "      8        \u001b[36m0.0740\u001b[0m        \u001b[32m0.0808\u001b[0m  0.8700\n",
            "      9        \u001b[36m0.0709\u001b[0m        \u001b[32m0.0747\u001b[0m  0.8780\n",
            "     10        \u001b[36m0.0647\u001b[0m        \u001b[32m0.0685\u001b[0m  0.8984\n",
            "     11        \u001b[36m0.0638\u001b[0m        \u001b[32m0.0620\u001b[0m  0.7913\n",
            "     12        \u001b[36m0.0581\u001b[0m        \u001b[32m0.0594\u001b[0m  0.6746\n",
            "     13        \u001b[36m0.0532\u001b[0m        0.0633  0.6497\n",
            "     14        \u001b[36m0.0509\u001b[0m        0.0647  0.6729\n",
            "     15        \u001b[36m0.0485\u001b[0m        0.0690  0.6427\n",
            "     16        \u001b[36m0.0459\u001b[0m        0.0667  0.6394\n",
            "     17        0.0480        0.1437  0.6344\n",
            "     18        0.0461        0.1021  0.6517\n",
            "     19        0.0495        0.1241  0.6504\n",
            "     20        \u001b[36m0.0432\u001b[0m        0.1065  0.6465\n",
            "     21        \u001b[36m0.0398\u001b[0m        0.0832  0.6542\n",
            "     22        \u001b[36m0.0318\u001b[0m        0.0816  0.6593\n",
            "     23        \u001b[36m0.0222\u001b[0m        0.1137  0.6400\n",
            "     24        0.0224        0.0898  0.6504\n",
            "     25        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0323\u001b[0m  0.6445\n",
            "     26        \u001b[36m0.0137\u001b[0m        \u001b[32m0.0207\u001b[0m  0.6582\n",
            "     27        \u001b[36m0.0123\u001b[0m        0.0321  0.8335\n",
            "     28        \u001b[36m0.0114\u001b[0m        0.0227  0.8679\n",
            "     29        \u001b[36m0.0096\u001b[0m        0.0245  0.8603\n",
            "     30        \u001b[36m0.0093\u001b[0m        0.0208  0.9149\n",
            "     31        \u001b[36m0.0090\u001b[0m        0.0227  0.6637\n",
            "     32        \u001b[36m0.0090\u001b[0m        \u001b[32m0.0194\u001b[0m  0.6430\n",
            "     33        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0180\u001b[0m  0.6383\n",
            "     34        \u001b[36m0.0083\u001b[0m        0.0185  0.6417\n",
            "     35        \u001b[36m0.0080\u001b[0m        \u001b[32m0.0142\u001b[0m  0.7772\n",
            "     36        0.0080        0.0145  0.8876\n",
            "     37        \u001b[36m0.0064\u001b[0m        \u001b[32m0.0105\u001b[0m  0.8766\n",
            "     38        \u001b[36m0.0062\u001b[0m        \u001b[32m0.0089\u001b[0m  0.9093\n",
            "     39        \u001b[36m0.0060\u001b[0m        0.0090  0.8618\n",
            "     40        0.0063        0.0149  0.6454\n",
            "     41        \u001b[36m0.0056\u001b[0m        0.0107  0.6592\n",
            "     42        0.0067        0.0104  0.6662\n",
            "     43        0.0061        0.0128  0.6614\n",
            "     44        0.0064        0.0162  0.7105\n",
            "     45        0.0057        0.0102  0.8589\n",
            "     46        \u001b[36m0.0054\u001b[0m        0.0098  0.8631\n",
            "     47        \u001b[36m0.0051\u001b[0m        \u001b[32m0.0080\u001b[0m  0.8785\n",
            "     48        0.0057        0.0189  0.8950\n",
            "     49        0.0056        0.0093  0.6449\n",
            "     50        0.0058        \u001b[32m0.0052\u001b[0m  0.6762\n",
            "     51        \u001b[36m0.0034\u001b[0m        \u001b[32m0.0033\u001b[0m  0.6547\n",
            "     52        0.0043        0.0066  0.6381\n",
            "     53        0.0045        0.0052  0.6454\n",
            "     54        0.0042        \u001b[32m0.0031\u001b[0m  0.6615\n",
            "     55        0.0039        \u001b[32m0.0031\u001b[0m  0.6634\n",
            "     56        0.0037        0.0044  0.6782\n",
            "     57        0.0050        0.0095  0.6590\n",
            "     58        0.0077        0.0234  0.6570\n",
            "     59        0.0073        0.0151  0.6514\n",
            "     60        0.0063        0.0142  0.6519\n",
            "     61        0.0049        0.0061  0.6414\n",
            "     62        0.0042        0.0072  0.6615\n",
            "     63        0.0059        0.0087  0.6520\n",
            "     64        0.0058        0.0095  0.8656\n",
            "     65        0.0042        0.0063  0.8746\n",
            "     66        \u001b[36m0.0032\u001b[0m        0.0058  0.8447\n",
            "     67        0.0033        0.0086  0.9046\n",
            "     68        0.0041        0.0098  0.7545\n",
            "     69        0.0047        0.0151  0.6771\n",
            "     70        0.0047        0.0073  0.6566\n",
            "     71        0.0033        0.0065  0.6423\n",
            "     72        \u001b[36m0.0032\u001b[0m        0.0081  0.6429\n",
            "     73        0.0035        0.0089  0.6550\n",
            "     74        0.0046        0.0096  0.6466\n",
            "     75        0.0048        0.0080  0.6519\n",
            "     76        0.0037        0.0064  0.6429\n",
            "     77        0.0046        0.0043  0.6432\n",
            "     78        0.0032        0.0046  0.6388\n",
            "     79        0.0035        0.0073  0.6381\n",
            "     80        0.0037        0.0104  0.6567\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  57.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1153\u001b[0m        \u001b[32m0.1770\u001b[0m  0.6549\n",
            "      2        \u001b[36m0.1150\u001b[0m        0.1851  0.6408\n",
            "      3        \u001b[36m0.1009\u001b[0m        \u001b[32m0.1484\u001b[0m  0.7995\n",
            "      4        0.1036        \u001b[32m0.1466\u001b[0m  0.8727\n",
            "      5        \u001b[36m0.0936\u001b[0m        \u001b[32m0.1156\u001b[0m  0.8506\n",
            "      6        \u001b[36m0.0914\u001b[0m        0.1237  0.8939\n",
            "      7        \u001b[36m0.0802\u001b[0m        \u001b[32m0.1003\u001b[0m  0.7917\n",
            "      8        \u001b[36m0.0781\u001b[0m        0.1053  0.6520\n",
            "      9        \u001b[36m0.0700\u001b[0m        0.1027  0.6657\n",
            "     10        \u001b[36m0.0667\u001b[0m        \u001b[32m0.1000\u001b[0m  0.6319\n",
            "     11        \u001b[36m0.0636\u001b[0m        \u001b[32m0.0995\u001b[0m  0.6453\n",
            "     12        \u001b[36m0.0575\u001b[0m        0.1075  0.6490\n",
            "     13        0.0585        0.1016  0.6689\n",
            "     14        \u001b[36m0.0559\u001b[0m        \u001b[32m0.0840\u001b[0m  0.6493\n",
            "     15        \u001b[36m0.0542\u001b[0m        \u001b[32m0.0622\u001b[0m  0.6534\n",
            "     16        \u001b[36m0.0511\u001b[0m        0.0672  0.6332\n",
            "     17        \u001b[36m0.0507\u001b[0m        0.0841  0.6498\n",
            "     18        0.0509        0.0810  0.6414\n",
            "     19        \u001b[36m0.0484\u001b[0m        0.0939  0.6536\n",
            "     20        \u001b[36m0.0457\u001b[0m        0.0748  0.6549\n",
            "     21        0.0506        0.0736  0.6316\n",
            "     22        0.0474        0.0671  0.6788\n",
            "     23        0.0476        0.0641  0.8594\n",
            "     24        0.0480        0.0635  0.8577\n",
            "     25        0.0473        0.0793  0.8468\n",
            "     26        0.0514        0.0918  0.8893\n",
            "     27        0.0499        0.1215  0.7467\n",
            "     28        0.0510        0.1113  0.6382\n",
            "     29        0.0546        0.1150  0.6432\n",
            "     30        0.0526        0.1023  0.6459\n",
            "     31        0.0511        0.0755  0.6447\n",
            "     32        0.0488        0.0681  0.6629\n",
            "     33        0.0495        0.0638  0.6531\n",
            "     34        0.0491        0.0789  0.6357\n",
            "     35        0.0478        0.1035  0.6456\n",
            "     36        0.0499        0.0978  0.6347\n",
            "     37        0.0482        0.0859  0.6379\n",
            "     38        0.0474        0.0723  0.6470\n",
            "     39        0.0468        0.0687  0.6443\n",
            "     40        0.0476        \u001b[32m0.0583\u001b[0m  0.6394\n",
            "     41        0.0479        0.0635  0.6459\n",
            "     42        0.0487        0.0754  0.6784\n",
            "     43        0.0491        0.1081  0.8216\n",
            "     44        0.0497        0.0792  0.8745\n",
            "     45        0.0478        0.0858  0.8612\n",
            "     46        \u001b[36m0.0437\u001b[0m        0.0599  0.9207\n",
            "     47        \u001b[36m0.0381\u001b[0m        \u001b[32m0.0559\u001b[0m  0.6764\n",
            "     48        \u001b[36m0.0353\u001b[0m        0.0619  0.6597\n",
            "     49        0.0374        0.0722  0.6596\n",
            "     50        0.0367        0.1512  0.6412\n",
            "     51        0.0397        0.0795  0.6577\n",
            "     52        \u001b[36m0.0338\u001b[0m        \u001b[32m0.0398\u001b[0m  0.6573\n",
            "     53        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0289\u001b[0m  0.6360\n",
            "     54        \u001b[36m0.0114\u001b[0m        0.0316  0.6519\n",
            "     55        \u001b[36m0.0102\u001b[0m        0.0372  0.6564\n",
            "     56        \u001b[36m0.0100\u001b[0m        0.0326  0.6711\n",
            "     57        0.0100        \u001b[32m0.0268\u001b[0m  0.6526\n",
            "     58        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0264\u001b[0m  0.6586\n",
            "     59        \u001b[36m0.0082\u001b[0m        0.0278  0.6463\n",
            "     60        \u001b[36m0.0073\u001b[0m        \u001b[32m0.0227\u001b[0m  0.6630\n",
            "     61        \u001b[36m0.0070\u001b[0m        0.0311  0.6442\n",
            "     62        \u001b[36m0.0064\u001b[0m        0.0275  0.8213\n",
            "     63        \u001b[36m0.0059\u001b[0m        0.0280  0.8716\n",
            "     64        0.0077        0.0433  0.8341\n",
            "     65        0.0081        0.0463  0.9146\n",
            "     66        0.0092        0.0515  0.8152\n",
            "     67        0.0095        \u001b[32m0.0180\u001b[0m  0.6541\n",
            "     68        0.0076        \u001b[32m0.0116\u001b[0m  0.6410\n",
            "     69        0.0077        \u001b[32m0.0057\u001b[0m  0.6468\n",
            "     70        0.0092        0.1760  0.6560\n",
            "     71        0.0180        0.0829  0.6430\n",
            "     72        0.0127        0.0505  0.6462\n",
            "     73        0.0113        0.0651  0.6425\n",
            "     74        0.0123        0.0578  0.6537\n",
            "     75        0.0103        0.0462  0.6639\n",
            "     76        0.0089        0.0337  0.6592\n",
            "     77        0.0074        0.0249  0.6455\n",
            "     78        0.0062        0.0199  0.6542\n",
            "     79        \u001b[36m0.0056\u001b[0m        0.0150  0.6571\n",
            "     80        \u001b[36m0.0054\u001b[0m        0.0106  0.6371\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  56.3s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0905\u001b[0m        \u001b[32m0.2710\u001b[0m  0.8034\n",
            "      2        0.1027        \u001b[32m0.2666\u001b[0m  0.9218\n",
            "      3        0.1052        0.2736  0.9345\n",
            "      4        0.1131        \u001b[32m0.2431\u001b[0m  0.9729\n",
            "      5        0.1125        \u001b[32m0.2132\u001b[0m  0.8627\n",
            "      6        0.1244        \u001b[32m0.1859\u001b[0m  0.6979\n",
            "      7        0.1145        \u001b[32m0.1345\u001b[0m  0.6937\n",
            "      8        0.1127        \u001b[32m0.1146\u001b[0m  0.6927\n",
            "      9        0.1079        \u001b[32m0.0983\u001b[0m  0.7033\n",
            "     10        0.1092        \u001b[32m0.0969\u001b[0m  0.6671\n",
            "     11        0.1043        0.0973  0.6949\n",
            "     12        0.1044        \u001b[32m0.0890\u001b[0m  0.6940\n",
            "     13        0.1012        \u001b[32m0.0825\u001b[0m  0.6918\n",
            "     14        0.0944        \u001b[32m0.0816\u001b[0m  0.7077\n",
            "     15        \u001b[36m0.0882\u001b[0m        \u001b[32m0.0780\u001b[0m  0.7049\n",
            "     16        \u001b[36m0.0876\u001b[0m        \u001b[32m0.0751\u001b[0m  0.6721\n",
            "     17        \u001b[36m0.0799\u001b[0m        \u001b[32m0.0661\u001b[0m  0.7017\n",
            "     18        \u001b[36m0.0680\u001b[0m        0.0735  0.7096\n",
            "     19        \u001b[36m0.0566\u001b[0m        0.0920  0.6777\n",
            "     20        \u001b[36m0.0483\u001b[0m        0.0932  0.9208\n",
            "     21        \u001b[36m0.0408\u001b[0m        0.0899  0.9243\n",
            "     22        \u001b[36m0.0341\u001b[0m        0.0770  0.9578\n",
            "     23        0.0343        \u001b[32m0.0609\u001b[0m  0.9625\n",
            "     24        \u001b[36m0.0305\u001b[0m        \u001b[32m0.0569\u001b[0m  0.7395\n",
            "     25        0.0334        \u001b[32m0.0449\u001b[0m  0.7105\n",
            "     26        \u001b[36m0.0298\u001b[0m        0.0924  0.6961\n",
            "     27        0.0302        0.0649  0.7071\n",
            "     28        \u001b[36m0.0286\u001b[0m        \u001b[32m0.0376\u001b[0m  0.6884\n",
            "     29        \u001b[36m0.0272\u001b[0m        0.0445  0.7010\n",
            "     30        0.0273        0.0385  0.7003\n",
            "     31        \u001b[36m0.0259\u001b[0m        0.0568  0.6900\n",
            "     32        \u001b[36m0.0246\u001b[0m        \u001b[32m0.0363\u001b[0m  0.6981\n",
            "     33        \u001b[36m0.0238\u001b[0m        0.0400  0.6932\n",
            "     34        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0272\u001b[0m  0.6924\n",
            "     35        \u001b[36m0.0219\u001b[0m        0.0492  0.6881\n",
            "     36        \u001b[36m0.0213\u001b[0m        0.0284  0.6781\n",
            "     37        \u001b[36m0.0195\u001b[0m        \u001b[32m0.0197\u001b[0m  0.6943\n",
            "     38        0.0202        0.0236  0.8080\n",
            "     39        \u001b[36m0.0191\u001b[0m        0.0257  0.9110\n",
            "     40        \u001b[36m0.0190\u001b[0m        \u001b[32m0.0188\u001b[0m  0.8961\n",
            "     41        \u001b[36m0.0170\u001b[0m        0.0253  0.9549\n",
            "     42        0.0174        \u001b[32m0.0137\u001b[0m  0.8322\n",
            "     43        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0124\u001b[0m  0.6758\n",
            "     44        \u001b[36m0.0156\u001b[0m        0.0154  0.6831\n",
            "     45        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0102\u001b[0m  0.6819\n",
            "     46        \u001b[36m0.0153\u001b[0m        0.0104  0.6707\n",
            "     47        \u001b[36m0.0137\u001b[0m        0.0144  0.6784\n",
            "     48        \u001b[36m0.0136\u001b[0m        0.0166  0.6878\n",
            "     49        \u001b[36m0.0135\u001b[0m        0.0115  0.6873\n",
            "     50        \u001b[36m0.0132\u001b[0m        0.0110  0.6864\n",
            "     51        \u001b[36m0.0128\u001b[0m        0.0126  0.6852\n",
            "     52        0.0128        0.0121  0.6659\n",
            "     53        \u001b[36m0.0120\u001b[0m        0.0160  0.7062\n",
            "     54        \u001b[36m0.0118\u001b[0m        0.0191  0.6826\n",
            "     55        \u001b[36m0.0116\u001b[0m        0.0189  0.6829\n",
            "     56        \u001b[36m0.0109\u001b[0m        0.0150  0.6850\n",
            "     57        0.0109        0.0154  0.8917\n",
            "     58        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0098\u001b[0m  0.9306\n",
            "     59        \u001b[36m0.0099\u001b[0m        0.0148  0.9202\n",
            "     60        0.0100        0.0135  0.9614\n",
            "     61        \u001b[36m0.0093\u001b[0m        0.0111  0.7651\n",
            "     62        0.0093        0.0141  0.6853\n",
            "     63        \u001b[36m0.0092\u001b[0m        \u001b[32m0.0079\u001b[0m  0.6772\n",
            "     64        \u001b[36m0.0090\u001b[0m        0.0088  0.6789\n",
            "     65        \u001b[36m0.0088\u001b[0m        0.0120  0.6778\n",
            "     66        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0065\u001b[0m  0.6897\n",
            "     67        \u001b[36m0.0083\u001b[0m        0.0100  0.6878\n",
            "     68        \u001b[36m0.0081\u001b[0m        0.0070  0.6836\n",
            "     69        \u001b[36m0.0077\u001b[0m        0.0074  0.6859\n",
            "     70        0.0080        0.0082  0.6840\n",
            "     71        0.0080        0.0072  0.6809\n",
            "     72        0.0078        0.0075  0.6828\n",
            "     73        0.0082        0.0103  0.6817\n",
            "     74        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0057\u001b[0m  0.6845\n",
            "     75        0.0072        0.0066  0.7237\n",
            "     76        0.0073        0.0066  0.9147\n",
            "     77        0.0077        0.0059  0.9133\n",
            "     78        \u001b[36m0.0069\u001b[0m        \u001b[32m0.0056\u001b[0m  0.9314\n",
            "     79        \u001b[36m0.0068\u001b[0m        0.0079  0.9638\n",
            "     80        0.0071        0.0106  0.7017\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.0min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1035\u001b[0m        \u001b[32m0.2822\u001b[0m  0.7039\n",
            "      2        0.1073        \u001b[32m0.2115\u001b[0m  0.7072\n",
            "      3        \u001b[36m0.1025\u001b[0m        0.2189  0.6957\n",
            "      4        0.1030        \u001b[32m0.1911\u001b[0m  0.7020\n",
            "      5        0.1063        \u001b[32m0.1770\u001b[0m  0.6810\n",
            "      6        0.1045        0.1964  0.6911\n",
            "      7        0.1040        0.1794  0.7013\n",
            "      8        0.1074        \u001b[32m0.1440\u001b[0m  0.7029\n",
            "      9        0.1051        \u001b[32m0.1148\u001b[0m  0.6765\n",
            "     10        \u001b[36m0.0992\u001b[0m        \u001b[32m0.0971\u001b[0m  0.6984\n",
            "     11        \u001b[36m0.0945\u001b[0m        \u001b[32m0.0846\u001b[0m  0.6780\n",
            "     12        \u001b[36m0.0904\u001b[0m        0.0926  0.6761\n",
            "     13        0.0922        \u001b[32m0.0808\u001b[0m  0.6927\n",
            "     14        \u001b[36m0.0870\u001b[0m        0.0910  0.8889\n",
            "     15        \u001b[36m0.0793\u001b[0m        0.0848  0.9166\n",
            "     16        \u001b[36m0.0754\u001b[0m        \u001b[32m0.0800\u001b[0m  0.9297\n",
            "     17        \u001b[36m0.0732\u001b[0m        0.0963  0.9840\n",
            "     18        \u001b[36m0.0714\u001b[0m        \u001b[32m0.0794\u001b[0m  0.7661\n",
            "     19        \u001b[36m0.0710\u001b[0m        \u001b[32m0.0699\u001b[0m  0.6936\n",
            "     20        \u001b[36m0.0613\u001b[0m        \u001b[32m0.0601\u001b[0m  0.7163\n",
            "     21        \u001b[36m0.0553\u001b[0m        0.0755  0.7010\n",
            "     22        0.0559        0.0678  0.7113\n",
            "     23        \u001b[36m0.0512\u001b[0m        0.0818  0.6964\n",
            "     24        \u001b[36m0.0484\u001b[0m        0.0707  0.6912\n",
            "     25        0.0496        \u001b[32m0.0573\u001b[0m  0.6869\n",
            "     26        \u001b[36m0.0440\u001b[0m        0.0706  0.6967\n",
            "     27        0.0458        \u001b[32m0.0544\u001b[0m  0.6981\n",
            "     28        \u001b[36m0.0425\u001b[0m        0.0728  0.6858\n",
            "     29        \u001b[36m0.0414\u001b[0m        0.0890  0.7006\n",
            "     30        0.0415        \u001b[32m0.0535\u001b[0m  0.7039\n",
            "     31        \u001b[36m0.0376\u001b[0m        0.0683  0.6669\n",
            "     32        0.0378        0.0705  0.8021\n",
            "     33        \u001b[36m0.0369\u001b[0m        0.0871  0.9298\n",
            "     34        0.0404        0.0605  0.9212\n",
            "     35        \u001b[36m0.0360\u001b[0m        0.0572  0.9794\n",
            "     36        \u001b[36m0.0341\u001b[0m        0.0603  0.8970\n",
            "     37        \u001b[36m0.0338\u001b[0m        0.0596  0.6850\n",
            "     38        0.0340        0.0661  0.6916\n",
            "     39        0.0341        0.0588  0.6819\n",
            "     40        0.0348        \u001b[32m0.0524\u001b[0m  0.6825\n",
            "     41        0.0350        0.0682  0.6963\n",
            "     42        0.0368        0.0589  0.6806\n",
            "     43        \u001b[36m0.0330\u001b[0m        0.0642  0.7020\n",
            "     44        0.0354        \u001b[32m0.0503\u001b[0m  0.7276\n",
            "     45        0.0330        0.0610  0.6982\n",
            "     46        0.0333        0.0714  0.6908\n",
            "     47        0.0334        0.0810  0.7063\n",
            "     48        0.0345        0.0725  0.6989\n",
            "     49        0.0345        0.0802  0.6907\n",
            "     50        \u001b[36m0.0326\u001b[0m        0.0831  0.7196\n",
            "     51        0.0329        0.0615  0.9109\n",
            "     52        \u001b[36m0.0324\u001b[0m        0.0538  0.9439\n",
            "     53        \u001b[36m0.0311\u001b[0m        \u001b[32m0.0457\u001b[0m  0.9287\n",
            "     54        \u001b[36m0.0308\u001b[0m        \u001b[32m0.0440\u001b[0m  0.9378\n",
            "     55        0.0321        0.0453  0.6708\n",
            "     56        0.0322        0.0470  0.7115\n",
            "     57        0.0330        0.0725  0.7157\n",
            "     58        0.0322        0.1016  0.7042\n",
            "     59        0.0362        0.1051  0.6982\n",
            "     60        0.0328        0.0949  0.7059\n",
            "     61        0.0348        0.0857  0.6894\n",
            "     62        0.0328        0.0671  0.7104\n",
            "     63        0.0319        0.0528  0.7010\n",
            "     64        0.0312        0.0498  0.6852\n",
            "     65        0.0312        0.0514  0.6926\n",
            "     66        0.0324        0.0663  0.6990\n",
            "     67        0.0325        0.0644  0.7088\n",
            "     68        0.0312        0.0625  0.6734\n",
            "     69        \u001b[36m0.0305\u001b[0m        0.0445  0.8974\n",
            "     70        0.0319        0.0509  0.9340\n",
            "     71        0.0311        0.0597  0.9090\n",
            "     72        0.0318        0.0617  0.9759\n",
            "     73        0.0308        0.0559  0.7207\n",
            "     74        \u001b[36m0.0302\u001b[0m        0.0511  0.6916\n",
            "     75        0.0307        0.0597  0.6715\n",
            "     76        0.0318        0.0447  0.6775\n",
            "     77        0.0343        0.0505  0.6765\n",
            "     78        0.0357        0.0617  0.6775\n",
            "     79        0.0352        0.0507  0.6746\n",
            "     80        0.0305        0.0624  0.6908\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  59.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0945\u001b[0m        \u001b[32m0.2903\u001b[0m  0.6504\n",
            "      2        0.0981        \u001b[32m0.1224\u001b[0m  0.6474\n",
            "      3        \u001b[36m0.0808\u001b[0m        \u001b[32m0.0988\u001b[0m  0.6361\n",
            "      4        0.0810        \u001b[32m0.0986\u001b[0m  0.6429\n",
            "      5        \u001b[36m0.0792\u001b[0m        \u001b[32m0.0844\u001b[0m  0.6275\n",
            "      6        \u001b[36m0.0754\u001b[0m        \u001b[32m0.0808\u001b[0m  0.6564\n",
            "      7        \u001b[36m0.0717\u001b[0m        0.0868  0.6897\n",
            "      8        \u001b[36m0.0694\u001b[0m        0.0949  0.8407\n",
            "      9        \u001b[36m0.0629\u001b[0m        0.0867  0.8591\n",
            "     10        \u001b[36m0.0587\u001b[0m        0.1008  0.8519\n",
            "     11        \u001b[36m0.0584\u001b[0m        0.1248  0.8968\n",
            "     12        \u001b[36m0.0551\u001b[0m        0.0890  0.6453\n",
            "     13        0.0552        \u001b[32m0.0749\u001b[0m  0.6316\n",
            "     14        \u001b[36m0.0520\u001b[0m        0.0966  0.6602\n",
            "     15        0.0532        0.1018  0.6647\n",
            "     16        0.0536        0.0943  0.6359\n",
            "     17        \u001b[36m0.0492\u001b[0m        0.1136  0.6467\n",
            "     18        \u001b[36m0.0485\u001b[0m        0.1080  0.6319\n",
            "     19        0.0577        \u001b[32m0.0673\u001b[0m  0.6630\n",
            "     20        0.0540        \u001b[32m0.0650\u001b[0m  0.6405\n",
            "     21        \u001b[36m0.0474\u001b[0m        0.0692  0.6205\n",
            "     22        \u001b[36m0.0455\u001b[0m        0.0893  0.6495\n",
            "     23        0.0456        0.0904  0.6482\n",
            "     24        0.0461        0.0756  0.6295\n",
            "     25        \u001b[36m0.0405\u001b[0m        0.0817  0.6456\n",
            "     26        0.0422        \u001b[32m0.0575\u001b[0m  0.6444\n",
            "     27        0.0409        0.0634  0.7565\n",
            "     28        0.0493        0.1041  0.9418\n",
            "     29        0.0470        0.0846  0.9419\n",
            "     30        0.0447        0.1180  0.9254\n",
            "     31        0.0485        0.0676  1.0282\n",
            "     32        \u001b[36m0.0402\u001b[0m        \u001b[32m0.0544\u001b[0m  1.0273\n",
            "     33        \u001b[36m0.0364\u001b[0m        0.0613  1.0499\n",
            "     34        0.0377        \u001b[32m0.0525\u001b[0m  0.8209\n",
            "     35        \u001b[36m0.0285\u001b[0m        0.0549  0.6631\n",
            "     36        \u001b[36m0.0201\u001b[0m        \u001b[32m0.0463\u001b[0m  0.6523\n",
            "     37        \u001b[36m0.0143\u001b[0m        \u001b[32m0.0434\u001b[0m  0.6472\n",
            "     38        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0360\u001b[0m  0.6501\n",
            "     39        \u001b[36m0.0103\u001b[0m        0.0447  0.6439\n",
            "     40        \u001b[36m0.0089\u001b[0m        0.0525  0.6419\n",
            "     41        \u001b[36m0.0082\u001b[0m        \u001b[32m0.0334\u001b[0m  0.6495\n",
            "     42        0.0087        \u001b[32m0.0176\u001b[0m  0.6408\n",
            "     43        0.0088        0.0212  0.6410\n",
            "     44        \u001b[36m0.0078\u001b[0m        0.0223  0.6455\n",
            "     45        \u001b[36m0.0062\u001b[0m        0.0201  0.6558\n",
            "     46        \u001b[36m0.0053\u001b[0m        0.0264  0.6475\n",
            "     47        0.0060        0.0233  0.6429\n",
            "     48        0.0068        0.0261  0.6367\n",
            "     49        0.0064        0.0674  0.8154\n",
            "     50        0.0072        0.0601  0.8778\n",
            "     51        0.0087        0.0292  0.8465\n",
            "     52        0.0067        0.0204  0.9053\n",
            "     53        \u001b[36m0.0048\u001b[0m        0.0178  0.7751\n",
            "     54        0.0050        \u001b[32m0.0167\u001b[0m  0.6575\n",
            "     55        0.0073        \u001b[32m0.0086\u001b[0m  0.6586\n",
            "     56        0.0052        0.0249  0.6484\n",
            "     57        0.0068        0.0214  0.6572\n",
            "     58        0.0107        0.0766  0.6622\n",
            "     59        0.0126        0.1629  0.6485\n",
            "     60        0.0218        0.0907  0.6894\n",
            "     61        0.0174        0.0804  0.6662\n",
            "     62        0.0157        0.1203  0.6685\n",
            "     63        0.0160        0.1050  0.6546\n",
            "     64        0.0142        0.0547  0.6631\n",
            "     65        0.0127        0.0637  0.6517\n",
            "     66        0.0120        0.0559  0.6494\n",
            "     67        0.0121        0.0597  0.6506\n",
            "     68        0.0100        0.0638  0.7445\n",
            "     69        0.0088        0.0699  0.8536\n",
            "     70        0.0092        0.0571  0.8600\n",
            "     71        0.0090        0.0388  0.8855\n",
            "     72        0.0086        0.0214  0.8802\n",
            "     73        0.0072        0.0403  0.6758\n",
            "     74        0.0083        0.0267  0.6365\n",
            "     75        0.0062        0.0189  0.6458\n",
            "     76        0.0049        0.0145  0.6570\n",
            "     77        \u001b[36m0.0047\u001b[0m        0.0148  0.6588\n",
            "     78        0.0058        0.0173  0.6747\n",
            "     79        0.0056        0.0167  0.6561\n",
            "     80        \u001b[36m0.0046\u001b[0m        0.0179  0.6636\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  57.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0819\u001b[0m        \u001b[32m0.2023\u001b[0m  0.6521\n",
            "      2        0.0933        \u001b[32m0.1303\u001b[0m  0.6614\n",
            "      3        0.0836        \u001b[32m0.1275\u001b[0m  0.6447\n",
            "      4        \u001b[36m0.0806\u001b[0m        \u001b[32m0.1239\u001b[0m  0.6849\n",
            "      5        0.0817        \u001b[32m0.1190\u001b[0m  0.6739\n",
            "      6        \u001b[36m0.0781\u001b[0m        0.1253  0.6754\n",
            "      7        0.0811        \u001b[32m0.1070\u001b[0m  0.7325\n",
            "      8        \u001b[36m0.0765\u001b[0m        \u001b[32m0.0941\u001b[0m  0.8688\n",
            "      9        \u001b[36m0.0712\u001b[0m        \u001b[32m0.0785\u001b[0m  0.8586\n",
            "     10        \u001b[36m0.0694\u001b[0m        0.1026  0.8991\n",
            "     11        0.0735        \u001b[32m0.0777\u001b[0m  0.8967\n",
            "     12        \u001b[36m0.0692\u001b[0m        0.0813  0.6636\n",
            "     13        0.0714        0.1159  0.6498\n",
            "     14        0.0705        0.0914  0.6534\n",
            "     15        0.0704        \u001b[32m0.0773\u001b[0m  0.6428\n",
            "     16        \u001b[36m0.0648\u001b[0m        0.1241  0.6622\n",
            "     17        0.0722        0.0787  0.6608\n",
            "     18        \u001b[36m0.0639\u001b[0m        0.0936  0.6523\n",
            "     19        \u001b[36m0.0618\u001b[0m        \u001b[32m0.0736\u001b[0m  0.6492\n",
            "     20        \u001b[36m0.0579\u001b[0m        \u001b[32m0.0689\u001b[0m  0.6490\n",
            "     21        \u001b[36m0.0467\u001b[0m        0.0937  0.6394\n",
            "     22        \u001b[36m0.0354\u001b[0m        0.0775  0.6471\n",
            "     23        \u001b[36m0.0288\u001b[0m        0.0875  0.6678\n",
            "     24        \u001b[36m0.0240\u001b[0m        0.1703  0.6387\n",
            "     25        \u001b[36m0.0201\u001b[0m        0.1587  0.6788\n",
            "     26        \u001b[36m0.0174\u001b[0m        0.1274  0.6710\n",
            "     27        \u001b[36m0.0158\u001b[0m        0.0855  0.8234\n",
            "     28        0.0161        \u001b[32m0.0624\u001b[0m  0.8831\n",
            "     29        0.0177        \u001b[32m0.0362\u001b[0m  0.8595\n",
            "     30        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0325\u001b[0m  0.9142\n",
            "     31        \u001b[36m0.0102\u001b[0m        0.0339  0.8598\n",
            "     32        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0324\u001b[0m  0.6782\n",
            "     33        \u001b[36m0.0082\u001b[0m        \u001b[32m0.0305\u001b[0m  0.6710\n",
            "     34        0.0084        0.0314  0.6726\n",
            "     35        0.0084        \u001b[32m0.0305\u001b[0m  0.6793\n",
            "     36        \u001b[36m0.0077\u001b[0m        \u001b[32m0.0283\u001b[0m  0.6780\n",
            "     37        \u001b[36m0.0067\u001b[0m        \u001b[32m0.0262\u001b[0m  0.6616\n",
            "     38        \u001b[36m0.0063\u001b[0m        0.0280  0.6758\n",
            "     39        \u001b[36m0.0058\u001b[0m        0.0327  0.6708\n",
            "     40        \u001b[36m0.0048\u001b[0m        0.0374  0.6567\n",
            "     41        0.0079        \u001b[32m0.0252\u001b[0m  0.6515\n",
            "     42        0.0066        0.0254  0.6304\n",
            "     43        0.0067        0.0492  0.6420\n",
            "     44        0.0084        0.0582  0.6502\n",
            "     45        \u001b[36m0.0042\u001b[0m        0.0347  0.6487\n",
            "     46        0.0057        0.0320  0.7174\n",
            "     47        \u001b[36m0.0030\u001b[0m        0.0440  0.8565\n",
            "     48        0.0032        0.0439  0.8589\n",
            "     49        \u001b[36m0.0026\u001b[0m        0.0593  0.8729\n",
            "     50        0.0074        0.0978  0.8888\n",
            "     51        0.0059        \u001b[32m0.0217\u001b[0m  0.6923\n",
            "     52        0.0044        \u001b[32m0.0175\u001b[0m  0.6737\n",
            "     53        0.0053        \u001b[32m0.0077\u001b[0m  0.6621\n",
            "     54        \u001b[36m0.0024\u001b[0m        \u001b[32m0.0055\u001b[0m  0.6470\n",
            "     55        0.0029        0.0100  0.6602\n",
            "     56        0.0026        0.0236  0.6478\n",
            "     57        \u001b[36m0.0023\u001b[0m        0.0330  0.6447\n",
            "     58        0.0030        0.0331  0.6623\n",
            "     59        0.0031        0.0365  0.6407\n",
            "     60        \u001b[36m0.0021\u001b[0m        0.0268  0.6437\n",
            "     61        0.0050        0.0630  0.6457\n",
            "     62        0.0067        0.0405  0.6478\n",
            "     63        0.0068        0.0202  0.6564\n",
            "     64        0.0037        0.0080  0.6634\n",
            "     65        0.0026        0.0144  0.6605\n",
            "     66        0.0022        0.0072  0.7611\n",
            "     67        \u001b[36m0.0020\u001b[0m        0.0114  0.8858\n",
            "     68        0.0027        0.0220  0.8482\n",
            "     69        0.0037        0.0152  0.9147\n",
            "     70        0.0028        0.0127  0.8602\n",
            "     71        0.0020        0.0318  0.6414\n",
            "     72        0.0029        0.0480  0.6530\n",
            "     73        0.0039        0.0090  0.6466\n",
            "     74        0.0035        0.0172  0.6376\n",
            "     75        0.0034        0.0103  0.6488\n",
            "     76        \u001b[36m0.0018\u001b[0m        0.0126  0.6504\n",
            "     77        0.0034        0.0213  0.6437\n",
            "     78        0.0040        0.0082  0.6410\n",
            "     79        0.0038        \u001b[32m0.0051\u001b[0m  0.6517\n",
            "     80        0.0061        0.0087  0.6460\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  56.9s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0942\u001b[0m        \u001b[32m0.3038\u001b[0m  0.7394\n",
            "      2        \u001b[36m0.0926\u001b[0m        0.3098  0.7467\n",
            "      3        \u001b[36m0.0842\u001b[0m        \u001b[32m0.2869\u001b[0m  0.7297\n",
            "      4        \u001b[36m0.0838\u001b[0m        0.3339  0.7383\n",
            "      5        0.0871        0.3473  0.8828\n",
            "      6        0.0849        0.3398  0.9582\n",
            "      7        0.0865        0.2942  0.9520\n",
            "      8        0.0852        \u001b[32m0.2735\u001b[0m  0.9846\n",
            "      9        0.0876        \u001b[32m0.2649\u001b[0m  0.8396\n",
            "     10        \u001b[36m0.0747\u001b[0m        0.2784  0.7443\n",
            "     11        0.0881        \u001b[32m0.2448\u001b[0m  0.7417\n",
            "     12        0.0935        \u001b[32m0.2008\u001b[0m  0.7348\n",
            "     13        0.0950        \u001b[32m0.1912\u001b[0m  0.7467\n",
            "     14        0.1004        \u001b[32m0.1658\u001b[0m  0.7571\n",
            "     15        0.0950        \u001b[32m0.1386\u001b[0m  0.7571\n",
            "     16        0.0950        \u001b[32m0.1100\u001b[0m  0.7337\n",
            "     17        0.0902        0.1186  0.7456\n",
            "     18        0.0963        0.1106  0.7417\n",
            "     19        0.0947        \u001b[32m0.1006\u001b[0m  0.7367\n",
            "     20        0.0870        \u001b[32m0.0998\u001b[0m  0.7431\n",
            "     21        0.0861        \u001b[32m0.0847\u001b[0m  0.7579\n",
            "     22        0.0801        0.0908  0.8220\n",
            "     23        0.0749        \u001b[32m0.0811\u001b[0m  0.9524\n",
            "     24        \u001b[36m0.0636\u001b[0m        \u001b[32m0.0699\u001b[0m  0.9797\n",
            "     25        \u001b[36m0.0579\u001b[0m        0.0950  1.0027\n",
            "     26        0.0751        0.0998  0.9079\n",
            "     27        \u001b[36m0.0521\u001b[0m        0.0796  0.7519\n",
            "     28        \u001b[36m0.0435\u001b[0m        0.0994  0.7453\n",
            "     29        \u001b[36m0.0399\u001b[0m        \u001b[32m0.0635\u001b[0m  0.7488\n",
            "     30        0.0457        \u001b[32m0.0594\u001b[0m  0.7371\n",
            "     31        \u001b[36m0.0332\u001b[0m        0.0690  0.7246\n",
            "     32        \u001b[36m0.0303\u001b[0m        0.0757  0.7520\n",
            "     33        0.0352        \u001b[32m0.0489\u001b[0m  0.7832\n",
            "     34        0.0314        0.0544  0.7645\n",
            "     35        \u001b[36m0.0282\u001b[0m        \u001b[32m0.0462\u001b[0m  0.7503\n",
            "     36        \u001b[36m0.0264\u001b[0m        0.0540  0.7509\n",
            "     37        \u001b[36m0.0238\u001b[0m        0.0653  0.7678\n",
            "     38        \u001b[36m0.0215\u001b[0m        0.0517  0.7687\n",
            "     39        \u001b[36m0.0209\u001b[0m        0.0470  0.8431\n",
            "     40        \u001b[36m0.0187\u001b[0m        \u001b[32m0.0311\u001b[0m  0.9780\n",
            "     41        \u001b[36m0.0176\u001b[0m        \u001b[32m0.0293\u001b[0m  0.9935\n",
            "     42        \u001b[36m0.0169\u001b[0m        0.0301  1.0062\n",
            "     43        \u001b[36m0.0151\u001b[0m        0.0299  0.9118\n",
            "     44        0.0156        \u001b[32m0.0210\u001b[0m  0.7407\n",
            "     45        0.0154        0.0260  0.7508\n",
            "     46        \u001b[36m0.0146\u001b[0m        0.0415  0.7510\n",
            "     47        \u001b[36m0.0142\u001b[0m        0.0254  0.7430\n",
            "     48        \u001b[36m0.0136\u001b[0m        0.0212  0.7480\n",
            "     49        0.0137        \u001b[32m0.0208\u001b[0m  0.7494\n",
            "     50        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0156\u001b[0m  0.7455\n",
            "     51        \u001b[36m0.0129\u001b[0m        0.0156  0.7491\n",
            "     52        0.0130        \u001b[32m0.0110\u001b[0m  0.7330\n",
            "     53        0.0129        0.0125  0.7734\n",
            "     54        \u001b[36m0.0121\u001b[0m        0.0144  0.7470\n",
            "     55        \u001b[36m0.0118\u001b[0m        0.0166  0.7384\n",
            "     56        0.0124        0.0131  0.7695\n",
            "     57        \u001b[36m0.0112\u001b[0m        0.0205  0.9698\n",
            "     58        0.0115        0.0185  0.9820\n",
            "     59        0.0113        0.0200  1.0137\n",
            "     60        \u001b[36m0.0108\u001b[0m        0.0132  0.9776\n",
            "     61        \u001b[36m0.0102\u001b[0m        0.0221  0.7968\n",
            "     62        \u001b[36m0.0097\u001b[0m        0.0113  0.7720\n",
            "     63        \u001b[36m0.0093\u001b[0m        0.0122  0.7688\n",
            "     64        0.0105        \u001b[32m0.0081\u001b[0m  0.7652\n",
            "     65        0.0100        0.0196  0.7473\n",
            "     66        0.0093        0.0163  0.7576\n",
            "     67        \u001b[36m0.0092\u001b[0m        0.0346  0.7310\n",
            "     68        \u001b[36m0.0084\u001b[0m        0.0276  0.7554\n",
            "     69        0.0097        0.0203  0.7492\n",
            "     70        \u001b[36m0.0083\u001b[0m        0.0183  0.7432\n",
            "     71        \u001b[36m0.0080\u001b[0m        0.0131  0.7601\n",
            "     72        \u001b[36m0.0072\u001b[0m        0.0139  0.7506\n",
            "     73        0.0083        0.0228  0.7729\n",
            "     74        0.0110        0.0377  0.9935\n",
            "     75        0.0117        0.0258  0.9836\n",
            "     76        0.0093        0.0152  1.0218\n",
            "     77        0.0072        0.0221  0.9872\n",
            "     78        0.0077        0.0107  0.7441\n",
            "     79        0.0074        0.0158  0.7400\n",
            "     80        \u001b[36m0.0068\u001b[0m        0.0102  0.7507\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0916\u001b[0m        \u001b[32m0.3030\u001b[0m  0.7515\n",
            "      2        0.0933        \u001b[32m0.2481\u001b[0m  0.7398\n",
            "      3        \u001b[36m0.0823\u001b[0m        0.2848  0.7545\n",
            "      4        0.0848        0.3416  0.7539\n",
            "      5        0.0932        0.2939  0.7441\n",
            "      6        \u001b[36m0.0810\u001b[0m        0.2998  0.7529\n",
            "      7        0.0867        0.2835  0.7484\n",
            "      8        0.0847        0.2938  0.7688\n",
            "      9        0.0848        0.3493  0.7447\n",
            "     10        0.0876        0.3750  0.7769\n",
            "     11        0.0919        0.3541  0.9874\n",
            "     12        0.0893        0.4041  0.9608\n",
            "     13        0.0948        0.2518  0.9771\n",
            "     14        0.0916        0.2799  0.9853\n",
            "     15        0.0928        0.2645  0.7328\n",
            "     16        0.0916        0.3733  0.7326\n",
            "     17        0.0928        0.2947  0.7365\n",
            "     18        0.0937        0.3358  0.7759\n",
            "     19        0.0847        0.3008  0.7403\n",
            "     20        0.0840        0.3110  0.7384\n",
            "     21        0.0820        0.3226  0.7376\n",
            "     22        0.0832        0.2990  0.7414\n",
            "     23        0.0857        0.3645  0.7372\n",
            "     24        0.0926        0.2860  0.7359\n",
            "     25        0.0912        0.3275  0.7395\n",
            "     26        0.0869        0.2657  0.7499\n",
            "     27        \u001b[36m0.0809\u001b[0m        0.3326  0.7387\n",
            "     28        0.0819        0.3113  0.8834\n",
            "     29        \u001b[36m0.0772\u001b[0m        0.3244  0.9792\n",
            "     30        0.0826        0.2907  0.9561\n",
            "     31        0.0860        \u001b[32m0.2445\u001b[0m  1.0176\n",
            "     32        0.0866        0.2612  0.7665\n",
            "     33        0.0898        0.2561  0.7399\n",
            "     34        0.0889        0.2665  0.7336\n",
            "     35        0.0862        0.2711  0.7407\n",
            "     36        0.0892        0.2754  0.7297\n",
            "     37        0.0867        0.2793  0.7427\n",
            "     38        0.0879        0.2597  0.7440\n",
            "     39        0.0868        0.2619  0.7387\n",
            "     40        0.0867        0.2475  0.7403\n",
            "     41        0.0864        0.2670  0.7337\n",
            "     42        0.0874        0.2564  0.7444\n",
            "     43        0.0870        0.2456  0.7403\n",
            "     44        0.0861        0.2768  0.7368\n",
            "     45        0.0874        0.2717  0.8702\n",
            "     46        0.0871        0.2672  0.9777\n",
            "     47        0.0867        0.2622  0.9555\n",
            "     48        0.0873        0.2637  0.9969\n",
            "     49        0.0868        0.2702  0.8667\n",
            "     50        0.0864        0.2666  0.7434\n",
            "     51        0.0882        0.2756  0.7216\n",
            "     52        0.0883        0.2785  0.7221\n",
            "     53        0.0859        0.2708  0.7438\n",
            "     54        0.0860        0.2686  0.7401\n",
            "     55        0.0862        0.2622  0.7285\n",
            "     56        0.0863        0.2693  0.7705\n",
            "     57        0.0862        0.2588  0.7694\n",
            "     58        0.0852        0.2706  0.7394\n",
            "     59        0.0870        0.2683  0.7601\n",
            "     60        0.0858        0.2701  0.7461\n",
            "     61        0.0858        0.2603  0.7542\n",
            "     62        0.0866        0.2562  0.7995\n",
            "     63        0.0875        0.2637  0.9634\n",
            "     64        0.0860        0.2566  0.9626\n",
            "     65        0.0864        0.2562  1.0018\n",
            "     66        0.0865        0.2638  0.9235\n",
            "     67        0.0872        0.2570  0.7397\n",
            "     68        0.0854        0.2673  0.7533\n",
            "     69        0.0870        0.2548  0.7627\n",
            "     70        0.0851        0.2678  0.7756\n",
            "     71        0.0857        0.2677  0.7488\n",
            "     72        0.0867        0.2683  0.7556\n",
            "     73        0.0875        0.2573  0.7571\n",
            "     74        0.0870        0.2568  0.7503\n",
            "     75        0.0885        0.2587  0.7355\n",
            "     76        0.0870        0.2536  0.7530\n",
            "     77        0.0875        0.2565  0.9708\n",
            "     78        0.0872        0.2502  1.0097\n",
            "     79        0.0864        0.2522  1.1056\n",
            "     80        0.0869        0.2558  1.1150\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.1min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
              "  module=<class 'GRU_model.GRUNet'>,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={'batch_size': [16, 32, 64],\n",
              "                         'iterator_train__shuffle': [False], 'max_epochs': [80],\n",
              "                         'module__drop_prob': [0.5],\n",
              "                         'module__hidden_dim': [16, 32, 64],\n",
              "                         'module__n_layers': [1, 2], 'optimizer__lr': [0.001],\n",
              "                         'optimizer__weight_decay': [0.0001, 0.001]},\n",
              "             refit=False, scoring='neg_mean_squared_error', verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={&#x27;batch_size&#x27;: [16, 32, 64],\n",
              "                         &#x27;iterator_train__shuffle&#x27;: [False], &#x27;max_epochs&#x27;: [80],\n",
              "                         &#x27;module__drop_prob&#x27;: [0.5],\n",
              "                         &#x27;module__hidden_dim&#x27;: [16, 32, 64],\n",
              "                         &#x27;module__n_layers&#x27;: [1, 2], &#x27;optimizer__lr&#x27;: [0.001],\n",
              "                         &#x27;optimizer__weight_decay&#x27;: [0.0001, 0.001]},\n",
              "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={&#x27;batch_size&#x27;: [16, 32, 64],\n",
              "                         &#x27;iterator_train__shuffle&#x27;: [False], &#x27;max_epochs&#x27;: [80],\n",
              "                         &#x27;module__drop_prob&#x27;: [0.5],\n",
              "                         &#x27;module__hidden_dim&#x27;: [16, 32, 64],\n",
              "                         &#x27;module__n_layers&#x27;: [1, 2], &#x27;optimizer__lr&#x27;: [0.001],\n",
              "                         &#x27;optimizer__weight_decay&#x27;: [0.0001, 0.001]},\n",
              "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NeuralNetRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NeuralNetRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              ")</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "gs = GridSearchCV(net, params, refit=False, cv=ps, scoring='neg_mean_squared_error', verbose=2)\n",
        "gs.fit(solar_X_combined, solar_y_combined)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = gs.best_params_\n",
        "best_score = gs.best_score_\n",
        "print(\"Best parameters:\", best_parameters)\n",
        "print(\"Best score (negative MSE):\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbJwaScBmJTv",
        "outputId": "998bdb61-a856-450c-d3e8-23e4a7d9d32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'batch_size': 64, 'iterator_train__shuffle': False, 'max_epochs': 80, 'module__drop_prob': 0.5, 'module__hidden_dim': 64, 'module__n_layers': 1, 'optimizer__lr': 0.001, 'optimizer__weight_decay': 0.001}\n",
            "Best score (negative MSE): -0.009975397028028965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOIfagHaeH1z"
      },
      "outputs": [],
      "source": [
        "gs_results = pd.DataFrame(gs.cv_results_)\n",
        "gs_results\n",
        "gs_results.to_csv('/content/drive/MyDrive/solar_data/gird_search_results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = [col for col in gs_results.columns if col.startswith('param_')]\n",
        "scores = 'mean_test_score'"
      ],
      "metadata": {
        "id": "L_mIiDKRD8L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup figure and axis\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Invert negative MSE to make higher scores appear better\n",
        "gs_results['mean_test_score_inverted'] = gs_results['mean_test_score'] * -1\n",
        "\n",
        "# Identify the index of the best (maximum) score\n",
        "best_index = gs_results['mean_test_score_inverted'].idxmin()\n",
        "\n",
        "# Plot all mean test scores\n",
        "ax.scatter(range(len(gs_results)), gs_results['mean_test_score_inverted'], color='blue')\n",
        "\n",
        "# Highlight the best score in red\n",
        "ax.scatter(best_index, gs_results['mean_test_score_inverted'].loc[best_index], color='red')\n",
        "\n",
        "# Set plot titles and labels\n",
        "ax.set_title('Validation Results for Each Parameter Combination')\n",
        "ax.set_xlabel('Parameter Combination Index')\n",
        "ax.set_ylabel('Mean Test Score (Inverted)')\n",
        "\n",
        "# Adjust layout to make sure everything fits well\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "NHmooxiUESFb",
        "outputId": "97199ae0-b0c5-45c3-9627-01b649accff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'gs_results' is your DataFrame\n",
        "# Convert mean_test_score to positive values if they're currently negative\n",
        "gs_results['mean_test_score_positive'] = gs_results['mean_test_score'] * -1\n",
        "\n",
        "# Create a pivot table\n",
        "pivot_table = gs_results.pivot_table(\n",
        "    values='mean_test_score_positive',\n",
        "    index='param_module__hidden_dim',  # Rows will represent hidden dimensions\n",
        "    columns='param_module__n_layers',  # Columns will represent the number of layers\n",
        "    aggfunc='max'  # Max function to use if multiple entries exist for the same combination\n",
        ")\n",
        "\n",
        "# Check the pivot table\n",
        "print(pivot_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "oGevKtsuIf0c",
        "outputId": "78a300d8-ea9f-42e0-88d1-bd061f44729a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gs_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f1bd62e2b208>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming 'gs_results' is your DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Convert mean_test_score to positive values if they're currently negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgs_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score_positive'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a pivot table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gs_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO retrain on entire dataset with best parameters\n",
        "#Test on test set\n",
        "# same for wind data and 24h prediction data\n",
        "#Best parameters: {'batch_size': 64, 'iterator_train__shuffle': False, 'max_epochs': 80, 'module__drop_prob': 0.5, 'module__hidden_dim': 64, 'module__n_layers': 1, 'optimizer__lr': 0.001, 'optimizer__weight_decay': 0.001}\n"
      ],
      "metadata": {
        "id": "cjLNyiQ8IzyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs = GridSearchCV(net, params, refit=False, cv=ps_wind, scoring='neg_mean_squared_error', verbose=2)\n",
        "gs.fit(wind_X_combined, wind_y_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_V319eTOsSQs",
        "outputId": "e361bf79-6565-487f-e725-4bd45b63f8eb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 36 candidates, totalling 36 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1475\u001b[0m        \u001b[32m0.1993\u001b[0m  3.0150\n",
            "      2        \u001b[36m0.1395\u001b[0m        \u001b[32m0.1808\u001b[0m  2.3048\n",
            "      3        \u001b[36m0.1323\u001b[0m        \u001b[32m0.1363\u001b[0m  2.6862\n",
            "      4        \u001b[36m0.1255\u001b[0m        \u001b[32m0.1194\u001b[0m  2.5643\n",
            "      5        \u001b[36m0.1061\u001b[0m        \u001b[32m0.0975\u001b[0m  2.1683\n",
            "      6        \u001b[36m0.0949\u001b[0m        \u001b[32m0.0572\u001b[0m  2.1620\n",
            "      7        \u001b[36m0.0507\u001b[0m        \u001b[32m0.0187\u001b[0m  2.1470\n",
            "      8        \u001b[36m0.0290\u001b[0m        0.0480  2.2176\n",
            "      9        \u001b[36m0.0212\u001b[0m        0.0194  2.6967\n",
            "     10        \u001b[36m0.0161\u001b[0m        0.0327  2.4295\n",
            "     11        \u001b[36m0.0142\u001b[0m        0.0524  2.1608\n",
            "     12        0.0165        0.0360  2.1685\n",
            "     13        0.0158        0.0463  2.1923\n",
            "     14        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0137\u001b[0m  2.1630\n",
            "     15        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0089\u001b[0m  2.8199\n",
            "     16        \u001b[36m0.0111\u001b[0m        0.0377  2.4077\n",
            "     17        0.0112        0.0669  2.1861\n",
            "     18        0.0113        0.0552  2.1961\n",
            "     19        0.0111        0.0463  2.1607\n",
            "     20        0.0119        0.1122  2.2614\n",
            "     21        \u001b[36m0.0104\u001b[0m        0.0708  2.8506\n",
            "     22        \u001b[36m0.0096\u001b[0m        0.0753  2.3282\n",
            "     23        \u001b[36m0.0085\u001b[0m        0.0400  2.4786\n",
            "     24        \u001b[36m0.0081\u001b[0m        0.0693  2.5274\n",
            "     25        \u001b[36m0.0063\u001b[0m        0.0643  2.9046\n",
            "     26        0.0076        0.0722  2.6747\n",
            "     27        \u001b[36m0.0059\u001b[0m        0.0794  2.5005\n",
            "     28        \u001b[36m0.0050\u001b[0m        0.0696  2.1885\n",
            "     29        0.0056        0.0648  2.1171\n",
            "     30        \u001b[36m0.0046\u001b[0m        0.0348  2.1827\n",
            "     31        0.0083        0.0576  2.1508\n",
            "     32        0.0058        0.0468  2.7587\n",
            "     33        0.0049        0.1014  2.4685\n",
            "     34        0.0049        0.0426  2.1799\n",
            "     35        0.0053        0.1145  2.1817\n",
            "     36        0.0071        0.0416  2.1445\n",
            "     37        0.0069        0.0183  2.2545\n",
            "     38        \u001b[36m0.0034\u001b[0m        0.0434  2.7819\n",
            "     39        \u001b[36m0.0033\u001b[0m        0.0119  2.3536\n",
            "     40        0.0051        0.0228  2.2233\n",
            "     41        0.0033        0.0178  2.2271\n",
            "     42        \u001b[36m0.0024\u001b[0m        0.0105  2.1725\n",
            "     43        0.0053        0.0241  2.3994\n",
            "     44        \u001b[36m0.0024\u001b[0m        0.0096  2.7518\n",
            "     45        \u001b[36m0.0021\u001b[0m        \u001b[32m0.0079\u001b[0m  2.2034\n",
            "     46        0.0051        0.0186  2.1972\n",
            "     47        0.0031        0.0151  2.1816\n",
            "     48        0.0021        0.0297  2.1694\n",
            "     49        0.0024        0.0454  2.4215\n",
            "     50        0.0035        0.0383  2.8216\n",
            "     51        \u001b[36m0.0016\u001b[0m        0.0377  2.1833\n",
            "     52        0.0021        0.0091  2.1940\n",
            "     53        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0066\u001b[0m  2.2163\n",
            "     54        0.0021        \u001b[32m0.0046\u001b[0m  2.1921\n",
            "     55        0.0042        \u001b[32m0.0042\u001b[0m  2.4839\n",
            "     56        0.0042        0.0287  2.7154\n",
            "     57        0.0038        0.0081  2.2101\n",
            "     58        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0025\u001b[0m  2.1734\n",
            "     59        0.0031        \u001b[32m0.0008\u001b[0m  2.2277\n",
            "     60        0.0014        0.0028  2.1727\n",
            "     61        0.0041        0.0027  3.3857\n",
            "     62        0.0016        0.2612  3.0789\n",
            "     63        0.0076        0.1599  2.1725\n",
            "     64        0.0050        0.1135  2.1811\n",
            "     65        0.0045        0.0352  2.1965\n",
            "     66        0.0031        0.0064  2.3478\n",
            "     67        0.0020        0.0068  2.8161\n",
            "     68        0.0023        0.0009  2.3099\n",
            "     69        0.0022        0.0010  2.1584\n",
            "     70        0.0013        0.0028  2.1558\n",
            "     71        0.0013        0.0011  2.1603\n",
            "     72        0.0012        0.0024  2.3374\n",
            "     73        0.0043        0.0510  2.7949\n",
            "     74        0.0046        0.0206  2.3073\n",
            "     75        0.0013        0.0170  2.1712\n",
            "     76        0.0015        0.0129  2.1779\n",
            "     77        0.0025        0.0028  2.1812\n",
            "     78        0.0024        0.0626  2.3705\n",
            "     79        0.0037        0.0241  2.8726\n",
            "     80        0.0017        0.0026  2.1977\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.2min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1662\u001b[0m        \u001b[32m0.2186\u001b[0m  2.2308\n",
            "      2        \u001b[36m0.1440\u001b[0m        \u001b[32m0.2020\u001b[0m  2.1781\n",
            "      3        \u001b[36m0.1329\u001b[0m        \u001b[32m0.1537\u001b[0m  2.1761\n",
            "      4        \u001b[36m0.1115\u001b[0m        \u001b[32m0.0992\u001b[0m  2.7338\n",
            "      5        \u001b[36m0.1102\u001b[0m        \u001b[32m0.0987\u001b[0m  2.5964\n",
            "      6        \u001b[36m0.0962\u001b[0m        \u001b[32m0.0932\u001b[0m  2.1756\n",
            "      7        \u001b[36m0.0625\u001b[0m        0.0935  2.1963\n",
            "      8        0.0668        \u001b[32m0.0785\u001b[0m  2.1569\n",
            "      9        \u001b[36m0.0309\u001b[0m        \u001b[32m0.0560\u001b[0m  2.1438\n",
            "     10        \u001b[36m0.0295\u001b[0m        0.0637  2.6280\n",
            "     11        \u001b[36m0.0268\u001b[0m        0.0664  2.5423\n",
            "     12        \u001b[36m0.0258\u001b[0m        0.0689  2.2190\n",
            "     13        \u001b[36m0.0251\u001b[0m        0.0679  2.1727\n",
            "     14        \u001b[36m0.0207\u001b[0m        0.0747  2.1572\n",
            "     15        \u001b[36m0.0185\u001b[0m        \u001b[32m0.0532\u001b[0m  2.1561\n",
            "     16        \u001b[36m0.0155\u001b[0m        0.0580  2.7865\n",
            "     17        \u001b[36m0.0122\u001b[0m        0.1264  2.5087\n",
            "     18        \u001b[36m0.0118\u001b[0m        0.1444  2.1930\n",
            "     19        0.0129        0.1400  2.2274\n",
            "     20        \u001b[36m0.0106\u001b[0m        0.1389  2.1753\n",
            "     21        0.0106        0.0978  2.2847\n",
            "     22        \u001b[36m0.0095\u001b[0m        0.0784  2.7742\n",
            "     23        \u001b[36m0.0091\u001b[0m        0.0825  2.3228\n",
            "     24        \u001b[36m0.0085\u001b[0m        \u001b[32m0.0452\u001b[0m  2.1782\n",
            "     25        \u001b[36m0.0079\u001b[0m        0.0575  2.1991\n",
            "     26        \u001b[36m0.0076\u001b[0m        0.0585  2.1555\n",
            "     27        \u001b[36m0.0075\u001b[0m        0.0588  2.2966\n",
            "     28        \u001b[36m0.0072\u001b[0m        0.0541  2.7870\n",
            "     29        \u001b[36m0.0070\u001b[0m        0.0557  2.2508\n",
            "     30        \u001b[36m0.0068\u001b[0m        0.0588  2.2329\n",
            "     31        \u001b[36m0.0067\u001b[0m        0.0688  2.1865\n",
            "     32        0.0068        0.0584  2.1598\n",
            "     33        \u001b[36m0.0067\u001b[0m        0.0680  2.4047\n",
            "     34        \u001b[36m0.0066\u001b[0m        0.0721  2.8578\n",
            "     35        \u001b[36m0.0064\u001b[0m        0.0643  2.1967\n",
            "     36        \u001b[36m0.0062\u001b[0m        0.0633  2.1375\n",
            "     37        \u001b[36m0.0061\u001b[0m        0.0563  2.1691\n",
            "     38        0.0062        0.0537  2.1846\n",
            "     39        \u001b[36m0.0060\u001b[0m        0.0629  2.3979\n",
            "     40        \u001b[36m0.0059\u001b[0m        0.0601  2.7404\n",
            "     41        0.0061        0.0510  2.2084\n",
            "     42        \u001b[36m0.0059\u001b[0m        0.0633  2.1868\n",
            "     43        0.0060        0.0557  2.1884\n",
            "     44        0.0060        0.0486  2.2435\n",
            "     45        0.0059        \u001b[32m0.0447\u001b[0m  2.5769\n",
            "     46        \u001b[36m0.0058\u001b[0m        0.0598  2.5882\n",
            "     47        0.0061        \u001b[32m0.0324\u001b[0m  2.1689\n",
            "     48        \u001b[36m0.0057\u001b[0m        0.0787  2.1965\n",
            "     49        0.0061        0.0483  2.1862\n",
            "     50        \u001b[36m0.0056\u001b[0m        0.0480  2.1637\n",
            "     51        0.0060        0.0570  2.6815\n",
            "     52        0.0057        0.0836  2.4929\n",
            "     53        0.0061        0.0750  2.2221\n",
            "     54        0.0061        0.0750  2.1959\n",
            "     55        0.0058        0.0754  2.2012\n",
            "     56        0.0062        0.0830  2.2250\n",
            "     57        \u001b[36m0.0055\u001b[0m        0.0647  2.8399\n",
            "     58        0.0066        0.0918  2.3114\n",
            "     59        0.0064        0.0886  2.1976\n",
            "     60        0.0063        0.0780  2.1989\n",
            "     61        0.0062        0.0688  2.1480\n",
            "     62        0.0061        0.0826  2.3220\n",
            "     63        0.0061        0.0714  2.8454\n",
            "     64        \u001b[36m0.0053\u001b[0m        0.0830  2.2528\n",
            "     65        0.0070        0.1086  2.1505\n",
            "     66        0.0063        0.0948  2.1733\n",
            "     67        0.0062        0.0841  2.1642\n",
            "     68        0.0063        0.0865  2.4199\n",
            "     69        0.0066        0.0545  2.9035\n",
            "     70        0.0065        0.1022  2.1821\n",
            "     71        0.0063        0.0734  2.1997\n",
            "     72        0.0061        0.0872  2.2520\n",
            "     73        0.0063        0.0820  2.1878\n",
            "     74        0.0065        0.0927  2.5720\n",
            "     75        0.0064        0.0556  2.7269\n",
            "     76        0.0066        0.0744  2.1899\n",
            "     77        0.0066        0.0584  2.2148\n",
            "     78        0.0066        0.0716  2.1692\n",
            "     79        0.0064        0.0644  2.1732\n",
            "     80        0.0061        0.0699  2.6479\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1514\u001b[0m        \u001b[32m0.2114\u001b[0m  2.7055\n",
            "      2        \u001b[36m0.1480\u001b[0m        0.2146  2.3666\n",
            "      3        0.1482        \u001b[32m0.2101\u001b[0m  2.3808\n",
            "      4        0.1491        0.2122  2.3789\n",
            "      5        \u001b[36m0.1460\u001b[0m        \u001b[32m0.2021\u001b[0m  2.6893\n",
            "      6        0.1466        \u001b[32m0.1936\u001b[0m  2.9171\n",
            "      7        \u001b[36m0.1377\u001b[0m        \u001b[32m0.1773\u001b[0m  2.3603\n",
            "      8        \u001b[36m0.1350\u001b[0m        \u001b[32m0.1630\u001b[0m  2.3973\n",
            "      9        \u001b[36m0.1295\u001b[0m        \u001b[32m0.1121\u001b[0m  2.4004\n",
            "     10        \u001b[36m0.1225\u001b[0m        \u001b[32m0.0908\u001b[0m  2.4922\n",
            "     11        \u001b[36m0.1061\u001b[0m        0.1153  3.1130\n",
            "     12        0.1145        \u001b[32m0.0844\u001b[0m  2.4709\n",
            "     13        \u001b[36m0.1035\u001b[0m        0.0940  2.3806\n",
            "     14        \u001b[36m0.0818\u001b[0m        \u001b[32m0.0732\u001b[0m  2.4225\n",
            "     15        \u001b[36m0.0452\u001b[0m        \u001b[32m0.0483\u001b[0m  2.3399\n",
            "     16        \u001b[36m0.0384\u001b[0m        0.0712  2.9445\n",
            "     17        \u001b[36m0.0374\u001b[0m        \u001b[32m0.0466\u001b[0m  2.7293\n",
            "     18        \u001b[36m0.0315\u001b[0m        \u001b[32m0.0410\u001b[0m  2.3872\n",
            "     19        \u001b[36m0.0293\u001b[0m        \u001b[32m0.0375\u001b[0m  2.3765\n",
            "     20        \u001b[36m0.0253\u001b[0m        0.0380  2.3885\n",
            "     21        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0303\u001b[0m  2.7086\n",
            "     22        \u001b[36m0.0230\u001b[0m        \u001b[32m0.0233\u001b[0m  2.9800\n",
            "     23        \u001b[36m0.0215\u001b[0m        0.0435  2.3684\n",
            "     24        \u001b[36m0.0214\u001b[0m        0.0388  2.3832\n",
            "     25        \u001b[36m0.0178\u001b[0m        0.0280  2.3498\n",
            "     26        0.0190        \u001b[32m0.0211\u001b[0m  2.3879\n",
            "     27        0.0193        0.0224  3.0937\n",
            "     28        \u001b[36m0.0157\u001b[0m        0.0292  2.5534\n",
            "     29        \u001b[36m0.0152\u001b[0m        0.0345  2.3951\n",
            "     30        0.0170        0.0383  2.3740\n",
            "     31        \u001b[36m0.0148\u001b[0m        0.0432  2.3668\n",
            "     32        \u001b[36m0.0145\u001b[0m        0.0313  2.9047\n",
            "     33        \u001b[36m0.0137\u001b[0m        0.0324  2.7721\n",
            "     34        0.0182        0.0519  2.3692\n",
            "     35        0.0160        0.0212  2.3251\n",
            "     36        0.0139        0.0257  2.3728\n",
            "     37        \u001b[36m0.0130\u001b[0m        0.0354  2.6225\n",
            "     38        \u001b[36m0.0126\u001b[0m        0.0374  3.1042\n",
            "     39        0.0128        0.0415  2.3870\n",
            "     40        \u001b[36m0.0120\u001b[0m        0.0285  2.3862\n",
            "     41        \u001b[36m0.0114\u001b[0m        0.0367  2.3671\n",
            "     42        \u001b[36m0.0094\u001b[0m        0.0331  2.3856\n",
            "     43        \u001b[36m0.0082\u001b[0m        0.0309  3.0276\n",
            "     44        0.0082        0.0460  2.6654\n",
            "     45        0.0083        0.0443  2.3391\n",
            "     46        \u001b[36m0.0076\u001b[0m        0.0455  2.3238\n",
            "     47        \u001b[36m0.0067\u001b[0m        0.0552  2.3567\n",
            "     48        \u001b[36m0.0065\u001b[0m        0.0478  2.7484\n",
            "     49        0.0091        0.0753  2.9634\n",
            "     50        0.0085        0.0608  2.3411\n",
            "     51        0.0077        0.0584  2.3864\n",
            "     52        \u001b[36m0.0062\u001b[0m        0.0459  2.3822\n",
            "     53        \u001b[36m0.0060\u001b[0m        0.0446  2.4482\n",
            "     54        0.0066        0.0568  3.1312\n",
            "     55        0.0072        0.0813  2.4522\n",
            "     56        0.0070        0.0562  2.3807\n",
            "     57        \u001b[36m0.0056\u001b[0m        0.0585  2.4383\n",
            "     58        0.0061        0.0686  2.3689\n",
            "     59        0.0056        0.0692  2.8555\n",
            "     60        0.0151        0.2545  2.7637\n",
            "     61        0.0106        0.0792  2.4052\n",
            "     62        0.0065        0.0905  2.4153\n",
            "     63        \u001b[36m0.0051\u001b[0m        0.0822  2.3518\n",
            "     64        0.0052        0.0892  2.5718\n",
            "     65        0.0063        0.1134  2.9915\n",
            "     66        0.0071        0.1257  2.3970\n",
            "     67        0.0071        0.1252  2.4400\n",
            "     68        0.0069        0.1203  2.4091\n",
            "     69        0.0070        0.1439  2.4979\n",
            "     70        0.0072        0.1323  3.1251\n",
            "     71        0.0057        0.1512  2.5085\n",
            "     72        0.0060        0.1082  2.3792\n",
            "     73        0.0053        0.1116  2.4340\n",
            "     74        0.0061        0.1258  2.4050\n",
            "     75        0.0055        0.1217  2.9182\n",
            "     76        0.0052        0.1173  2.8045\n",
            "     77        0.0061        0.1088  2.4092\n",
            "     78        0.0056        0.1036  2.3427\n",
            "     79        0.0062        0.1409  2.4055\n",
            "     80        0.0055        0.1111  2.6964\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.4min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1794\u001b[0m        \u001b[32m0.2062\u001b[0m  2.9067\n",
            "      2        \u001b[36m0.1488\u001b[0m        \u001b[32m0.2031\u001b[0m  2.4039\n",
            "      3        \u001b[36m0.1470\u001b[0m        \u001b[32m0.2014\u001b[0m  2.3389\n",
            "      4        0.1483        \u001b[32m0.1950\u001b[0m  2.3384\n",
            "      5        \u001b[36m0.1413\u001b[0m        \u001b[32m0.1755\u001b[0m  2.6181\n",
            "      6        \u001b[36m0.1307\u001b[0m        \u001b[32m0.1568\u001b[0m  3.1372\n",
            "      7        \u001b[36m0.1272\u001b[0m        0.1584  2.3631\n",
            "      8        \u001b[36m0.1210\u001b[0m        \u001b[32m0.1000\u001b[0m  2.3513\n",
            "      9        0.1224        \u001b[32m0.0882\u001b[0m  2.3622\n",
            "     10        \u001b[36m0.1108\u001b[0m        \u001b[32m0.0677\u001b[0m  2.3601\n",
            "     11        \u001b[36m0.1031\u001b[0m        0.1148  2.8921\n",
            "     12        \u001b[36m0.0923\u001b[0m        \u001b[32m0.0643\u001b[0m  2.6564\n",
            "     13        \u001b[36m0.0648\u001b[0m        0.0799  2.3345\n",
            "     14        0.0701        0.0985  2.3753\n",
            "     15        0.0676        0.0710  2.3507\n",
            "     16        \u001b[36m0.0515\u001b[0m        \u001b[32m0.0339\u001b[0m  2.7014\n",
            "     17        \u001b[36m0.0503\u001b[0m        0.1004  2.9160\n",
            "     18        \u001b[36m0.0354\u001b[0m        0.0526  2.4049\n",
            "     19        \u001b[36m0.0336\u001b[0m        \u001b[32m0.0327\u001b[0m  2.3721\n",
            "     20        \u001b[36m0.0307\u001b[0m        \u001b[32m0.0301\u001b[0m  2.3901\n",
            "     21        \u001b[36m0.0293\u001b[0m        \u001b[32m0.0262\u001b[0m  2.4430\n",
            "     22        \u001b[36m0.0284\u001b[0m        \u001b[32m0.0239\u001b[0m  3.1480\n",
            "     23        \u001b[36m0.0278\u001b[0m        \u001b[32m0.0176\u001b[0m  2.4875\n",
            "     24        0.0280        0.0216  2.4143\n",
            "     25        \u001b[36m0.0256\u001b[0m        0.0553  2.3817\n",
            "     26        0.0275        0.0533  2.3988\n",
            "     27        0.0264        0.0822  2.9116\n",
            "     28        0.0308        0.0456  2.6845\n",
            "     29        0.0347        0.0337  2.4166\n",
            "     30        0.0266        0.0488  2.3770\n",
            "     31        0.0257        0.1078  2.3549\n",
            "     32        \u001b[36m0.0236\u001b[0m        0.0813  2.6404\n",
            "     33        0.0242        0.0779  2.9724\n",
            "     34        0.0243        0.1148  2.4345\n",
            "     35        0.0250        0.1669  2.3627\n",
            "     36        0.0239        0.1499  2.3330\n",
            "     37        0.0266        0.0589  2.4189\n",
            "     38        0.0267        0.0793  3.1344\n",
            "     39        0.0296        0.0931  2.4350\n",
            "     40        0.0361        0.1006  2.3585\n",
            "     41        0.0294        0.1681  2.3476\n",
            "     42        0.0299        0.1174  2.3645\n",
            "     43        \u001b[36m0.0225\u001b[0m        0.0839  2.8564\n",
            "     44        0.0245        0.0571  2.8005\n",
            "     45        \u001b[36m0.0221\u001b[0m        0.1325  2.3392\n",
            "     46        0.0263        0.0950  2.3645\n",
            "     47        0.0228        0.1570  2.3460\n",
            "     48        0.0262        0.1447  2.6086\n",
            "     49        \u001b[36m0.0213\u001b[0m        0.1333  3.1631\n",
            "     50        0.0213        0.1374  2.4140\n",
            "     51        0.0222        0.0913  2.3880\n",
            "     52        0.0284        0.2341  2.4154\n",
            "     53        0.0331        0.0483  2.4749\n",
            "     54        \u001b[36m0.0208\u001b[0m        0.0517  3.0333\n",
            "     55        0.0297        0.1903  2.6070\n",
            "     56        0.0655        0.0668  2.3686\n",
            "     57        0.0262        0.0787  2.4088\n",
            "     58        0.0313        0.2521  2.4521\n",
            "     59        0.0777        0.0355  2.8414\n",
            "     60        0.0285        0.0497  2.7955\n",
            "     61        0.0247        0.1183  2.3690\n",
            "     62        \u001b[36m0.0204\u001b[0m        0.0841  2.3911\n",
            "     63        0.0272        0.1606  2.3627\n",
            "     64        0.0300        0.1018  2.5528\n",
            "     65        \u001b[36m0.0195\u001b[0m        0.1009  3.0322\n",
            "     66        \u001b[36m0.0193\u001b[0m        0.0903  2.3428\n",
            "     67        \u001b[36m0.0185\u001b[0m        0.1201  2.3892\n",
            "     68        0.0212        0.1185  2.3746\n",
            "     69        \u001b[36m0.0164\u001b[0m        0.1003  2.3669\n",
            "     70        \u001b[36m0.0163\u001b[0m        0.1228  3.0030\n",
            "     71        0.0172        0.1341  2.6239\n",
            "     72        0.0173        0.1444  2.3705\n",
            "     73        \u001b[36m0.0153\u001b[0m        0.1373  2.3362\n",
            "     74        0.0197        0.1304  2.3603\n",
            "     75        0.0181        0.1275  2.7516\n",
            "     76        0.0186        0.1167  2.9376\n",
            "     77        0.0221        0.1249  2.3515\n",
            "     78        0.0260        0.1531  2.3694\n",
            "     79        0.0223        0.1523  2.3620\n",
            "     80        0.0180        0.1436  2.4243\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1559\u001b[0m        \u001b[32m0.2032\u001b[0m  2.8424\n",
            "      2        \u001b[36m0.1376\u001b[0m        \u001b[32m0.1588\u001b[0m  2.1688\n",
            "      3        \u001b[36m0.1226\u001b[0m        \u001b[32m0.1230\u001b[0m  2.1399\n",
            "      4        \u001b[36m0.0919\u001b[0m        \u001b[32m0.1183\u001b[0m  2.1444\n",
            "      5        0.1041        \u001b[32m0.0988\u001b[0m  2.1375\n",
            "      6        \u001b[36m0.0910\u001b[0m        \u001b[32m0.0384\u001b[0m  2.4686\n",
            "      7        \u001b[36m0.0521\u001b[0m        0.0854  2.7577\n",
            "      8        \u001b[36m0.0463\u001b[0m        0.0703  2.1617\n",
            "      9        0.0643        0.0654  2.1533\n",
            "     10        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0198\u001b[0m  2.1434\n",
            "     11        \u001b[36m0.0170\u001b[0m        0.0760  2.1652\n",
            "     12        0.0188        0.0286  2.4718\n",
            "     13        0.0177        0.0220  2.7204\n",
            "     14        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0081\u001b[0m  2.1532\n",
            "     15        0.0149        0.0191  2.1837\n",
            "     16        0.0150        0.0218  2.1544\n",
            "     17        \u001b[36m0.0102\u001b[0m        0.0098  2.1693\n",
            "     18        0.0102        0.0436  2.5048\n",
            "     19        \u001b[36m0.0095\u001b[0m        0.0734  2.6928\n",
            "     20        0.0113        0.0490  2.1766\n",
            "     21        0.0114        0.0345  2.1435\n",
            "     22        \u001b[36m0.0075\u001b[0m        0.0521  2.2385\n",
            "     23        0.0100        0.0114  2.2090\n",
            "     24        \u001b[36m0.0065\u001b[0m        0.0755  2.5593\n",
            "     25        0.0074        0.1025  2.5480\n",
            "     26        0.0078        0.1346  2.1682\n",
            "     27        0.0070        0.0908  2.2079\n",
            "     28        0.0068        0.0818  2.1690\n",
            "     29        0.0088        0.0976  2.1821\n",
            "     30        0.0076        0.1149  2.7093\n",
            "     31        0.0097        0.0525  2.5339\n",
            "     32        0.0068        0.1128  2.1601\n",
            "     33        0.0107        0.1026  2.1999\n",
            "     34        0.0073        0.0439  2.1672\n",
            "     35        0.0089        0.0258  2.1891\n",
            "     36        \u001b[36m0.0052\u001b[0m        0.0431  2.7702\n",
            "     37        \u001b[36m0.0044\u001b[0m        0.0289  2.3957\n",
            "     38        \u001b[36m0.0038\u001b[0m        0.0761  2.1882\n",
            "     39        0.0072        0.0382  2.1953\n",
            "     40        0.0045        0.0165  2.1884\n",
            "     41        \u001b[36m0.0036\u001b[0m        0.0312  2.3015\n",
            "     42        \u001b[36m0.0033\u001b[0m        0.0275  2.7980\n",
            "     43        0.0047        0.0325  2.2877\n",
            "     44        0.0051        0.0652  2.1670\n",
            "     45        \u001b[36m0.0028\u001b[0m        0.0141  2.1603\n",
            "     46        0.0064        0.1037  2.1658\n",
            "     47        0.0053        0.0330  2.3001\n",
            "     48        0.0030        \u001b[32m0.0070\u001b[0m  2.8449\n",
            "     49        0.0036        \u001b[32m0.0065\u001b[0m  2.2129\n",
            "     50        0.0039        0.0294  2.1285\n",
            "     51        \u001b[36m0.0028\u001b[0m        0.0142  2.1585\n",
            "     52        0.0030        0.0888  2.1715\n",
            "     53        \u001b[36m0.0026\u001b[0m        0.0378  2.4001\n",
            "     54        \u001b[36m0.0021\u001b[0m        0.0151  2.8021\n",
            "     55        0.0022        0.0097  2.1455\n",
            "     56        0.0079        0.1245  2.1637\n",
            "     57        0.0106        0.1052  2.1730\n",
            "     58        0.0025        0.0588  2.1800\n",
            "     59        0.0054        0.0837  2.5483\n",
            "     60        0.0033        0.0672  2.7112\n",
            "     61        0.0100        0.1205  2.1922\n",
            "     62        0.0070        0.0917  2.1941\n",
            "     63        0.0044        0.0425  2.1706\n",
            "     64        0.0058        0.0177  2.1521\n",
            "     65        0.0038        0.0423  2.5174\n",
            "     66        0.0030        0.0350  2.5972\n",
            "     67        0.0025        0.0246  2.1443\n",
            "     68        0.0037        0.0808  2.1798\n",
            "     69        0.0027        0.0461  2.1695\n",
            "     70        0.0033        0.0269  2.1908\n",
            "     71        0.0097        0.0344  2.6429\n",
            "     72        0.0028        0.0184  2.5580\n",
            "     73        0.0023        0.0183  2.1967\n",
            "     74        0.0024        0.0228  2.2266\n",
            "     75        0.0033        0.0940  2.3304\n",
            "     76        0.0036        0.0246  2.1876\n",
            "     77        0.0024        0.0099  2.7862\n",
            "     78        0.0031        0.0096  2.3939\n",
            "     79        0.0022        \u001b[32m0.0062\u001b[0m  2.1704\n",
            "     80        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0040\u001b[0m  2.1931\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.1min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1467\u001b[0m        \u001b[32m0.2003\u001b[0m  2.1604\n",
            "      2        \u001b[36m0.1421\u001b[0m        \u001b[32m0.1768\u001b[0m  2.3829\n",
            "      3        \u001b[36m0.1360\u001b[0m        \u001b[32m0.1535\u001b[0m  2.7999\n",
            "      4        \u001b[36m0.1309\u001b[0m        0.1745  2.1710\n",
            "      5        \u001b[36m0.1155\u001b[0m        \u001b[32m0.1348\u001b[0m  2.2061\n",
            "      6        \u001b[36m0.1022\u001b[0m        \u001b[32m0.1170\u001b[0m  2.1835\n",
            "      7        \u001b[36m0.0703\u001b[0m        \u001b[32m0.0508\u001b[0m  2.1831\n",
            "      8        \u001b[36m0.0443\u001b[0m        0.1643  2.4556\n",
            "      9        0.0891        0.1149  2.6755\n",
            "     10        \u001b[36m0.0343\u001b[0m        \u001b[32m0.0198\u001b[0m  2.1759\n",
            "     11        \u001b[36m0.0214\u001b[0m        0.0291  2.1781\n",
            "     12        \u001b[36m0.0172\u001b[0m        0.0661  2.1484\n",
            "     13        \u001b[36m0.0148\u001b[0m        0.0869  2.1779\n",
            "     14        \u001b[36m0.0141\u001b[0m        0.0844  2.5796\n",
            "     15        \u001b[36m0.0128\u001b[0m        0.1465  2.6403\n",
            "     16        \u001b[36m0.0112\u001b[0m        0.1285  2.1757\n",
            "     17        0.0153        0.1831  2.1644\n",
            "     18        0.0123        0.1457  2.2025\n",
            "     19        \u001b[36m0.0098\u001b[0m        0.1173  2.1715\n",
            "     20        \u001b[36m0.0094\u001b[0m        0.1676  2.6384\n",
            "     21        0.0100        0.1138  2.5641\n",
            "     22        \u001b[36m0.0094\u001b[0m        0.1570  2.1838\n",
            "     23        \u001b[36m0.0089\u001b[0m        0.0882  2.1947\n",
            "     24        \u001b[36m0.0085\u001b[0m        0.1282  2.2004\n",
            "     25        \u001b[36m0.0084\u001b[0m        0.1390  2.1614\n",
            "     26        0.0085        0.1230  2.7308\n",
            "     27        \u001b[36m0.0080\u001b[0m        0.1246  2.4806\n",
            "     28        0.0082        0.1582  2.1750\n",
            "     29        0.0081        0.1434  2.1638\n",
            "     30        \u001b[36m0.0077\u001b[0m        0.1795  2.1819\n",
            "     31        0.0083        0.1894  2.2136\n",
            "     32        0.0082        0.1342  2.7831\n",
            "     33        \u001b[36m0.0073\u001b[0m        0.1378  2.3466\n",
            "     34        0.0074        0.1633  2.1690\n",
            "     35        0.0077        0.1903  2.1651\n",
            "     36        0.0074        0.2104  2.1697\n",
            "     37        0.0076        0.1988  2.3521\n",
            "     38        0.0075        0.1911  2.8127\n",
            "     39        0.0076        0.2085  2.2138\n",
            "     40        0.0075        0.1940  2.1600\n",
            "     41        0.0076        0.1951  2.1730\n",
            "     42        0.0074        0.2100  2.1721\n",
            "     43        0.0076        0.2144  2.3699\n",
            "     44        0.0076        0.2349  2.7373\n",
            "     45        0.0075        0.2183  2.2239\n",
            "     46        0.0074        0.2026  2.1651\n",
            "     47        \u001b[36m0.0073\u001b[0m        0.1946  2.1792\n",
            "     48        0.0073        0.1717  2.1681\n",
            "     49        \u001b[36m0.0070\u001b[0m        0.1575  2.5425\n",
            "     50        \u001b[36m0.0069\u001b[0m        0.1467  2.6309\n",
            "     51        0.0072        0.1530  2.1469\n",
            "     52        \u001b[36m0.0066\u001b[0m        0.1397  2.1608\n",
            "     53        0.0069        0.1046  2.1679\n",
            "     54        \u001b[36m0.0065\u001b[0m        0.0972  2.1809\n",
            "     55        0.0067        0.0935  2.6394\n",
            "     56        \u001b[36m0.0063\u001b[0m        0.1000  2.5210\n",
            "     57        0.0065        0.0981  2.1728\n",
            "     58        0.0063        0.0970  2.1662\n",
            "     59        0.0065        0.0899  2.2064\n",
            "     60        0.0068        0.0767  2.1821\n",
            "     61        \u001b[36m0.0062\u001b[0m        0.0921  2.7405\n",
            "     62        0.0064        0.0652  2.4412\n",
            "     63        0.0062        0.0769  2.1768\n",
            "     64        0.0063        0.0523  2.1661\n",
            "     65        \u001b[36m0.0061\u001b[0m        0.0459  2.1495\n",
            "     66        \u001b[36m0.0058\u001b[0m        0.0308  2.2559\n",
            "     67        0.0062        0.0307  2.7986\n",
            "     68        0.0058        0.0389  2.3642\n",
            "     69        0.0063        0.0567  2.1627\n",
            "     70        0.0060        0.0659  2.1637\n",
            "     71        0.0062        0.0412  2.1440\n",
            "     72        0.0061        0.0389  2.3406\n",
            "     73        0.0060        0.0403  2.8106\n",
            "     74        0.0061        0.0224  2.2405\n",
            "     75        0.0067        0.0250  2.1615\n",
            "     76        0.0060        0.0557  2.1923\n",
            "     77        0.0061        0.0295  2.1928\n",
            "     78        \u001b[36m0.0058\u001b[0m        0.0236  2.3986\n",
            "     79        0.0059        0.0305  2.7524\n",
            "     80        \u001b[36m0.0058\u001b[0m        0.0295  2.1149\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1594\u001b[0m        \u001b[32m0.2152\u001b[0m  2.3522\n",
            "      2        \u001b[36m0.1497\u001b[0m        0.2176  2.3271\n",
            "      3        0.1511        0.2160  2.3651\n",
            "      4        0.1515        \u001b[32m0.2147\u001b[0m  3.0297\n",
            "      5        0.1501        \u001b[32m0.2130\u001b[0m  2.5064\n",
            "      6        0.1499        0.2149  2.3157\n",
            "      7        \u001b[36m0.1490\u001b[0m        \u001b[32m0.2124\u001b[0m  2.3311\n",
            "      8        0.1502        \u001b[32m0.2091\u001b[0m  2.3796\n",
            "      9        \u001b[36m0.1459\u001b[0m        \u001b[32m0.1946\u001b[0m  2.7290\n",
            "     10        \u001b[36m0.1242\u001b[0m        \u001b[32m0.1506\u001b[0m  2.8219\n",
            "     11        \u001b[36m0.1122\u001b[0m        \u001b[32m0.1308\u001b[0m  2.3379\n",
            "     12        \u001b[36m0.0979\u001b[0m        \u001b[32m0.1260\u001b[0m  2.3549\n",
            "     13        \u001b[36m0.0843\u001b[0m        \u001b[32m0.1003\u001b[0m  2.3827\n",
            "     14        \u001b[36m0.0700\u001b[0m        \u001b[32m0.0966\u001b[0m  2.4463\n",
            "     15        0.0851        \u001b[32m0.0929\u001b[0m  3.1095\n",
            "     16        \u001b[36m0.0425\u001b[0m        \u001b[32m0.0809\u001b[0m  2.4435\n",
            "     17        0.0452        0.2542  2.4168\n",
            "     18        0.0477        \u001b[32m0.0244\u001b[0m  2.3810\n",
            "     19        \u001b[36m0.0220\u001b[0m        0.0351  2.3314\n",
            "     20        \u001b[36m0.0213\u001b[0m        \u001b[32m0.0186\u001b[0m  2.8369\n",
            "     21        \u001b[36m0.0204\u001b[0m        0.0238  2.7400\n",
            "     22        \u001b[36m0.0199\u001b[0m        0.0213  2.3635\n",
            "     23        0.0217        0.0295  2.3159\n",
            "     24        \u001b[36m0.0191\u001b[0m        0.0321  2.3560\n",
            "     25        \u001b[36m0.0180\u001b[0m        0.0401  2.4942\n",
            "     26        \u001b[36m0.0153\u001b[0m        0.0479  3.0421\n",
            "     27        0.0190        0.0211  2.3703\n",
            "     28        0.0180        0.0267  2.3643\n",
            "     29        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0120\u001b[0m  2.3664\n",
            "     30        0.0166        0.0553  2.3576\n",
            "     31        \u001b[36m0.0143\u001b[0m        0.0477  2.9246\n",
            "     32        0.0148        0.0560  2.6909\n",
            "     33        \u001b[36m0.0140\u001b[0m        0.0245  2.3255\n",
            "     34        0.0148        0.0771  2.3411\n",
            "     35        0.0163        0.0763  2.3461\n",
            "     36        0.0173        0.0564  2.6113\n",
            "     37        \u001b[36m0.0128\u001b[0m        0.0146  2.9545\n",
            "     38        \u001b[36m0.0125\u001b[0m        0.0391  2.3565\n",
            "     39        \u001b[36m0.0108\u001b[0m        0.0326  2.3737\n",
            "     40        0.0117        0.0890  2.3380\n",
            "     41        0.0135        0.1174  2.3756\n",
            "     42        0.0161        0.0833  3.0540\n",
            "     43        0.0119        0.1030  2.5774\n",
            "     44        0.0127        0.1046  2.3605\n",
            "     45        \u001b[36m0.0091\u001b[0m        0.0991  2.3519\n",
            "     46        0.0133        0.1065  2.3644\n",
            "     47        0.0155        0.1140  2.7341\n",
            "     48        0.0095        0.0991  2.8530\n",
            "     49        0.0144        0.0612  2.3431\n",
            "     50        0.0095        0.0559  2.3417\n",
            "     51        0.0115        0.0598  2.3850\n",
            "     52        0.0123        0.0620  2.4170\n",
            "     53        0.0105        0.0742  3.0992\n",
            "     54        \u001b[36m0.0082\u001b[0m        0.0594  2.4458\n",
            "     55        0.0083        0.0571  2.3467\n",
            "     56        \u001b[36m0.0080\u001b[0m        0.0309  2.3342\n",
            "     57        \u001b[36m0.0079\u001b[0m        0.0443  2.3219\n",
            "     58        0.0083        0.0723  2.7311\n",
            "     59        0.0113        0.0878  2.8471\n",
            "     60        0.0081        0.0646  2.3359\n",
            "     61        0.0105        0.0614  2.3029\n",
            "     62        0.0079        0.0672  2.4009\n",
            "     63        0.0096        0.0768  2.4922\n",
            "     64        \u001b[36m0.0067\u001b[0m        0.0908  3.1048\n",
            "     65        0.0069        0.0784  2.4129\n",
            "     66        \u001b[36m0.0062\u001b[0m        0.1135  2.3502\n",
            "     67        0.0063        0.1063  2.3447\n",
            "     68        0.0067        0.1399  2.3925\n",
            "     69        0.0181        0.1137  2.8741\n",
            "     70        \u001b[36m0.0061\u001b[0m        0.1208  2.7169\n",
            "     71        \u001b[36m0.0052\u001b[0m        0.1088  2.4184\n",
            "     72        0.0056        0.1250  2.3269\n",
            "     73        0.0063        0.1258  2.3823\n",
            "     74        \u001b[36m0.0047\u001b[0m        0.1135  2.5667\n",
            "     75        0.0052        0.1746  3.0101\n",
            "     76        0.0071        0.1970  2.3172\n",
            "     77        0.0058        0.1279  2.3833\n",
            "     78        \u001b[36m0.0045\u001b[0m        0.1268  2.3078\n",
            "     79        0.0057        0.1973  2.3419\n",
            "     80        0.0056        0.1106  2.9963\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.4min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1548\u001b[0m        \u001b[32m0.2099\u001b[0m  2.4393\n",
            "      2        \u001b[36m0.1493\u001b[0m        0.2103  2.3308\n",
            "      3        0.1500        0.2119  2.3393\n",
            "      4        0.1495        0.2101  2.3361\n",
            "      5        \u001b[36m0.1493\u001b[0m        \u001b[32m0.2071\u001b[0m  2.7565\n",
            "      6        \u001b[36m0.1489\u001b[0m        0.2167  2.7864\n",
            "      7        0.1511        0.2194  2.3274\n",
            "      8        0.1516        0.2203  2.3001\n",
            "      9        0.1520        0.2195  2.3421\n",
            "     10        0.1512        0.2199  2.4682\n",
            "     11        0.1514        0.2200  3.1227\n",
            "     12        0.1525        0.2192  2.4197\n",
            "     13        0.1525        0.2186  2.3496\n",
            "     14        0.1505        0.2205  2.3658\n",
            "     15        0.1531        0.2199  2.3757\n",
            "     16        0.1507        0.2203  2.8087\n",
            "     17        0.1519        0.2192  2.7351\n",
            "     18        0.1512        0.2195  2.3484\n",
            "     19        0.1517        0.2204  2.3354\n",
            "     20        0.1548        0.2193  2.3504\n",
            "     21        0.1509        0.2206  2.6178\n",
            "     22        0.1506        0.2200  3.0223\n",
            "     23        0.1510        0.2199  2.3615\n",
            "     24        0.1523        0.2204  2.3629\n",
            "     25        0.1560        0.2198  2.3545\n",
            "     26        0.1525        0.2195  2.3669\n",
            "     27        0.1526        0.2211  2.9531\n",
            "     28        0.1510        0.2198  2.5774\n",
            "     29        0.1500        0.2202  2.3443\n",
            "     30        0.1511        0.2210  2.3586\n",
            "     31        0.1516        0.2236  2.3678\n",
            "     32        0.1520        0.2203  2.6663\n",
            "     33        0.1501        0.2218  3.0021\n",
            "     34        0.1512        0.2207  2.3712\n",
            "     35        0.1510        0.2208  2.3686\n",
            "     36        0.1515        0.2230  2.3058\n",
            "     37        0.1498        0.2216  2.3626\n",
            "     38        0.1507        0.2211  3.0794\n",
            "     39        0.1520        0.2241  2.4898\n",
            "     40        0.1511        0.2220  2.3469\n",
            "     41        0.1521        0.2240  2.3363\n",
            "     42        0.1512        0.2208  2.3132\n",
            "     43        0.1518        0.2241  2.7189\n",
            "     44        0.1518        0.2242  2.8837\n",
            "     45        0.1520        0.2238  2.3767\n",
            "     46        0.1514        0.2234  2.3494\n",
            "     47        0.1521        0.2221  2.3339\n",
            "     48        0.1513        0.2206  2.5178\n",
            "     49        0.1508        0.2244  3.1171\n",
            "     50        0.1523        0.2235  2.3629\n",
            "     51        0.1509        0.2212  2.3350\n",
            "     52        0.1513        0.2232  2.3707\n",
            "     53        0.1507        0.2216  2.3717\n",
            "     54        0.1515        0.2216  2.8700\n",
            "     55        0.1518        0.2251  2.7117\n",
            "     56        0.1524        0.2252  2.3398\n",
            "     57        0.1512        0.2261  2.3910\n",
            "     58        0.1507        0.2216  2.3457\n",
            "     59        0.1509        0.2231  2.6183\n",
            "     60        0.1506        0.2210  3.0196\n",
            "     61        0.1506        0.2250  2.3514\n",
            "     62        0.1509        0.2212  2.3737\n",
            "     63        0.1510        0.2233  2.3558\n",
            "     64        0.1519        0.2228  2.3419\n",
            "     65        0.1539        0.2224  2.9640\n",
            "     66        0.1524        0.2241  2.6046\n",
            "     67        0.1521        0.2230  2.3646\n",
            "     68        0.1520        0.2241  2.3475\n",
            "     69        0.1513        0.2202  2.3899\n",
            "     70        0.1512        0.2249  2.6693\n",
            "     71        0.1521        0.2232  2.9076\n",
            "     72        0.1513        0.2236  2.3612\n",
            "     73        0.1508        0.2212  2.3542\n",
            "     74        0.1509        0.2210  2.3369\n",
            "     75        0.1516        0.2204  2.3585\n",
            "     76        0.1507        0.2217  3.1366\n",
            "     77        0.1518        0.2255  2.5682\n",
            "     78        0.1514        0.2259  2.3949\n",
            "     79        0.1505        0.2203  2.3852\n",
            "     80        0.1505        0.2235  2.3922\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1556\u001b[0m        \u001b[32m0.2426\u001b[0m  2.8400\n",
            "      2        \u001b[36m0.1511\u001b[0m        \u001b[32m0.2336\u001b[0m  2.6019\n",
            "      3        \u001b[36m0.1460\u001b[0m        \u001b[32m0.1973\u001b[0m  2.1894\n",
            "      4        \u001b[36m0.1253\u001b[0m        \u001b[32m0.1761\u001b[0m  2.1555\n",
            "      5        \u001b[36m0.1072\u001b[0m        \u001b[32m0.1323\u001b[0m  2.1558\n",
            "      6        \u001b[36m0.1022\u001b[0m        \u001b[32m0.0841\u001b[0m  2.1830\n",
            "      7        \u001b[36m0.0940\u001b[0m        0.1119  2.7423\n",
            "      8        \u001b[36m0.0704\u001b[0m        \u001b[32m0.0622\u001b[0m  2.4143\n",
            "      9        0.0792        \u001b[32m0.0602\u001b[0m  2.1757\n",
            "     10        \u001b[36m0.0505\u001b[0m        0.1018  2.1461\n",
            "     11        \u001b[36m0.0455\u001b[0m        \u001b[32m0.0436\u001b[0m  2.1590\n",
            "     12        \u001b[36m0.0225\u001b[0m        \u001b[32m0.0163\u001b[0m  2.2961\n",
            "     13        \u001b[36m0.0197\u001b[0m        0.0302  2.8055\n",
            "     14        \u001b[36m0.0171\u001b[0m        0.0179  2.2970\n",
            "     15        0.0296        0.0934  2.1599\n",
            "     16        0.0255        0.0820  2.1536\n",
            "     17        \u001b[36m0.0164\u001b[0m        0.0910  2.1984\n",
            "     18        0.0172        0.0496  2.3561\n",
            "     19        \u001b[36m0.0140\u001b[0m        0.0475  2.8152\n",
            "     20        0.0151        0.0400  2.1518\n",
            "     21        0.0152        0.0215  2.1701\n",
            "     22        \u001b[36m0.0113\u001b[0m        0.0472  2.1790\n",
            "     23        0.0126        0.0573  2.1701\n",
            "     24        0.0126        0.0531  2.4714\n",
            "     25        0.0134        0.0498  2.7512\n",
            "     26        0.0122        0.0435  2.1713\n",
            "     27        0.0181        0.0519  2.2057\n",
            "     28        \u001b[36m0.0109\u001b[0m        0.0453  2.1655\n",
            "     29        0.0147        0.0490  2.2043\n",
            "     30        0.0181        \u001b[32m0.0156\u001b[0m  2.5507\n",
            "     31        \u001b[36m0.0101\u001b[0m        0.0449  2.6920\n",
            "     32        0.0145        0.0315  2.2137\n",
            "     33        0.0124        0.0326  2.2054\n",
            "     34        0.0175        0.0173  2.2014\n",
            "     35        0.0126        0.0386  2.2319\n",
            "     36        0.0124        0.0451  2.7295\n",
            "     37        \u001b[36m0.0090\u001b[0m        0.0368  2.5725\n",
            "     38        \u001b[36m0.0077\u001b[0m        0.0366  2.2026\n",
            "     39        \u001b[36m0.0065\u001b[0m        0.0649  2.1698\n",
            "     40        0.0114        \u001b[32m0.0129\u001b[0m  2.2340\n",
            "     41        0.0065        0.0287  2.1711\n",
            "     42        0.0066        0.1269  2.7149\n",
            "     43        0.0105        0.0501  2.4976\n",
            "     44        0.0113        0.0335  2.1778\n",
            "     45        0.0073        0.0429  2.1711\n",
            "     46        \u001b[36m0.0060\u001b[0m        0.0577  2.1819\n",
            "     47        \u001b[36m0.0060\u001b[0m        0.0668  2.2512\n",
            "     48        0.0061        0.0439  2.8465\n",
            "     49        \u001b[36m0.0047\u001b[0m        \u001b[32m0.0113\u001b[0m  2.3133\n",
            "     50        \u001b[36m0.0033\u001b[0m        0.0254  2.1773\n",
            "     51        \u001b[36m0.0033\u001b[0m        \u001b[32m0.0096\u001b[0m  2.1809\n",
            "     52        0.0038        0.1048  2.1740\n",
            "     53        0.0036        \u001b[32m0.0065\u001b[0m  2.4408\n",
            "     54        0.0037        0.0243  2.8727\n",
            "     55        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0051\u001b[0m  2.2320\n",
            "     56        0.0051        0.0246  2.1884\n",
            "     57        0.0021        0.0070  2.1437\n",
            "     58        \u001b[36m0.0009\u001b[0m        \u001b[32m0.0031\u001b[0m  2.2000\n",
            "     59        0.0050        0.0260  2.4395\n",
            "     60        0.0041        0.0517  2.8026\n",
            "     61        0.0025        0.0076  2.1465\n",
            "     62        0.0022        0.0713  2.1720\n",
            "     63        0.0043        0.0146  2.1467\n",
            "     64        0.0010        0.0077  2.1853\n",
            "     65        0.0018        \u001b[32m0.0029\u001b[0m  2.4941\n",
            "     66        \u001b[36m0.0009\u001b[0m        \u001b[32m0.0011\u001b[0m  2.7516\n",
            "     67        0.0020        0.0153  2.1905\n",
            "     68        0.0059        0.0395  2.1799\n",
            "     69        0.0018        0.0013  2.1841\n",
            "     70        \u001b[36m0.0006\u001b[0m        0.0028  2.1518\n",
            "     71        0.0076        0.0627  2.5402\n",
            "     72        0.0048        0.0682  2.6308\n",
            "     73        0.0020        0.0053  2.1534\n",
            "     74        0.0014        0.0058  2.1837\n",
            "     75        0.0054        0.1909  2.2052\n",
            "     76        0.0086        0.1148  2.1827\n",
            "     77        0.0046        0.0692  2.6354\n",
            "     78        0.0063        0.0336  2.6234\n",
            "     79        0.0068        0.0471  2.1937\n",
            "     80        0.0036        0.2558  2.2348\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.1min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1548\u001b[0m        \u001b[32m0.2776\u001b[0m  2.2022\n",
            "      2        0.1553        \u001b[32m0.2626\u001b[0m  2.2698\n",
            "      3        \u001b[36m0.1499\u001b[0m        \u001b[32m0.2615\u001b[0m  2.8015\n",
            "      4        \u001b[36m0.1434\u001b[0m        \u001b[32m0.2246\u001b[0m  2.2944\n",
            "      5        \u001b[36m0.1318\u001b[0m        \u001b[32m0.2184\u001b[0m  2.2029\n",
            "      6        \u001b[36m0.1192\u001b[0m        \u001b[32m0.1345\u001b[0m  2.1611\n",
            "      7        \u001b[36m0.1077\u001b[0m        \u001b[32m0.0831\u001b[0m  2.1940\n",
            "      8        \u001b[36m0.0986\u001b[0m        0.1419  2.3696\n",
            "      9        \u001b[36m0.0963\u001b[0m        0.1198  2.8306\n",
            "     10        \u001b[36m0.0940\u001b[0m        0.1077  2.1830\n",
            "     11        \u001b[36m0.0792\u001b[0m        \u001b[32m0.0634\u001b[0m  2.2088\n",
            "     12        \u001b[36m0.0615\u001b[0m        0.0822  2.1666\n",
            "     13        \u001b[36m0.0337\u001b[0m        0.0718  2.1843\n",
            "     14        \u001b[36m0.0335\u001b[0m        \u001b[32m0.0366\u001b[0m  2.5155\n",
            "     15        \u001b[36m0.0232\u001b[0m        0.0474  2.7302\n",
            "     16        \u001b[36m0.0156\u001b[0m        0.0670  2.1594\n",
            "     17        \u001b[36m0.0143\u001b[0m        0.0708  2.1697\n",
            "     18        \u001b[36m0.0128\u001b[0m        0.1226  2.1858\n",
            "     19        \u001b[36m0.0128\u001b[0m        0.1025  2.2225\n",
            "     20        \u001b[36m0.0115\u001b[0m        0.0724  2.6175\n",
            "     21        \u001b[36m0.0107\u001b[0m        0.1868  2.5614\n",
            "     22        0.0113        0.3010  2.1776\n",
            "     23        0.0123        0.1956  2.2081\n",
            "     24        \u001b[36m0.0098\u001b[0m        0.1357  2.1988\n",
            "     25        \u001b[36m0.0095\u001b[0m        0.1416  2.1820\n",
            "     26        \u001b[36m0.0090\u001b[0m        0.1252  2.7083\n",
            "     27        \u001b[36m0.0090\u001b[0m        0.1721  2.4745\n",
            "     28        0.0091        0.1777  2.1555\n",
            "     29        \u001b[36m0.0089\u001b[0m        0.1801  2.1735\n",
            "     30        \u001b[36m0.0085\u001b[0m        0.1851  2.1907\n",
            "     31        \u001b[36m0.0085\u001b[0m        0.1873  2.2317\n",
            "     32        \u001b[36m0.0083\u001b[0m        0.1617  2.8191\n",
            "     33        \u001b[36m0.0079\u001b[0m        0.1915  2.3053\n",
            "     34        0.0079        0.1950  2.1613\n",
            "     35        0.0082        0.1825  2.1881\n",
            "     36        0.0082        0.2185  2.1930\n",
            "     37        0.0081        0.2274  2.3276\n",
            "     38        \u001b[36m0.0076\u001b[0m        0.2090  2.8289\n",
            "     39        0.0077        0.2172  2.3106\n",
            "     40        \u001b[36m0.0076\u001b[0m        0.2111  2.1581\n",
            "     41        \u001b[36m0.0075\u001b[0m        0.1615  2.2084\n",
            "     42        \u001b[36m0.0074\u001b[0m        0.1819  2.1524\n",
            "     43        \u001b[36m0.0071\u001b[0m        0.1373  2.4859\n",
            "     44        0.0071        0.1439  2.6799\n",
            "     45        \u001b[36m0.0070\u001b[0m        0.1441  2.1750\n",
            "     46        0.0072        0.1355  2.2217\n",
            "     47        \u001b[36m0.0069\u001b[0m        0.1231  2.1535\n",
            "     48        0.0070        0.1188  2.1896\n",
            "     49        0.0069        0.1182  2.5796\n",
            "     50        \u001b[36m0.0068\u001b[0m        0.1061  2.6475\n",
            "     51        0.0071        0.1013  2.2129\n",
            "     52        0.0068        0.1056  2.1902\n",
            "     53        \u001b[36m0.0066\u001b[0m        0.0876  2.1876\n",
            "     54        0.0068        0.0974  2.1694\n",
            "     55        0.0066        0.0677  2.6806\n",
            "     56        \u001b[36m0.0065\u001b[0m        0.0577  2.5228\n",
            "     57        \u001b[36m0.0063\u001b[0m        0.0531  2.1992\n",
            "     58        0.0065        0.0454  2.1749\n",
            "     59        0.0064        0.0391  2.2288\n",
            "     60        0.0064        \u001b[32m0.0317\u001b[0m  2.2156\n",
            "     61        0.0067        \u001b[32m0.0242\u001b[0m  2.8026\n",
            "     62        0.0067        0.0244  2.3350\n",
            "     63        \u001b[36m0.0063\u001b[0m        \u001b[32m0.0210\u001b[0m  2.2317\n",
            "     64        0.0066        0.0268  2.1945\n",
            "     65        0.0066        \u001b[32m0.0185\u001b[0m  2.1815\n",
            "     66        0.0069        0.0231  2.3663\n",
            "     67        0.0066        0.0237  2.8408\n",
            "     68        0.0064        0.0249  2.2083\n",
            "     69        0.0067        0.0264  2.1758\n",
            "     70        0.0063        0.0188  2.2119\n",
            "     71        0.0065        \u001b[32m0.0183\u001b[0m  2.2143\n",
            "     72        0.0068        0.0206  2.4669\n",
            "     73        0.0063        0.0320  2.7009\n",
            "     74        \u001b[36m0.0061\u001b[0m        0.0213  2.1877\n",
            "     75        0.0064        0.0202  2.1980\n",
            "     76        0.0061        0.0325  2.2136\n",
            "     77        0.0065        0.0213  2.1751\n",
            "     78        0.0068        \u001b[32m0.0172\u001b[0m  2.5852\n",
            "     79        0.0066        0.0191  2.6246\n",
            "     80        0.0067        0.0360  2.1769\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1608\u001b[0m        \u001b[32m0.2151\u001b[0m  2.3678\n",
            "      2        \u001b[36m0.1503\u001b[0m        0.2163  2.3339\n",
            "      3        0.1517        0.2175  2.5263\n",
            "      4        0.1514        0.2187  3.0586\n",
            "      5        0.1504        0.2173  2.3984\n",
            "      6        0.1504        0.2159  2.4236\n",
            "      7        \u001b[36m0.1496\u001b[0m        \u001b[32m0.2082\u001b[0m  2.3940\n",
            "      8        0.1514        0.2112  2.4184\n",
            "      9        \u001b[36m0.1495\u001b[0m        \u001b[32m0.2034\u001b[0m  2.9877\n",
            "     10        \u001b[36m0.1478\u001b[0m        0.2061  2.6150\n",
            "     11        \u001b[36m0.1423\u001b[0m        \u001b[32m0.1890\u001b[0m  2.3625\n",
            "     12        \u001b[36m0.1239\u001b[0m        \u001b[32m0.1561\u001b[0m  2.3609\n",
            "     13        \u001b[36m0.1192\u001b[0m        \u001b[32m0.1436\u001b[0m  2.3569\n",
            "     14        \u001b[36m0.1139\u001b[0m        \u001b[32m0.1232\u001b[0m  2.7556\n",
            "     15        \u001b[36m0.1007\u001b[0m        \u001b[32m0.0887\u001b[0m  2.8547\n",
            "     16        \u001b[36m0.0592\u001b[0m        \u001b[32m0.0683\u001b[0m  2.3666\n",
            "     17        \u001b[36m0.0370\u001b[0m        \u001b[32m0.0233\u001b[0m  2.3534\n",
            "     18        \u001b[36m0.0301\u001b[0m        \u001b[32m0.0210\u001b[0m  2.3676\n",
            "     19        \u001b[36m0.0269\u001b[0m        \u001b[32m0.0206\u001b[0m  2.4969\n",
            "     20        \u001b[36m0.0253\u001b[0m        \u001b[32m0.0173\u001b[0m  3.0912\n",
            "     21        \u001b[36m0.0242\u001b[0m        \u001b[32m0.0166\u001b[0m  2.3757\n",
            "     22        \u001b[36m0.0214\u001b[0m        \u001b[32m0.0142\u001b[0m  2.3472\n",
            "     23        0.0219        0.0179  2.3702\n",
            "     24        \u001b[36m0.0202\u001b[0m        0.0226  2.3455\n",
            "     25        \u001b[36m0.0185\u001b[0m        0.0201  2.9315\n",
            "     26        0.0191        0.0169  2.7108\n",
            "     27        \u001b[36m0.0179\u001b[0m        \u001b[32m0.0136\u001b[0m  2.3874\n",
            "     28        \u001b[36m0.0150\u001b[0m        0.0294  2.3645\n",
            "     29        0.0159        0.0271  2.3458\n",
            "     30        0.0169        0.0292  2.6397\n",
            "     31        \u001b[36m0.0149\u001b[0m        0.0476  2.9894\n",
            "     32        0.0162        0.0277  2.3946\n",
            "     33        \u001b[36m0.0129\u001b[0m        0.0524  2.3492\n",
            "     34        0.0155        0.0788  2.3581\n",
            "     35        \u001b[36m0.0128\u001b[0m        0.0438  2.4006\n",
            "     36        0.0137        0.0445  3.0785\n",
            "     37        0.0147        0.0546  2.5617\n",
            "     38        0.0132        0.0628  2.3625\n",
            "     39        \u001b[36m0.0122\u001b[0m        0.0523  2.3812\n",
            "     40        0.0123        0.0547  2.3931\n",
            "     41        \u001b[36m0.0110\u001b[0m        0.0505  2.7590\n",
            "     42        0.0117        0.0616  2.8195\n",
            "     43        0.0142        0.0575  2.3343\n",
            "     44        \u001b[36m0.0098\u001b[0m        0.0578  2.3428\n",
            "     45        0.0127        0.0470  2.3940\n",
            "     46        0.0112        0.0760  2.4637\n",
            "     47        0.0104        0.0570  3.1100\n",
            "     48        0.0107        0.0633  2.4255\n",
            "     49        0.0113        0.0779  2.3539\n",
            "     50        0.0177        0.0706  2.4065\n",
            "     51        0.0113        0.0529  2.3587\n",
            "     52        0.0106        0.0554  2.8034\n",
            "     53        0.0135        0.0747  2.7583\n",
            "     54        0.0112        0.0686  2.3736\n",
            "     55        0.0108        0.0682  2.4041\n",
            "     56        0.0106        0.0858  2.4043\n",
            "     57        0.0103        0.0923  2.6220\n",
            "     58        0.0101        0.0782  3.0212\n",
            "     59        0.0121        0.1039  2.3769\n",
            "     60        0.0136        0.0669  2.3352\n",
            "     61        \u001b[36m0.0090\u001b[0m        0.0731  2.3804\n",
            "     62        \u001b[36m0.0084\u001b[0m        0.0671  2.3458\n",
            "     63        0.0114        0.0783  3.0164\n",
            "     64        0.0094        0.1028  2.6387\n",
            "     65        0.0116        0.1193  2.3903\n",
            "     66        0.0139        0.0915  2.3562\n",
            "     67        0.0094        0.0955  2.3801\n",
            "     68        \u001b[36m0.0079\u001b[0m        0.0670  2.7866\n",
            "     69        0.0083        0.0546  2.9262\n",
            "     70        0.0095        0.0836  2.3463\n",
            "     71        0.0107        0.0883  2.3959\n",
            "     72        0.0088        0.0673  2.3712\n",
            "     73        0.0080        0.0637  2.4281\n",
            "     74        0.0093        0.0918  3.1151\n",
            "     75        0.0088        0.1084  2.5335\n",
            "     76        0.0081        0.0591  2.3335\n",
            "     77        \u001b[36m0.0075\u001b[0m        0.0651  2.3643\n",
            "     78        0.0105        0.0705  2.4465\n",
            "     79        0.0089        0.0711  2.8153\n",
            "     80        \u001b[36m0.0073\u001b[0m        0.0640  2.7933\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.4min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1646\u001b[0m        \u001b[32m0.2197\u001b[0m  2.3477\n",
            "      2        \u001b[36m0.1517\u001b[0m        \u001b[32m0.2191\u001b[0m  2.3524\n",
            "      3        \u001b[36m0.1517\u001b[0m        0.2205  2.3830\n",
            "      4        \u001b[36m0.1515\u001b[0m        0.2194  2.6801\n",
            "      5        0.1527        0.2220  2.8886\n",
            "      6        0.1525        0.2209  2.3675\n",
            "      7        0.1532        \u001b[32m0.2185\u001b[0m  2.3655\n",
            "      8        0.1524        0.2201  2.3851\n",
            "      9        0.1549        0.2259  2.4487\n",
            "     10        0.1549        0.2215  3.0555\n",
            "     11        0.1536        0.2299  2.4979\n",
            "     12        0.1551        0.2257  2.4356\n",
            "     13        0.1536        0.2210  2.3779\n",
            "     14        0.1531        0.2191  2.3598\n",
            "     15        0.1524        0.2209  2.8498\n",
            "     16        0.1533        0.2211  2.8358\n",
            "     17        0.1518        0.2200  2.3883\n",
            "     18        0.1537        0.2203  2.3840\n",
            "     19        0.1529        0.2251  2.3343\n",
            "     20        0.1536        0.2214  2.5556\n",
            "     21        0.1522        0.2206  3.1167\n",
            "     22        0.1530        0.2229  2.4212\n",
            "     23        0.1542        0.2216  2.3739\n",
            "     24        0.1531        0.2219  2.3756\n",
            "     25        0.1527        0.2226  2.3863\n",
            "     26        0.1536        0.2225  3.1210\n",
            "     27        0.1532        0.2219  2.7230\n",
            "     28        0.1531        0.2205  2.3836\n",
            "     29        0.1540        0.2199  2.3394\n",
            "     30        0.1535        0.2211  2.3698\n",
            "     31        0.1536        0.2207  2.7373\n",
            "     32        0.1537        0.2211  2.9446\n",
            "     33        0.1531        0.2208  2.3758\n",
            "     34        0.1534        0.2215  2.3821\n",
            "     35        0.1520        0.2202  2.4298\n",
            "     36        0.1525        0.2211  2.4940\n",
            "     37        0.1543        0.2227  3.1037\n",
            "     38        0.1519        0.2207  2.4687\n",
            "     39        0.1533        0.2208  2.4398\n",
            "     40        0.1523        0.2205  2.3427\n",
            "     41        0.1529        0.2198  2.4041\n",
            "     42        0.1534        0.2208  2.8725\n",
            "     43        0.1545        0.2212  2.7530\n",
            "     44        0.1550        0.2221  2.3233\n",
            "     45        0.1525        0.2216  2.3520\n",
            "     46        0.1541        0.2202  2.4308\n",
            "     47        0.1537        0.2202  2.6534\n",
            "     48        0.1522        0.2193  3.0681\n",
            "     49        0.1525        0.2211  2.3982\n",
            "     50        0.1534        0.2215  2.4162\n",
            "     51        0.1542        0.2211  2.4018\n",
            "     52        0.1524        0.2220  2.3857\n",
            "     53        \u001b[36m0.1511\u001b[0m        0.2205  3.0702\n",
            "     54        0.1540        0.2207  2.5269\n",
            "     55        0.1519        0.2228  2.3605\n",
            "     56        \u001b[36m0.1505\u001b[0m        0.2204  2.3626\n",
            "     57        0.1516        0.2214  2.3598\n",
            "     58        0.1518        0.2216  2.7455\n",
            "     59        0.1507        0.2213  2.8407\n",
            "     60        0.1536        0.2206  2.3733\n",
            "     61        0.1532        0.2214  2.3211\n",
            "     62        0.1522        0.2200  2.3299\n",
            "     63        \u001b[36m0.1505\u001b[0m        0.2205  2.3869\n",
            "     64        0.1512        0.2217  3.0453\n",
            "     65        0.1510        0.2213  2.4901\n",
            "     66        \u001b[36m0.1503\u001b[0m        0.2202  2.3344\n",
            "     67        \u001b[36m0.1490\u001b[0m        0.2203  2.3266\n",
            "     68        0.1500        0.2203  2.3628\n",
            "     69        0.1497        0.2206  2.8138\n",
            "     70        0.1491        0.2213  2.8125\n",
            "     71        0.1492        0.2201  2.3949\n",
            "     72        0.1507        0.2207  2.3420\n",
            "     73        \u001b[36m0.1489\u001b[0m        0.2213  2.3539\n",
            "     74        0.1503        0.2213  2.5239\n",
            "     75        0.1534        0.2240  3.1451\n",
            "     76        \u001b[36m0.1483\u001b[0m        0.2221  2.3700\n",
            "     77        0.1532        0.2208  2.3435\n",
            "     78        0.1527        0.2210  2.3568\n",
            "     79        0.1528        0.2207  2.3290\n",
            "     80        0.1530        0.2199  2.8709\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1506\u001b[0m        \u001b[32m0.1994\u001b[0m  1.4632\n",
            "      2        \u001b[36m0.1269\u001b[0m        \u001b[32m0.1691\u001b[0m  1.1674\n",
            "      3        \u001b[36m0.1217\u001b[0m        \u001b[32m0.1073\u001b[0m  1.1574\n",
            "      4        \u001b[36m0.1057\u001b[0m        \u001b[32m0.1022\u001b[0m  1.1623\n",
            "      5        \u001b[36m0.0943\u001b[0m        0.1302  1.1667\n",
            "      6        0.0956        \u001b[32m0.1003\u001b[0m  1.1513\n",
            "      7        \u001b[36m0.0839\u001b[0m        0.1476  1.1586\n",
            "      8        0.1041        \u001b[32m0.0366\u001b[0m  1.1600\n",
            "      9        \u001b[36m0.0733\u001b[0m        0.0432  1.1621\n",
            "     10        \u001b[36m0.0596\u001b[0m        \u001b[32m0.0206\u001b[0m  1.5166\n",
            "     11        \u001b[36m0.0242\u001b[0m        0.0470  1.5460\n",
            "     12        \u001b[36m0.0201\u001b[0m        0.0459  1.3444\n",
            "     13        \u001b[36m0.0166\u001b[0m        0.0534  1.1485\n",
            "     14        \u001b[36m0.0160\u001b[0m        0.0590  1.1676\n",
            "     15        \u001b[36m0.0143\u001b[0m        0.0337  1.1660\n",
            "     16        \u001b[36m0.0126\u001b[0m        0.0291  1.1498\n",
            "     17        \u001b[36m0.0122\u001b[0m        0.0312  1.1913\n",
            "     18        \u001b[36m0.0113\u001b[0m        0.0294  1.1658\n",
            "     19        \u001b[36m0.0110\u001b[0m        0.0337  1.1513\n",
            "     20        \u001b[36m0.0107\u001b[0m        0.0326  1.1685\n",
            "     21        \u001b[36m0.0096\u001b[0m        0.0394  1.4957\n",
            "     22        \u001b[36m0.0093\u001b[0m        0.0427  1.5094\n",
            "     23        \u001b[36m0.0088\u001b[0m        0.0418  1.3404\n",
            "     24        0.0089        0.0410  1.1576\n",
            "     25        \u001b[36m0.0087\u001b[0m        0.0409  1.1508\n",
            "     26        \u001b[36m0.0079\u001b[0m        0.0400  1.1710\n",
            "     27        \u001b[36m0.0074\u001b[0m        0.0371  1.1468\n",
            "     28        \u001b[36m0.0072\u001b[0m        0.0315  1.1653\n",
            "     29        \u001b[36m0.0070\u001b[0m        0.0278  1.1460\n",
            "     30        \u001b[36m0.0064\u001b[0m        0.0219  1.1619\n",
            "     31        \u001b[36m0.0061\u001b[0m        \u001b[32m0.0178\u001b[0m  1.1646\n",
            "     32        \u001b[36m0.0056\u001b[0m        \u001b[32m0.0172\u001b[0m  1.4658\n",
            "     33        \u001b[36m0.0051\u001b[0m        \u001b[32m0.0160\u001b[0m  1.4845\n",
            "     34        \u001b[36m0.0047\u001b[0m        0.0170  1.3324\n",
            "     35        \u001b[36m0.0047\u001b[0m        0.0229  1.1478\n",
            "     36        0.0051        \u001b[32m0.0122\u001b[0m  1.1687\n",
            "     37        \u001b[36m0.0040\u001b[0m        \u001b[32m0.0103\u001b[0m  1.1602\n",
            "     38        0.0041        0.0384  1.1817\n",
            "     39        0.0045        0.0184  1.1794\n",
            "     40        0.0050        \u001b[32m0.0049\u001b[0m  1.1897\n",
            "     41        \u001b[36m0.0034\u001b[0m        0.0111  1.1437\n",
            "     42        \u001b[36m0.0032\u001b[0m        0.0121  1.1527\n",
            "     43        \u001b[36m0.0030\u001b[0m        0.0171  1.5311\n",
            "     44        0.0030        0.0264  1.5206\n",
            "     45        0.0030        0.0316  1.2774\n",
            "     46        0.0036        0.0284  1.1675\n",
            "     47        0.0046        0.0262  1.1669\n",
            "     48        0.0046        0.0139  1.1408\n",
            "     49        0.0036        0.0195  1.1601\n",
            "     50        0.0036        0.0185  1.1694\n",
            "     51        0.0034        0.0180  1.1528\n",
            "     52        0.0032        0.0198  1.1488\n",
            "     53        0.0031        0.0244  1.2720\n",
            "     54        \u001b[36m0.0029\u001b[0m        0.0279  1.4933\n",
            "     55        \u001b[36m0.0026\u001b[0m        0.0266  1.5238\n",
            "     56        \u001b[36m0.0022\u001b[0m        0.0330  1.2549\n",
            "     57        0.0026        0.0435  1.1803\n",
            "     58        0.0033        0.0399  1.1589\n",
            "     59        0.0035        0.0458  1.1512\n",
            "     60        0.0035        0.0496  1.1494\n",
            "     61        0.0038        0.0459  1.1392\n",
            "     62        0.0040        0.0266  1.1660\n",
            "     63        0.0028        0.0258  1.1420\n",
            "     64        0.0028        0.0468  1.2560\n",
            "     65        0.0048        0.0425  1.5049\n",
            "     66        0.0049        0.0298  1.5768\n",
            "     67        0.0041        0.0427  1.2827\n",
            "     68        0.0036        0.0260  1.1483\n",
            "     69        0.0033        0.0342  1.1464\n",
            "     70        0.0043        0.0463  1.1512\n",
            "     71        0.0025        0.0372  1.1806\n",
            "     72        0.0040        0.0319  1.1478\n",
            "     73        0.0026        0.0152  1.1470\n",
            "     74        0.0029        0.0302  1.1549\n",
            "     75        0.0024        0.0249  1.2144\n",
            "     76        0.0025        0.0301  1.5462\n",
            "     77        0.0039        0.0554  1.5292\n",
            "     78        0.0072        0.0296  1.2942\n",
            "     79        0.0048        0.0167  1.2053\n",
            "     80        0.0037        0.0178  1.1550\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.7min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1603\u001b[0m        \u001b[32m0.2257\u001b[0m  1.1473\n",
            "      2        \u001b[36m0.1426\u001b[0m        \u001b[32m0.1910\u001b[0m  1.1582\n",
            "      3        \u001b[36m0.1365\u001b[0m        \u001b[32m0.1771\u001b[0m  1.1509\n",
            "      4        \u001b[36m0.1270\u001b[0m        \u001b[32m0.1601\u001b[0m  1.1519\n",
            "      5        \u001b[36m0.1217\u001b[0m        \u001b[32m0.1584\u001b[0m  1.1602\n",
            "      6        0.1225        \u001b[32m0.0987\u001b[0m  1.2990\n",
            "      7        \u001b[36m0.1106\u001b[0m        0.1097  1.4821\n",
            "      8        \u001b[36m0.1055\u001b[0m        \u001b[32m0.0584\u001b[0m  1.5624\n",
            "      9        \u001b[36m0.0824\u001b[0m        \u001b[32m0.0429\u001b[0m  1.2182\n",
            "     10        \u001b[36m0.0539\u001b[0m        \u001b[32m0.0400\u001b[0m  1.1520\n",
            "     11        \u001b[36m0.0324\u001b[0m        \u001b[32m0.0321\u001b[0m  1.1308\n",
            "     12        \u001b[36m0.0221\u001b[0m        0.0329  1.1533\n",
            "     13        \u001b[36m0.0205\u001b[0m        \u001b[32m0.0199\u001b[0m  1.1543\n",
            "     14        \u001b[36m0.0191\u001b[0m        0.0422  1.1653\n",
            "     15        \u001b[36m0.0172\u001b[0m        0.0452  1.1802\n",
            "     16        \u001b[36m0.0167\u001b[0m        0.0364  1.1494\n",
            "     17        \u001b[36m0.0163\u001b[0m        0.0381  1.2581\n",
            "     18        \u001b[36m0.0161\u001b[0m        0.0357  1.4785\n",
            "     19        \u001b[36m0.0148\u001b[0m        0.0337  1.5612\n",
            "     20        \u001b[36m0.0143\u001b[0m        0.0281  1.1852\n",
            "     21        \u001b[36m0.0139\u001b[0m        0.0270  1.1633\n",
            "     22        \u001b[36m0.0134\u001b[0m        0.0224  1.1472\n",
            "     23        \u001b[36m0.0130\u001b[0m        0.0233  1.1996\n",
            "     24        0.0130        0.0221  1.1563\n",
            "     25        \u001b[36m0.0120\u001b[0m        0.0206  1.1851\n",
            "     26        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0168\u001b[0m  1.1703\n",
            "     27        \u001b[36m0.0102\u001b[0m        0.0193  1.1468\n",
            "     28        0.0108        0.0291  1.3152\n",
            "     29        \u001b[36m0.0101\u001b[0m        0.0280  1.4796\n",
            "     30        \u001b[36m0.0086\u001b[0m        \u001b[32m0.0150\u001b[0m  1.5522\n",
            "     31        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0096\u001b[0m  1.1432\n",
            "     32        \u001b[36m0.0076\u001b[0m        0.0439  1.1738\n",
            "     33        0.0106        0.0105  1.1566\n",
            "     34        \u001b[36m0.0069\u001b[0m        0.0138  1.1397\n",
            "     35        \u001b[36m0.0066\u001b[0m        0.0175  1.1582\n",
            "     36        \u001b[36m0.0064\u001b[0m        0.0193  1.1561\n",
            "     37        0.0068        0.1273  1.1547\n",
            "     38        0.0084        0.0320  1.1543\n",
            "     39        0.0082        0.0319  1.3183\n",
            "     40        0.0093        0.0251  1.4686\n",
            "     41        0.0087        0.0301  1.5406\n",
            "     42        0.0075        0.0269  1.1919\n",
            "     43        0.0070        0.0285  1.1460\n",
            "     44        0.0078        0.0226  1.1501\n",
            "     45        0.0089        0.0186  1.1668\n",
            "     46        0.0079        0.0163  1.1501\n",
            "     47        0.0065        0.0145  1.1920\n",
            "     48        \u001b[36m0.0054\u001b[0m        0.0139  1.1579\n",
            "     49        \u001b[36m0.0051\u001b[0m        0.0180  1.1583\n",
            "     50        0.0054        0.0364  1.2970\n",
            "     51        0.0062        0.0175  1.4748\n",
            "     52        0.0056        0.0377  1.5260\n",
            "     53        0.0077        0.0171  1.1782\n",
            "     54        0.0064        0.0227  1.1415\n",
            "     55        0.0195        0.0194  1.1633\n",
            "     56        0.0055        0.0102  1.1494\n",
            "     57        \u001b[36m0.0041\u001b[0m        0.0269  1.1659\n",
            "     58        0.0048        0.0479  1.1485\n",
            "     59        0.0049        0.0633  1.1682\n",
            "     60        0.0052        0.0697  1.1478\n",
            "     61        0.0055        0.0796  1.2823\n",
            "     62        0.0057        0.0829  1.4848\n",
            "     63        0.0060        0.0800  1.5616\n",
            "     64        0.0060        0.0822  1.2044\n",
            "     65        0.0059        0.0922  1.1683\n",
            "     66        0.0061        0.1001  1.1690\n",
            "     67        0.0060        0.0757  1.1487\n",
            "     68        0.0066        0.0780  1.1750\n",
            "     69        0.0066        0.0534  1.1873\n",
            "     70        0.0062        0.0548  1.1490\n",
            "     71        0.0090        0.0288  1.1520\n",
            "     72        0.0056        0.0393  1.2967\n",
            "     73        0.0050        0.0294  1.4728\n",
            "     74        0.0047        0.0248  1.5752\n",
            "     75        0.0050        0.0489  1.1882\n",
            "     76        0.0048        0.0476  1.1779\n",
            "     77        0.0051        0.0507  1.1571\n",
            "     78        0.0059        0.0595  1.1526\n",
            "     79        0.0051        0.0517  1.1475\n",
            "     80        0.0049        0.0413  1.1584\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.7min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1582\u001b[0m        \u001b[32m0.2109\u001b[0m  1.3230\n",
            "      2        \u001b[36m0.1521\u001b[0m        \u001b[32m0.2046\u001b[0m  1.2719\n",
            "      3        \u001b[36m0.1499\u001b[0m        \u001b[32m0.1981\u001b[0m  1.5939\n",
            "      4        \u001b[36m0.1462\u001b[0m        \u001b[32m0.1879\u001b[0m  1.6620\n",
            "      5        \u001b[36m0.1358\u001b[0m        \u001b[32m0.1797\u001b[0m  1.4181\n",
            "      6        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1309\u001b[0m  1.2672\n",
            "      7        \u001b[36m0.1240\u001b[0m        \u001b[32m0.0798\u001b[0m  1.2526\n",
            "      8        \u001b[36m0.1156\u001b[0m        \u001b[32m0.0621\u001b[0m  1.2368\n",
            "      9        \u001b[36m0.1091\u001b[0m        0.0658  1.2259\n",
            "     10        \u001b[36m0.1014\u001b[0m        \u001b[32m0.0505\u001b[0m  1.2428\n",
            "     11        \u001b[36m0.0462\u001b[0m        0.0512  1.2407\n",
            "     12        \u001b[36m0.0346\u001b[0m        \u001b[32m0.0384\u001b[0m  1.2334\n",
            "     13        \u001b[36m0.0306\u001b[0m        0.0440  1.4903\n",
            "     14        \u001b[36m0.0276\u001b[0m        \u001b[32m0.0323\u001b[0m  1.6307\n",
            "     15        \u001b[36m0.0234\u001b[0m        0.0408  1.5226\n",
            "     16        \u001b[36m0.0215\u001b[0m        0.0350  1.2390\n",
            "     17        \u001b[36m0.0199\u001b[0m        \u001b[32m0.0305\u001b[0m  1.2482\n",
            "     18        \u001b[36m0.0183\u001b[0m        0.0337  1.2608\n",
            "     19        \u001b[36m0.0174\u001b[0m        \u001b[32m0.0293\u001b[0m  1.2660\n",
            "     20        0.0178        0.0315  1.2618\n",
            "     21        \u001b[36m0.0160\u001b[0m        0.0343  1.2697\n",
            "     22        0.0171        \u001b[32m0.0269\u001b[0m  1.2573\n",
            "     23        0.0166        \u001b[32m0.0262\u001b[0m  1.4510\n",
            "     24        \u001b[36m0.0151\u001b[0m        0.0279  1.6991\n",
            "     25        \u001b[36m0.0148\u001b[0m        0.0276  1.5639\n",
            "     26        \u001b[36m0.0143\u001b[0m        \u001b[32m0.0192\u001b[0m  1.2495\n",
            "     27        \u001b[36m0.0137\u001b[0m        0.0219  1.3238\n",
            "     28        0.0140        \u001b[32m0.0183\u001b[0m  1.2839\n",
            "     29        \u001b[36m0.0128\u001b[0m        \u001b[32m0.0181\u001b[0m  1.2261\n",
            "     30        \u001b[36m0.0118\u001b[0m        0.0215  1.2728\n",
            "     31        0.0122        0.0271  1.2521\n",
            "     32        0.0118        0.0362  1.2575\n",
            "     33        0.0118        0.0273  1.3702\n",
            "     34        \u001b[36m0.0118\u001b[0m        0.0221  1.6514\n",
            "     35        \u001b[36m0.0103\u001b[0m        0.0255  1.6425\n",
            "     36        0.0105        0.0191  1.2072\n",
            "     37        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0153\u001b[0m  1.2390\n",
            "     38        0.0101        0.0217  1.2432\n",
            "     39        \u001b[36m0.0093\u001b[0m        0.0154  1.2394\n",
            "     40        0.0093        \u001b[32m0.0101\u001b[0m  1.2541\n",
            "     41        \u001b[36m0.0091\u001b[0m        0.0150  1.2328\n",
            "     42        0.0095        0.0134  1.2410\n",
            "     43        \u001b[36m0.0091\u001b[0m        0.0133  1.3028\n",
            "     44        \u001b[36m0.0090\u001b[0m        0.0219  1.6214\n",
            "     45        \u001b[36m0.0083\u001b[0m        0.0183  1.6560\n",
            "     46        \u001b[36m0.0083\u001b[0m        0.0134  1.3270\n",
            "     47        0.0091        0.0165  1.2414\n",
            "     48        0.0083        0.0153  1.2487\n",
            "     49        \u001b[36m0.0076\u001b[0m        0.0129  1.2464\n",
            "     50        0.0094        \u001b[32m0.0088\u001b[0m  1.2549\n",
            "     51        0.0083        0.0121  1.2343\n",
            "     52        0.0080        0.0117  1.2283\n",
            "     53        \u001b[36m0.0066\u001b[0m        0.0109  1.2440\n",
            "     54        0.0071        0.0165  1.5825\n",
            "     55        0.0069        \u001b[32m0.0065\u001b[0m  1.6217\n",
            "     56        0.0068        0.0194  1.4334\n",
            "     57        0.0076        0.0065  1.2481\n",
            "     58        0.0068        0.0244  1.2357\n",
            "     59        0.0094        0.0105  1.2435\n",
            "     60        0.0106        0.0111  1.2801\n",
            "     61        0.0070        0.0081  1.2482\n",
            "     62        \u001b[36m0.0054\u001b[0m        0.0119  1.2417\n",
            "     63        0.0061        0.0087  1.2356\n",
            "     64        0.0062        \u001b[32m0.0039\u001b[0m  1.4531\n",
            "     65        0.0059        0.0079  1.6332\n",
            "     66        0.0072        0.0087  1.5652\n",
            "     67        0.0066        0.0065  1.2582\n",
            "     68        0.0068        0.0251  1.2508\n",
            "     69        0.0077        0.0130  1.2359\n",
            "     70        0.0055        0.0203  1.2545\n",
            "     71        0.0068        0.0130  1.2530\n",
            "     72        \u001b[36m0.0054\u001b[0m        0.0164  1.2398\n",
            "     73        0.0078        0.0086  1.2523\n",
            "     74        \u001b[36m0.0052\u001b[0m        0.0217  1.4424\n",
            "     75        0.0084        0.0155  1.6240\n",
            "     76        0.0068        0.0049  1.6289\n",
            "     77        0.0074        0.0052  1.2479\n",
            "     78        0.0054        0.0078  1.2143\n",
            "     79        0.0074        0.0116  1.2382\n",
            "     80        0.0061        0.0113  1.2423\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1585\u001b[0m        \u001b[32m0.2197\u001b[0m  1.2877\n",
            "      2        \u001b[36m0.1567\u001b[0m        \u001b[32m0.2158\u001b[0m  1.2511\n",
            "      3        \u001b[36m0.1536\u001b[0m        \u001b[32m0.2083\u001b[0m  1.2288\n",
            "      4        0.1557        0.2088  1.4419\n",
            "      5        \u001b[36m0.1522\u001b[0m        \u001b[32m0.2070\u001b[0m  1.6102\n",
            "      6        0.1531        \u001b[32m0.2056\u001b[0m  1.5798\n",
            "      7        \u001b[36m0.1445\u001b[0m        \u001b[32m0.2048\u001b[0m  1.2394\n",
            "      8        0.1450        \u001b[32m0.1977\u001b[0m  1.2332\n",
            "      9        \u001b[36m0.1380\u001b[0m        \u001b[32m0.1974\u001b[0m  1.2315\n",
            "     10        0.1409        0.2029  1.2545\n",
            "     11        0.1486        \u001b[32m0.1912\u001b[0m  1.2351\n",
            "     12        0.1456        0.1917  1.2498\n",
            "     13        0.1424        \u001b[32m0.1769\u001b[0m  1.2640\n",
            "     14        0.1388        \u001b[32m0.1539\u001b[0m  1.3479\n",
            "     15        \u001b[36m0.1364\u001b[0m        \u001b[32m0.0965\u001b[0m  1.6019\n",
            "     16        \u001b[36m0.1315\u001b[0m        0.1390  1.6656\n",
            "     17        \u001b[36m0.1261\u001b[0m        \u001b[32m0.0908\u001b[0m  1.2246\n",
            "     18        \u001b[36m0.1032\u001b[0m        \u001b[32m0.0766\u001b[0m  1.2231\n",
            "     19        \u001b[36m0.0986\u001b[0m        0.1110  1.2275\n",
            "     20        \u001b[36m0.0641\u001b[0m        \u001b[32m0.0671\u001b[0m  1.2476\n",
            "     21        \u001b[36m0.0454\u001b[0m        \u001b[32m0.0252\u001b[0m  1.2270\n",
            "     22        0.0465        0.0553  1.2249\n",
            "     23        \u001b[36m0.0362\u001b[0m        0.0597  1.2429\n",
            "     24        \u001b[36m0.0345\u001b[0m        0.0791  1.2346\n",
            "     25        0.0354        0.0705  1.6558\n",
            "     26        \u001b[36m0.0339\u001b[0m        0.0484  1.6363\n",
            "     27        0.0346        0.0611  1.3704\n",
            "     28        \u001b[36m0.0323\u001b[0m        0.0642  1.2328\n",
            "     29        0.0348        0.0301  1.2275\n",
            "     30        \u001b[36m0.0294\u001b[0m        0.0555  1.2257\n",
            "     31        \u001b[36m0.0263\u001b[0m        0.0620  1.2521\n",
            "     32        \u001b[36m0.0259\u001b[0m        0.0759  1.2297\n",
            "     33        \u001b[36m0.0231\u001b[0m        0.0674  1.2348\n",
            "     34        0.0237        0.0654  1.2384\n",
            "     35        \u001b[36m0.0229\u001b[0m        0.0544  1.4618\n",
            "     36        \u001b[36m0.0209\u001b[0m        0.0731  1.6465\n",
            "     37        0.0211        0.0702  1.5355\n",
            "     38        \u001b[36m0.0191\u001b[0m        0.0665  1.2372\n",
            "     39        0.0215        0.0552  1.2337\n",
            "     40        \u001b[36m0.0189\u001b[0m        0.0469  1.2531\n",
            "     41        0.0193        0.0436  1.2656\n",
            "     42        \u001b[36m0.0181\u001b[0m        0.0483  1.2461\n",
            "     43        0.0184        0.0382  1.2318\n",
            "     44        0.0184        0.0479  1.2657\n",
            "     45        \u001b[36m0.0172\u001b[0m        0.0493  1.4065\n",
            "     46        0.0176        0.0385  1.6106\n",
            "     47        0.0172        0.0410  1.6717\n",
            "     48        0.0175        0.0304  1.2342\n",
            "     49        0.0174        0.0353  1.2351\n",
            "     50        \u001b[36m0.0164\u001b[0m        0.0289  1.2458\n",
            "     51        0.0171        0.0263  1.2514\n",
            "     52        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0221\u001b[0m  1.2327\n",
            "     53        0.0160        0.0285  1.2315\n",
            "     54        0.0162        0.0397  1.2558\n",
            "     55        0.0166        0.0269  1.2634\n",
            "     56        0.0163        0.0306  1.6254\n",
            "     57        0.0163        0.0252  1.6621\n",
            "     58        \u001b[36m0.0151\u001b[0m        0.0277  1.3455\n",
            "     59        0.0159        \u001b[32m0.0209\u001b[0m  1.2674\n",
            "     60        0.0162        0.0259  1.2305\n",
            "     61        0.0151        \u001b[32m0.0204\u001b[0m  1.2527\n",
            "     62        0.0161        0.0207  1.2314\n",
            "     63        0.0152        0.0216  1.2374\n",
            "     64        0.0156        \u001b[32m0.0179\u001b[0m  1.2459\n",
            "     65        0.0153        \u001b[32m0.0174\u001b[0m  1.2274\n",
            "     66        0.0151        0.0205  1.5169\n",
            "     67        0.0156        \u001b[32m0.0153\u001b[0m  1.6398\n",
            "     68        \u001b[36m0.0149\u001b[0m        0.0156  1.4864\n",
            "     69        \u001b[36m0.0149\u001b[0m        0.0176  1.2316\n",
            "     70        0.0149        \u001b[32m0.0151\u001b[0m  1.2665\n",
            "     71        0.0187        0.0259  1.2413\n",
            "     72        0.0171        0.0190  1.2430\n",
            "     73        0.0152        \u001b[32m0.0148\u001b[0m  1.2309\n",
            "     74        0.0151        0.0163  1.2333\n",
            "     75        0.0151        \u001b[32m0.0140\u001b[0m  1.2300\n",
            "     76        \u001b[36m0.0144\u001b[0m        0.0151  1.4443\n",
            "     77        0.0168        0.0204  1.6475\n",
            "     78        0.0150        \u001b[32m0.0132\u001b[0m  1.5886\n",
            "     79        0.0151        0.0159  1.2205\n",
            "     80        0.0150        \u001b[32m0.0117\u001b[0m  1.2250\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1596\u001b[0m        \u001b[32m0.2071\u001b[0m  1.1612\n",
            "      2        \u001b[36m0.1441\u001b[0m        \u001b[32m0.1937\u001b[0m  1.1865\n",
            "      3        \u001b[36m0.1365\u001b[0m        \u001b[32m0.1697\u001b[0m  1.1622\n",
            "      4        \u001b[36m0.1198\u001b[0m        \u001b[32m0.1149\u001b[0m  1.1516\n",
            "      5        \u001b[36m0.0970\u001b[0m        \u001b[32m0.1095\u001b[0m  1.1512\n",
            "      6        \u001b[36m0.0752\u001b[0m        \u001b[32m0.0630\u001b[0m  1.1934\n",
            "      7        \u001b[36m0.0334\u001b[0m        \u001b[32m0.0354\u001b[0m  1.5181\n",
            "      8        \u001b[36m0.0332\u001b[0m        0.0433  1.5043\n",
            "      9        \u001b[36m0.0292\u001b[0m        0.0574  1.3219\n",
            "     10        \u001b[36m0.0222\u001b[0m        \u001b[32m0.0278\u001b[0m  1.1509\n",
            "     11        \u001b[36m0.0160\u001b[0m        0.0285  1.1427\n",
            "     12        \u001b[36m0.0139\u001b[0m        \u001b[32m0.0220\u001b[0m  1.1433\n",
            "     13        \u001b[36m0.0121\u001b[0m        0.0469  1.1372\n",
            "     14        \u001b[36m0.0116\u001b[0m        0.0566  1.1649\n",
            "     15        \u001b[36m0.0116\u001b[0m        0.0652  1.1583\n",
            "     16        \u001b[36m0.0104\u001b[0m        0.0585  1.1735\n",
            "     17        \u001b[36m0.0096\u001b[0m        0.0600  1.1968\n",
            "     18        0.0099        0.0739  1.4889\n",
            "     19        \u001b[36m0.0095\u001b[0m        0.0644  1.5415\n",
            "     20        0.0098        0.0481  1.2447\n",
            "     21        \u001b[36m0.0080\u001b[0m        0.0441  1.1511\n",
            "     22        \u001b[36m0.0068\u001b[0m        0.0361  1.1496\n",
            "     23        \u001b[36m0.0063\u001b[0m        0.0272  1.1449\n",
            "     24        \u001b[36m0.0061\u001b[0m        \u001b[32m0.0219\u001b[0m  1.1480\n",
            "     25        \u001b[36m0.0056\u001b[0m        \u001b[32m0.0154\u001b[0m  1.1675\n",
            "     26        \u001b[36m0.0052\u001b[0m        0.0179  1.1771\n",
            "     27        \u001b[36m0.0051\u001b[0m        0.0351  1.1731\n",
            "     28        \u001b[36m0.0047\u001b[0m        0.0333  1.2573\n",
            "     29        \u001b[36m0.0045\u001b[0m        0.0444  1.4891\n",
            "     30        0.0052        0.0170  1.5643\n",
            "     31        \u001b[36m0.0041\u001b[0m        0.0264  1.2383\n",
            "     32        0.0042        0.0245  1.1403\n",
            "     33        \u001b[36m0.0034\u001b[0m        0.0374  1.1618\n",
            "     34        \u001b[36m0.0031\u001b[0m        0.0418  1.1458\n",
            "     35        0.0056        0.0428  1.1450\n",
            "     36        0.0033        0.0747  1.1814\n",
            "     37        0.0053        0.0610  1.1358\n",
            "     38        0.0047        0.0472  1.1789\n",
            "     39        0.0060        0.0172  1.2472\n",
            "     40        0.0043        \u001b[32m0.0101\u001b[0m  1.4717\n",
            "     41        \u001b[36m0.0027\u001b[0m        \u001b[32m0.0093\u001b[0m  1.5205\n",
            "     42        0.0032        \u001b[32m0.0055\u001b[0m  1.2558\n",
            "     43        \u001b[36m0.0025\u001b[0m        0.0095  1.1512\n",
            "     44        \u001b[36m0.0023\u001b[0m        0.0384  1.1606\n",
            "     45        0.0030        0.0312  1.1819\n",
            "     46        0.0025        0.0506  1.1817\n",
            "     47        0.0027        0.0346  1.1451\n",
            "     48        0.0024        0.0492  1.1442\n",
            "     49        0.0029        0.0490  1.1567\n",
            "     50        0.0026        0.0590  1.2392\n",
            "     51        0.0035        0.0363  1.5086\n",
            "     52        0.0036        0.0156  1.5328\n",
            "     53        \u001b[36m0.0020\u001b[0m        0.0279  1.2779\n",
            "     54        0.0033        0.0285  1.1802\n",
            "     55        0.0024        0.0199  1.1496\n",
            "     56        0.0024        0.0282  1.1272\n",
            "     57        0.0028        0.0190  1.1589\n",
            "     58        0.0022        0.0279  1.1648\n",
            "     59        0.0023        0.0323  1.1561\n",
            "     60        0.0025        0.0277  1.1599\n",
            "     61        0.0024        0.0395  1.1991\n",
            "     62        0.0030        0.0317  1.5130\n",
            "     63        0.0029        0.0170  1.5113\n",
            "     64        0.0024        0.0226  1.3201\n",
            "     65        0.0027        0.0268  1.1740\n",
            "     66        0.0037        0.0240  1.1486\n",
            "     67        0.0049        0.0055  1.1448\n",
            "     68        \u001b[36m0.0014\u001b[0m        0.0219  1.1503\n",
            "     69        0.0019        0.0237  1.1700\n",
            "     70        0.0021        0.0384  1.1481\n",
            "     71        0.0028        0.0442  1.1538\n",
            "     72        0.0046        0.0233  1.1444\n",
            "     73        0.0022        0.0238  1.4893\n",
            "     74        0.0031        0.0276  1.5255\n",
            "     75        0.0039        0.0178  1.3532\n",
            "     76        0.0021        0.0288  1.1544\n",
            "     77        0.0030        0.0156  1.1563\n",
            "     78        0.0021        0.0115  1.1561\n",
            "     79        0.0017        0.0200  1.1667\n",
            "     80        0.0035        0.0119  1.1463\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.7min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1545\u001b[0m        \u001b[32m0.1852\u001b[0m  1.2000\n",
            "      2        \u001b[36m0.1460\u001b[0m        \u001b[32m0.1746\u001b[0m  1.1454\n",
            "      3        \u001b[36m0.1374\u001b[0m        \u001b[32m0.1596\u001b[0m  1.2573\n",
            "      4        \u001b[36m0.1246\u001b[0m        \u001b[32m0.1509\u001b[0m  1.5004\n",
            "      5        0.1315        \u001b[32m0.1470\u001b[0m  1.5242\n",
            "      6        \u001b[36m0.1142\u001b[0m        \u001b[32m0.1175\u001b[0m  1.2460\n",
            "      7        \u001b[36m0.0997\u001b[0m        0.1179  1.1390\n",
            "      8        \u001b[36m0.0937\u001b[0m        \u001b[32m0.0557\u001b[0m  1.1274\n",
            "      9        \u001b[36m0.0744\u001b[0m        \u001b[32m0.0323\u001b[0m  1.1494\n",
            "     10        \u001b[36m0.0421\u001b[0m        0.0414  1.1413\n",
            "     11        \u001b[36m0.0240\u001b[0m        \u001b[32m0.0263\u001b[0m  1.1400\n",
            "     12        \u001b[36m0.0184\u001b[0m        \u001b[32m0.0173\u001b[0m  1.1460\n",
            "     13        \u001b[36m0.0155\u001b[0m        0.0245  1.1395\n",
            "     14        \u001b[36m0.0148\u001b[0m        0.0338  1.2180\n",
            "     15        0.0152        0.0321  1.5000\n",
            "     16        \u001b[36m0.0141\u001b[0m        0.0322  1.5090\n",
            "     17        0.0142        0.0411  1.2688\n",
            "     18        \u001b[36m0.0130\u001b[0m        0.0268  1.1591\n",
            "     19        \u001b[36m0.0125\u001b[0m        0.0186  1.1627\n",
            "     20        \u001b[36m0.0107\u001b[0m        0.0196  1.1659\n",
            "     21        \u001b[36m0.0101\u001b[0m        0.0194  1.1639\n",
            "     22        \u001b[36m0.0097\u001b[0m        0.0215  1.1925\n",
            "     23        \u001b[36m0.0092\u001b[0m        0.0220  1.1951\n",
            "     24        0.0095        0.0228  1.1744\n",
            "     25        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0153\u001b[0m  1.2742\n",
            "     26        0.0093        0.0266  1.5078\n",
            "     27        \u001b[36m0.0077\u001b[0m        0.0190  1.5386\n",
            "     28        \u001b[36m0.0069\u001b[0m        0.0164  1.2442\n",
            "     29        \u001b[36m0.0063\u001b[0m        0.0204  1.1524\n",
            "     30        0.0066        \u001b[32m0.0089\u001b[0m  1.1320\n",
            "     31        0.0070        \u001b[32m0.0084\u001b[0m  1.1619\n",
            "     32        0.0072        0.0202  1.1496\n",
            "     33        0.0069        0.0334  1.1423\n",
            "     34        \u001b[36m0.0061\u001b[0m        0.0409  1.1382\n",
            "     35        \u001b[36m0.0055\u001b[0m        0.0332  1.1635\n",
            "     36        0.0057        0.0300  1.2268\n",
            "     37        \u001b[36m0.0055\u001b[0m        0.0366  1.4804\n",
            "     38        \u001b[36m0.0054\u001b[0m        0.0562  1.5759\n",
            "     39        0.0078        0.0272  1.2077\n",
            "     40        \u001b[36m0.0050\u001b[0m        0.0630  1.1435\n",
            "     41        0.0076        0.0366  1.1449\n",
            "     42        0.0052        0.0611  1.1400\n",
            "     43        0.0060        0.0692  1.1486\n",
            "     44        0.0057        0.0569  1.1459\n",
            "     45        0.0067        0.0507  1.1319\n",
            "     46        0.0058        0.0429  1.1372\n",
            "     47        0.0063        0.0424  1.2633\n",
            "     48        0.0073        0.0247  1.4858\n",
            "     49        0.0067        0.0246  1.5208\n",
            "     50        0.0056        0.0189  1.2237\n",
            "     51        \u001b[36m0.0049\u001b[0m        0.0161  1.1322\n",
            "     52        \u001b[36m0.0048\u001b[0m        0.0305  1.1421\n",
            "     53        0.0050        0.0167  1.1561\n",
            "     54        \u001b[36m0.0038\u001b[0m        0.0372  1.1704\n",
            "     55        0.0040        0.0492  1.1495\n",
            "     56        0.0066        0.1360  1.1796\n",
            "     57        0.0074        0.0880  1.1318\n",
            "     58        0.0090        0.0320  1.2080\n",
            "     59        0.0049        0.0299  1.4855\n",
            "     60        0.0047        0.0441  1.5097\n",
            "     61        0.0053        0.0445  1.2246\n",
            "     62        0.0048        0.0378  1.1572\n",
            "     63        0.0047        0.0479  1.1567\n",
            "     64        0.0046        0.0432  1.1537\n",
            "     65        0.0052        0.0570  1.1584\n",
            "     66        0.0047        0.0496  1.1583\n",
            "     67        0.0048        0.0627  1.1526\n",
            "     68        0.0059        0.0521  1.1293\n",
            "     69        0.0054        0.0388  1.1929\n",
            "     70        0.0043        0.0504  1.4988\n",
            "     71        0.0043        0.0527  1.5170\n",
            "     72        0.0043        0.0546  1.2849\n",
            "     73        0.0051        0.0567  1.1752\n",
            "     74        0.0047        0.0521  1.1766\n",
            "     75        0.0046        0.0530  1.1605\n",
            "     76        0.0046        0.0508  1.1308\n",
            "     77        0.0048        0.0538  1.1553\n",
            "     78        0.0048        0.0504  1.1611\n",
            "     79        0.0048        0.0435  1.1478\n",
            "     80        0.0049        0.0494  1.2448\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.7min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1567\u001b[0m        \u001b[32m0.2227\u001b[0m  1.6153\n",
            "      2        \u001b[36m0.1514\u001b[0m        \u001b[32m0.2093\u001b[0m  1.6406\n",
            "      3        \u001b[36m0.1493\u001b[0m        \u001b[32m0.2077\u001b[0m  1.2309\n",
            "      4        \u001b[36m0.1475\u001b[0m        0.2084  1.2404\n",
            "      5        \u001b[36m0.1447\u001b[0m        \u001b[32m0.2013\u001b[0m  1.2404\n",
            "      6        \u001b[36m0.1394\u001b[0m        \u001b[32m0.1883\u001b[0m  1.2357\n",
            "      7        \u001b[36m0.1340\u001b[0m        \u001b[32m0.1658\u001b[0m  1.2047\n",
            "      8        \u001b[36m0.1242\u001b[0m        \u001b[32m0.1293\u001b[0m  1.2120\n",
            "      9        \u001b[36m0.1181\u001b[0m        \u001b[32m0.0968\u001b[0m  1.2372\n",
            "     10        \u001b[36m0.1099\u001b[0m        \u001b[32m0.0667\u001b[0m  1.2595\n",
            "     11        \u001b[36m0.0947\u001b[0m        \u001b[32m0.0574\u001b[0m  1.6302\n",
            "     12        \u001b[36m0.0513\u001b[0m        0.0857  1.6635\n",
            "     13        \u001b[36m0.0339\u001b[0m        \u001b[32m0.0293\u001b[0m  1.3708\n",
            "     14        \u001b[36m0.0258\u001b[0m        \u001b[32m0.0282\u001b[0m  1.2505\n",
            "     15        \u001b[36m0.0223\u001b[0m        \u001b[32m0.0253\u001b[0m  1.2624\n",
            "     16        \u001b[36m0.0199\u001b[0m        0.0265  1.2401\n",
            "     17        \u001b[36m0.0183\u001b[0m        0.0409  1.2306\n",
            "     18        \u001b[36m0.0182\u001b[0m        0.0292  1.2331\n",
            "     19        \u001b[36m0.0164\u001b[0m        0.0395  1.2288\n",
            "     20        0.0176        \u001b[32m0.0224\u001b[0m  1.2353\n",
            "     21        0.0177        0.0235  1.5365\n",
            "     22        0.0179        \u001b[32m0.0156\u001b[0m  1.6473\n",
            "     23        0.0169        \u001b[32m0.0102\u001b[0m  1.5127\n",
            "     24        \u001b[36m0.0161\u001b[0m        0.0145  1.2342\n",
            "     25        0.0187        0.0141  1.2571\n",
            "     26        0.0163        0.0126  1.2484\n",
            "     27        \u001b[36m0.0145\u001b[0m        0.0201  1.2363\n",
            "     28        \u001b[36m0.0138\u001b[0m        0.0213  1.2358\n",
            "     29        \u001b[36m0.0131\u001b[0m        0.0145  1.2404\n",
            "     30        \u001b[36m0.0128\u001b[0m        0.0254  1.2374\n",
            "     31        0.0141        0.0219  1.3896\n",
            "     32        0.0153        0.0345  1.6080\n",
            "     33        0.0131        0.0337  1.6564\n",
            "     34        \u001b[36m0.0118\u001b[0m        0.0286  1.2453\n",
            "     35        \u001b[36m0.0106\u001b[0m        0.0367  1.2414\n",
            "     36        0.0107        0.0244  1.2535\n",
            "     37        \u001b[36m0.0102\u001b[0m        0.0227  1.2514\n",
            "     38        0.0109        0.0309  1.2518\n",
            "     39        0.0137        0.0345  1.2450\n",
            "     40        0.0110        0.0284  1.2677\n",
            "     41        \u001b[36m0.0094\u001b[0m        0.0179  1.3262\n",
            "     42        0.0099        0.0188  1.6475\n",
            "     43        0.0105        0.0193  1.6919\n",
            "     44        0.0116        0.0280  1.2879\n",
            "     45        \u001b[36m0.0086\u001b[0m        0.0290  1.2726\n",
            "     46        0.0086        0.0239  1.2409\n",
            "     47        \u001b[36m0.0083\u001b[0m        0.0175  1.2410\n",
            "     48        0.0093        0.0227  1.2339\n",
            "     49        \u001b[36m0.0076\u001b[0m        0.0139  1.2485\n",
            "     50        0.0085        0.0117  1.2447\n",
            "     51        0.0080        0.0157  1.2553\n",
            "     52        0.0079        0.0177  1.6010\n",
            "     53        \u001b[36m0.0071\u001b[0m        0.0105  1.6438\n",
            "     54        0.0097        0.0218  1.4180\n",
            "     55        0.0083        0.0105  1.2459\n",
            "     56        0.0075        0.0116  1.2688\n",
            "     57        0.0086        \u001b[32m0.0061\u001b[0m  1.2478\n",
            "     58        0.0072        0.0180  1.2463\n",
            "     59        \u001b[36m0.0067\u001b[0m        0.0094  1.2498\n",
            "     60        0.0075        0.0134  1.2409\n",
            "     61        0.0081        0.0175  1.2406\n",
            "     62        0.0082        0.0257  1.4982\n",
            "     63        0.0085        0.0345  1.6351\n",
            "     64        0.0084        0.0304  1.5048\n",
            "     65        0.0086        0.0214  1.2404\n",
            "     66        0.0085        0.0221  1.2473\n",
            "     67        0.0099        0.0363  1.2334\n",
            "     68        \u001b[36m0.0065\u001b[0m        0.0264  1.2635\n",
            "     69        0.0075        0.0143  1.2357\n",
            "     70        \u001b[36m0.0061\u001b[0m        0.0247  1.2320\n",
            "     71        0.0123        0.0311  1.2339\n",
            "     72        0.0107        0.0187  1.4266\n",
            "     73        0.0081        0.0092  1.6467\n",
            "     74        0.0077        0.0085  1.6226\n",
            "     75        0.0073        0.0099  1.2449\n",
            "     76        \u001b[36m0.0060\u001b[0m        0.0324  1.2411\n",
            "     77        0.0075        0.0137  1.2317\n",
            "     78        0.0067        0.0244  1.2356\n",
            "     79        0.0088        0.0319  1.2426\n",
            "     80        0.0081        0.0183  1.2557\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1575\u001b[0m        \u001b[32m0.2193\u001b[0m  1.2620\n",
            "      2        \u001b[36m0.1478\u001b[0m        \u001b[32m0.2095\u001b[0m  1.4048\n",
            "      3        0.1492        \u001b[32m0.2082\u001b[0m  1.6028\n",
            "      4        0.1521        \u001b[32m0.2040\u001b[0m  1.6513\n",
            "      5        0.1522        \u001b[32m0.2029\u001b[0m  1.2362\n",
            "      6        0.1506        0.2033  1.2292\n",
            "      7        0.1490        \u001b[32m0.2019\u001b[0m  1.2414\n",
            "      8        \u001b[36m0.1466\u001b[0m        0.2065  1.2609\n",
            "      9        \u001b[36m0.1437\u001b[0m        0.2032  1.2465\n",
            "     10        \u001b[36m0.1402\u001b[0m        0.2065  1.2636\n",
            "     11        \u001b[36m0.1375\u001b[0m        \u001b[32m0.1609\u001b[0m  1.2253\n",
            "     12        \u001b[36m0.1263\u001b[0m        \u001b[32m0.1545\u001b[0m  1.3107\n",
            "     13        \u001b[36m0.1240\u001b[0m        \u001b[32m0.1293\u001b[0m  1.6220\n",
            "     14        \u001b[36m0.1161\u001b[0m        \u001b[32m0.0864\u001b[0m  1.6729\n",
            "     15        \u001b[36m0.1017\u001b[0m        \u001b[32m0.0844\u001b[0m  1.2814\n",
            "     16        \u001b[36m0.0711\u001b[0m        \u001b[32m0.0561\u001b[0m  1.2392\n",
            "     17        \u001b[36m0.0473\u001b[0m        0.0774  1.2250\n",
            "     18        \u001b[36m0.0390\u001b[0m        \u001b[32m0.0374\u001b[0m  1.2403\n",
            "     19        \u001b[36m0.0327\u001b[0m        \u001b[32m0.0271\u001b[0m  1.2528\n",
            "     20        \u001b[36m0.0296\u001b[0m        0.0478  1.2394\n",
            "     21        \u001b[36m0.0270\u001b[0m        0.0575  1.2485\n",
            "     22        \u001b[36m0.0248\u001b[0m        0.0637  1.2413\n",
            "     23        \u001b[36m0.0237\u001b[0m        0.0565  1.6165\n",
            "     24        0.0245        0.0531  1.6249\n",
            "     25        \u001b[36m0.0226\u001b[0m        0.0516  1.3999\n",
            "     26        0.0259        0.0488  1.2485\n",
            "     27        \u001b[36m0.0226\u001b[0m        0.0439  1.2309\n",
            "     28        \u001b[36m0.0204\u001b[0m        0.0563  1.2299\n",
            "     29        0.0213        0.0561  1.2431\n",
            "     30        \u001b[36m0.0190\u001b[0m        0.0517  1.2296\n",
            "     31        \u001b[36m0.0187\u001b[0m        0.0607  1.2435\n",
            "     32        \u001b[36m0.0187\u001b[0m        0.0645  1.2488\n",
            "     33        \u001b[36m0.0175\u001b[0m        0.0620  1.5155\n",
            "     34        \u001b[36m0.0175\u001b[0m        0.0553  1.6246\n",
            "     35        \u001b[36m0.0166\u001b[0m        0.0510  1.4952\n",
            "     36        0.0210        0.0475  1.2137\n",
            "     37        0.0310        \u001b[32m0.0266\u001b[0m  1.2582\n",
            "     38        0.0198        0.0628  1.2336\n",
            "     39        0.0193        0.0300  1.2495\n",
            "     40        0.0178        0.0335  1.2553\n",
            "     41        0.0181        0.0500  1.2444\n",
            "     42        0.0184        0.0483  1.2385\n",
            "     43        0.0182        0.0582  1.4358\n",
            "     44        0.0173        0.0366  1.6323\n",
            "     45        \u001b[36m0.0152\u001b[0m        0.0379  1.6008\n",
            "     46        0.0163        0.0367  1.2530\n",
            "     47        \u001b[36m0.0146\u001b[0m        0.0344  1.2448\n",
            "     48        0.0154        0.0313  1.2085\n",
            "     49        0.0158        \u001b[32m0.0249\u001b[0m  1.2802\n",
            "     50        0.0185        0.0371  1.2312\n",
            "     51        0.0186        0.0461  1.2450\n",
            "     52        0.0165        0.0377  1.2540\n",
            "     53        0.0152        0.0374  1.3836\n",
            "     54        0.0152        0.0256  1.6131\n",
            "     55        0.0158        0.0397  1.7438\n",
            "     56        0.0172        0.0269  1.2635\n",
            "     57        0.0189        0.0315  1.2472\n",
            "     58        0.0160        0.0298  1.2373\n",
            "     59        0.0179        \u001b[32m0.0222\u001b[0m  1.2444\n",
            "     60        0.0155        0.0319  1.2263\n",
            "     61        \u001b[36m0.0145\u001b[0m        0.0255  1.2408\n",
            "     62        0.0149        0.0255  1.2294\n",
            "     63        0.0146        \u001b[32m0.0173\u001b[0m  1.2518\n",
            "     64        0.0150        0.0691  1.6173\n",
            "     65        0.0157        0.0284  1.6412\n",
            "     66        0.0159        0.0256  1.3689\n",
            "     67        0.0151        0.0187  1.2656\n",
            "     68        0.0179        0.0361  1.2548\n",
            "     69        0.0161        0.0267  1.2429\n",
            "     70        \u001b[36m0.0144\u001b[0m        0.0196  1.2331\n",
            "     71        0.0170        0.0842  1.2459\n",
            "     72        0.0167        0.0281  1.2568\n",
            "     73        \u001b[36m0.0139\u001b[0m        0.0223  1.2347\n",
            "     74        0.0159        0.0489  1.5736\n",
            "     75        0.0184        0.0231  1.6436\n",
            "     76        0.0146        0.0230  1.4079\n",
            "     77        0.0150        0.0243  1.2420\n",
            "     78        \u001b[36m0.0138\u001b[0m        0.0193  1.2738\n",
            "     79        0.0162        0.0385  1.2542\n",
            "     80        0.0146        \u001b[32m0.0169\u001b[0m  1.2336\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1669\u001b[0m        \u001b[32m0.1838\u001b[0m  1.1540\n",
            "      2        \u001b[36m0.1435\u001b[0m        \u001b[32m0.1624\u001b[0m  1.1563\n",
            "      3        \u001b[36m0.1373\u001b[0m        \u001b[32m0.1298\u001b[0m  1.1802\n",
            "      4        \u001b[36m0.1103\u001b[0m        \u001b[32m0.0829\u001b[0m  1.3833\n",
            "      5        \u001b[36m0.1072\u001b[0m        \u001b[32m0.0753\u001b[0m  1.4846\n",
            "      6        \u001b[36m0.1057\u001b[0m        \u001b[32m0.0742\u001b[0m  1.4325\n",
            "      7        \u001b[36m0.0829\u001b[0m        \u001b[32m0.0585\u001b[0m  1.1388\n",
            "      8        \u001b[36m0.0723\u001b[0m        0.0845  1.1513\n",
            "      9        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0370\u001b[0m  1.1587\n",
            "     10        \u001b[36m0.0295\u001b[0m        \u001b[32m0.0213\u001b[0m  1.1680\n",
            "     11        \u001b[36m0.0219\u001b[0m        \u001b[32m0.0143\u001b[0m  1.1453\n",
            "     12        \u001b[36m0.0201\u001b[0m        0.0297  1.1698\n",
            "     13        \u001b[36m0.0176\u001b[0m        0.0476  1.1545\n",
            "     14        \u001b[36m0.0158\u001b[0m        0.0212  1.1731\n",
            "     15        \u001b[36m0.0144\u001b[0m        0.0218  1.4374\n",
            "     16        \u001b[36m0.0141\u001b[0m        0.0238  1.5139\n",
            "     17        \u001b[36m0.0124\u001b[0m        0.0329  1.4634\n",
            "     18        0.0127        0.0216  1.1719\n",
            "     19        \u001b[36m0.0109\u001b[0m        0.0341  1.1622\n",
            "     20        0.0113        0.0285  1.2179\n",
            "     21        \u001b[36m0.0104\u001b[0m        0.0197  1.1780\n",
            "     22        \u001b[36m0.0097\u001b[0m        0.0373  1.1816\n",
            "     23        \u001b[36m0.0094\u001b[0m        0.0467  1.1519\n",
            "     24        \u001b[36m0.0090\u001b[0m        0.0457  1.1836\n",
            "     25        \u001b[36m0.0089\u001b[0m        0.0457  1.1667\n",
            "     26        0.0090        0.0549  1.4075\n",
            "     27        0.0093        0.0494  1.4851\n",
            "     28        0.0090        0.0439  1.3810\n",
            "     29        \u001b[36m0.0085\u001b[0m        0.0473  1.1603\n",
            "     30        0.0091        0.0441  1.1766\n",
            "     31        \u001b[36m0.0080\u001b[0m        0.0372  1.1513\n",
            "     32        \u001b[36m0.0076\u001b[0m        0.0284  1.1483\n",
            "     33        \u001b[36m0.0068\u001b[0m        0.0425  1.1451\n",
            "     34        \u001b[36m0.0067\u001b[0m        0.0440  1.1826\n",
            "     35        0.0216        0.0448  1.1840\n",
            "     36        0.0115        0.0316  1.1703\n",
            "     37        0.0072        0.0348  1.4806\n",
            "     38        \u001b[36m0.0062\u001b[0m        0.0238  1.4838\n",
            "     39        \u001b[36m0.0054\u001b[0m        0.0223  1.3096\n",
            "     40        \u001b[36m0.0050\u001b[0m        0.0180  1.1437\n",
            "     41        \u001b[36m0.0046\u001b[0m        0.0161  1.1594\n",
            "     42        \u001b[36m0.0046\u001b[0m        0.0158  1.1576\n",
            "     43        \u001b[36m0.0042\u001b[0m        0.0620  1.1430\n",
            "     44        0.0044        0.0476  1.1561\n",
            "     45        \u001b[36m0.0041\u001b[0m        0.0303  1.1587\n",
            "     46        0.0062        0.0201  1.1545\n",
            "     47        0.0042        \u001b[32m0.0102\u001b[0m  1.1922\n",
            "     48        \u001b[36m0.0028\u001b[0m        0.0153  1.4855\n",
            "     49        0.0031        0.0215  1.5349\n",
            "     50        0.0031        0.0146  1.3537\n",
            "     51        0.0030        0.0106  1.1718\n",
            "     52        0.0032        \u001b[32m0.0076\u001b[0m  1.1574\n",
            "     53        \u001b[36m0.0025\u001b[0m        0.0354  1.1464\n",
            "     54        0.0057        0.0185  1.1454\n",
            "     55        0.0042        \u001b[32m0.0025\u001b[0m  1.1567\n",
            "     56        \u001b[36m0.0015\u001b[0m        0.0125  1.1534\n",
            "     57        0.0019        0.0518  1.1437\n",
            "     58        0.0031        0.0620  1.1468\n",
            "     59        0.0039        0.0391  1.4832\n",
            "     60        0.0051        0.0438  1.5230\n",
            "     61        0.0061        0.0199  1.3539\n",
            "     62        0.0036        0.0346  1.1659\n",
            "     63        0.0028        0.0337  1.1745\n",
            "     64        0.0044        0.0305  1.1792\n",
            "     65        0.0034        0.0172  1.1819\n",
            "     66        0.0021        0.0221  1.1591\n",
            "     67        0.0021        0.0210  1.1556\n",
            "     68        0.0020        0.0379  1.1730\n",
            "     69        0.0026        0.0520  1.1398\n",
            "     70        0.0035        0.0522  1.4917\n",
            "     71        0.0033        0.0431  1.5136\n",
            "     72        0.0042        0.0618  1.2982\n",
            "     73        0.0072        0.0097  1.1765\n",
            "     74        0.0018        0.0342  1.2038\n",
            "     75        0.0033        0.0579  1.1682\n",
            "     76        0.0024        0.0578  1.1462\n",
            "     77        0.0039        0.0487  1.1685\n",
            "     78        0.0023        0.0459  1.1619\n",
            "     79        0.0026        0.0489  1.1726\n",
            "     80        0.0024        0.0453  1.2204\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.7min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1521\u001b[0m        \u001b[32m0.1917\u001b[0m  1.5146\n",
            "      2        \u001b[36m0.1424\u001b[0m        \u001b[32m0.1502\u001b[0m  1.5868\n",
            "      3        \u001b[36m0.1339\u001b[0m        \u001b[32m0.1465\u001b[0m  1.2139\n",
            "      4        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1379\u001b[0m  1.1714\n",
            "      5        \u001b[36m0.1025\u001b[0m        \u001b[32m0.0657\u001b[0m  1.1596\n",
            "      6        \u001b[36m0.0971\u001b[0m        0.0727  1.1647\n",
            "      7        0.0974        \u001b[32m0.0489\u001b[0m  1.1899\n",
            "      8        \u001b[36m0.0844\u001b[0m        0.0641  1.1489\n",
            "      9        \u001b[36m0.0678\u001b[0m        0.1353  1.1852\n",
            "     10        \u001b[36m0.0533\u001b[0m        0.0626  1.1433\n",
            "     11        \u001b[36m0.0308\u001b[0m        0.0795  1.3251\n",
            "     12        \u001b[36m0.0270\u001b[0m        0.0517  1.4934\n",
            "     13        \u001b[36m0.0213\u001b[0m        \u001b[32m0.0464\u001b[0m  1.5901\n",
            "     14        \u001b[36m0.0186\u001b[0m        0.0939  1.1876\n",
            "     15        0.0222        \u001b[32m0.0368\u001b[0m  1.1899\n",
            "     16        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0181\u001b[0m  1.1791\n",
            "     17        \u001b[36m0.0135\u001b[0m        0.0183  1.1849\n",
            "     18        0.0149        0.0313  1.1912\n",
            "     19        0.0148        0.0317  1.1733\n",
            "     20        \u001b[36m0.0128\u001b[0m        0.0308  1.1595\n",
            "     21        \u001b[36m0.0113\u001b[0m        0.0339  1.1607\n",
            "     22        \u001b[36m0.0105\u001b[0m        0.0387  1.4058\n",
            "     23        \u001b[36m0.0102\u001b[0m        0.0339  1.5134\n",
            "     24        \u001b[36m0.0097\u001b[0m        0.0273  1.5765\n",
            "     25        \u001b[36m0.0094\u001b[0m        0.0267  1.1429\n",
            "     26        \u001b[36m0.0087\u001b[0m        0.0225  1.1562\n",
            "     27        \u001b[36m0.0081\u001b[0m        0.0264  1.1673\n",
            "     28        \u001b[36m0.0078\u001b[0m        0.0249  1.1748\n",
            "     29        0.0084        0.0312  1.1643\n",
            "     30        \u001b[36m0.0074\u001b[0m        0.0300  1.1712\n",
            "     31        0.0075        0.0350  1.1918\n",
            "     32        0.0077        0.0265  1.1667\n",
            "     33        \u001b[36m0.0069\u001b[0m        0.0270  1.3848\n",
            "     34        \u001b[36m0.0065\u001b[0m        0.0314  1.5065\n",
            "     35        0.0068        0.0334  1.5399\n",
            "     36        0.0069        0.0292  1.1813\n",
            "     37        0.0066        0.0276  1.1814\n",
            "     38        \u001b[36m0.0060\u001b[0m        0.0406  1.1577\n",
            "     39        0.0068        0.0332  1.2143\n",
            "     40        0.0070        \u001b[32m0.0181\u001b[0m  1.1549\n",
            "     41        \u001b[36m0.0056\u001b[0m        0.0257  1.1625\n",
            "     42        0.0058        0.0308  1.1752\n",
            "     43        0.0064        0.0316  1.1763\n",
            "     44        \u001b[36m0.0053\u001b[0m        0.0356  1.3695\n",
            "     45        0.0068        0.0253  1.5414\n",
            "     46        0.0064        0.0402  1.5141\n",
            "     47        0.0096        0.0413  1.1403\n",
            "     48        0.0071        0.0311  1.1541\n",
            "     49        0.0063        0.0338  1.1441\n",
            "     50        0.0062        0.0324  1.1616\n",
            "     51        0.0061        0.0276  1.1833\n",
            "     52        0.0062        0.0313  1.1811\n",
            "     53        0.0059        0.0349  1.1639\n",
            "     54        0.0063        0.0357  1.1640\n",
            "     55        0.0063        0.0376  1.4033\n",
            "     56        0.0060        0.0298  1.4968\n",
            "     57        0.0058        0.0330  1.5271\n",
            "     58        0.0062        0.0313  1.1310\n",
            "     59        0.0053        0.0328  1.1288\n",
            "     60        0.0057        0.0332  1.1415\n",
            "     61        0.0056        0.0386  1.1625\n",
            "     62        0.0055        0.0551  1.1609\n",
            "     63        0.0060        0.0312  1.1966\n",
            "     64        \u001b[36m0.0051\u001b[0m        0.0432  1.1808\n",
            "     65        0.0054        0.0531  1.1646\n",
            "     66        0.0053        0.0464  1.3669\n",
            "     67        0.0059        0.0388  1.4910\n",
            "     68        0.0054        0.0403  1.5588\n",
            "     69        0.0053        0.0449  1.1726\n",
            "     70        0.0054        0.0469  1.1633\n",
            "     71        0.0054        0.0419  1.1968\n",
            "     72        0.0052        0.0448  1.1988\n",
            "     73        0.0056        0.0367  1.1632\n",
            "     74        0.0052        0.0599  1.1909\n",
            "     75        0.0053        0.0552  1.1692\n",
            "     76        0.0062        0.0673  1.1667\n",
            "     77        0.0089        0.0463  1.3829\n",
            "     78        0.0077        0.0382  1.4983\n",
            "     79        0.0071        0.0305  1.5083\n",
            "     80        0.0059        0.0303  1.2033\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.7min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1684\u001b[0m        \u001b[32m0.2115\u001b[0m  1.3127\n",
            "      2        \u001b[36m0.1526\u001b[0m        0.2129  1.2861\n",
            "      3        \u001b[36m0.1515\u001b[0m        \u001b[32m0.2068\u001b[0m  1.2752\n",
            "      4        \u001b[36m0.1496\u001b[0m        \u001b[32m0.1978\u001b[0m  1.2890\n",
            "      5        \u001b[36m0.1427\u001b[0m        \u001b[32m0.1773\u001b[0m  1.2763\n",
            "      6        \u001b[36m0.1314\u001b[0m        \u001b[32m0.1206\u001b[0m  1.2789\n",
            "      7        \u001b[36m0.1171\u001b[0m        0.1317  1.5115\n",
            "      8        \u001b[36m0.1090\u001b[0m        \u001b[32m0.0846\u001b[0m  1.6686\n",
            "      9        \u001b[36m0.0965\u001b[0m        \u001b[32m0.0739\u001b[0m  1.5833\n",
            "     10        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0625\u001b[0m  1.3049\n",
            "     11        \u001b[36m0.0452\u001b[0m        \u001b[32m0.0311\u001b[0m  1.2866\n",
            "     12        \u001b[36m0.0312\u001b[0m        0.0374  1.2973\n",
            "     13        \u001b[36m0.0268\u001b[0m        0.0433  1.2950\n",
            "     14        \u001b[36m0.0235\u001b[0m        0.0552  1.2863\n",
            "     15        \u001b[36m0.0215\u001b[0m        0.0466  1.3006\n",
            "     16        \u001b[36m0.0195\u001b[0m        \u001b[32m0.0254\u001b[0m  1.2962\n",
            "     17        \u001b[36m0.0167\u001b[0m        0.0309  1.5576\n",
            "     18        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0241\u001b[0m  1.6685\n",
            "     19        0.0166        0.0287  1.5450\n",
            "     20        \u001b[36m0.0157\u001b[0m        0.0252  1.2790\n",
            "     21        \u001b[36m0.0148\u001b[0m        0.0291  1.2783\n",
            "     22        0.0150        0.0281  1.2788\n",
            "     23        \u001b[36m0.0142\u001b[0m        0.0390  1.2696\n",
            "     24        \u001b[36m0.0130\u001b[0m        0.0401  1.2839\n",
            "     25        \u001b[36m0.0124\u001b[0m        0.0386  1.2596\n",
            "     26        0.0128        0.0348  1.2735\n",
            "     27        \u001b[36m0.0123\u001b[0m        0.0290  1.5276\n",
            "     28        0.0128        0.0318  1.6475\n",
            "     29        \u001b[36m0.0121\u001b[0m        0.0451  1.6243\n",
            "     30        0.0126        0.0368  1.3070\n",
            "     31        0.0142        0.0366  1.2728\n",
            "     32        0.0137        0.0419  1.2796\n",
            "     33        0.0122        0.0372  1.2690\n",
            "     34        0.0123        0.0378  1.2669\n",
            "     35        \u001b[36m0.0113\u001b[0m        0.0288  1.2983\n",
            "     36        \u001b[36m0.0108\u001b[0m        0.0275  1.2810\n",
            "     37        0.0114        0.0375  1.4728\n",
            "     38        0.0119        0.0349  1.6589\n",
            "     39        \u001b[36m0.0107\u001b[0m        0.0436  1.6125\n",
            "     40        \u001b[36m0.0107\u001b[0m        0.0418  1.2833\n",
            "     41        0.0126        0.0343  1.2698\n",
            "     42        0.0130        0.0384  1.3277\n",
            "     43        \u001b[36m0.0097\u001b[0m        0.0281  1.2834\n",
            "     44        0.0099        \u001b[32m0.0224\u001b[0m  1.2782\n",
            "     45        0.0139        0.0328  1.3014\n",
            "     46        0.0132        \u001b[32m0.0215\u001b[0m  1.3153\n",
            "     47        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0147\u001b[0m  1.4702\n",
            "     48        0.0102        0.0176  1.6638\n",
            "     49        0.0106        0.0193  1.5120\n",
            "     50        0.0096        0.0188  1.3069\n",
            "     51        \u001b[36m0.0075\u001b[0m        \u001b[32m0.0065\u001b[0m  1.2785\n",
            "     52        0.0107        0.0834  1.3074\n",
            "     53        0.0128        0.0094  1.3556\n",
            "     54        0.0085        0.0133  1.2930\n",
            "     55        0.0103        0.0193  1.2708\n",
            "     56        0.0097        0.0155  1.3112\n",
            "     57        0.0078        0.0104  1.6042\n",
            "     58        0.0091        0.0143  1.6276\n",
            "     59        \u001b[36m0.0075\u001b[0m        0.0145  1.4099\n",
            "     60        0.0091        0.0279  1.2885\n",
            "     61        0.0087        0.0276  1.2879\n",
            "     62        0.0091        0.0203  1.2648\n",
            "     63        0.0091        0.0206  1.2599\n",
            "     64        0.0093        0.0265  1.2785\n",
            "     65        0.0100        0.0140  1.2887\n",
            "     66        0.0091        0.0254  1.2815\n",
            "     67        \u001b[36m0.0073\u001b[0m        0.0302  1.6262\n",
            "     68        0.0076        0.0167  1.7294\n",
            "     69        0.0076        0.0136  1.3921\n",
            "     70        0.0104        0.0197  1.2766\n",
            "     71        0.0097        0.0182  1.2963\n",
            "     72        0.0073        0.0286  1.3179\n",
            "     73        0.0074        0.0433  1.2957\n",
            "     74        0.0105        0.0244  1.2797\n",
            "     75        0.0083        0.0236  1.2683\n",
            "     76        \u001b[36m0.0070\u001b[0m        0.0226  1.2743\n",
            "     77        \u001b[36m0.0060\u001b[0m        0.0175  1.6183\n",
            "     78        0.0069        0.0394  1.6658\n",
            "     79        0.0070        0.0338  1.3761\n",
            "     80        0.0074        0.0387  1.2928\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1592\u001b[0m        \u001b[32m0.2086\u001b[0m  1.2811\n",
            "      2        \u001b[36m0.1532\u001b[0m        \u001b[32m0.2051\u001b[0m  1.3198\n",
            "      3        \u001b[36m0.1509\u001b[0m        0.2079  1.2672\n",
            "      4        0.1517        \u001b[32m0.2029\u001b[0m  1.2674\n",
            "      5        \u001b[36m0.1498\u001b[0m        \u001b[32m0.2015\u001b[0m  1.2768\n",
            "      6        \u001b[36m0.1478\u001b[0m        \u001b[32m0.2011\u001b[0m  1.4192\n",
            "      7        \u001b[36m0.1459\u001b[0m        \u001b[32m0.1988\u001b[0m  1.5855\n",
            "      8        \u001b[36m0.1434\u001b[0m        \u001b[32m0.1949\u001b[0m  1.6920\n",
            "      9        \u001b[36m0.1427\u001b[0m        \u001b[32m0.1904\u001b[0m  1.2889\n",
            "     10        \u001b[36m0.1348\u001b[0m        \u001b[32m0.1673\u001b[0m  1.2925\n",
            "     11        \u001b[36m0.1299\u001b[0m        \u001b[32m0.1368\u001b[0m  1.3148\n",
            "     12        \u001b[36m0.1053\u001b[0m        0.1574  1.3111\n",
            "     13        0.1249        \u001b[32m0.1239\u001b[0m  1.2955\n",
            "     14        0.1075        \u001b[32m0.0709\u001b[0m  1.3212\n",
            "     15        \u001b[36m0.0777\u001b[0m        0.0778  1.2715\n",
            "     16        0.0859        \u001b[32m0.0345\u001b[0m  1.4623\n",
            "     17        \u001b[36m0.0378\u001b[0m        0.0600  1.6933\n",
            "     18        0.0378        0.0516  1.6541\n",
            "     19        \u001b[36m0.0342\u001b[0m        0.0647  1.3113\n",
            "     20        \u001b[36m0.0341\u001b[0m        0.0470  1.3018\n",
            "     21        \u001b[36m0.0336\u001b[0m        0.0590  1.2973\n",
            "     22        0.0338        0.0520  1.2676\n",
            "     23        \u001b[36m0.0317\u001b[0m        0.0506  1.2809\n",
            "     24        \u001b[36m0.0287\u001b[0m        0.0418  1.2878\n",
            "     25        \u001b[36m0.0238\u001b[0m        0.0582  1.2584\n",
            "     26        0.0239        0.0765  1.4880\n",
            "     27        \u001b[36m0.0227\u001b[0m        0.0807  1.6258\n",
            "     28        0.0239        0.0588  1.6336\n",
            "     29        \u001b[36m0.0221\u001b[0m        0.0580  1.2794\n",
            "     30        \u001b[36m0.0208\u001b[0m        0.0693  1.2693\n",
            "     31        \u001b[36m0.0205\u001b[0m        0.0554  1.2685\n",
            "     32        0.0214        0.0372  1.2783\n",
            "     33        0.0207        \u001b[32m0.0318\u001b[0m  1.2800\n",
            "     34        0.0212        \u001b[32m0.0274\u001b[0m  1.2864\n",
            "     35        \u001b[36m0.0194\u001b[0m        0.0444  1.2909\n",
            "     36        \u001b[36m0.0182\u001b[0m        0.0420  1.4437\n",
            "     37        \u001b[36m0.0180\u001b[0m        0.0319  1.6180\n",
            "     38        \u001b[36m0.0174\u001b[0m        0.0591  1.6381\n",
            "     39        0.0182        0.0571  1.2758\n",
            "     40        0.0177        0.0508  1.2850\n",
            "     41        \u001b[36m0.0171\u001b[0m        0.0514  1.3055\n",
            "     42        0.0181        0.0500  1.2613\n",
            "     43        \u001b[36m0.0162\u001b[0m        0.0456  1.2747\n",
            "     44        \u001b[36m0.0162\u001b[0m        0.0478  1.3009\n",
            "     45        0.0167        0.0459  1.2904\n",
            "     46        0.0199        0.0514  1.4373\n",
            "     47        0.0168        0.0385  1.6320\n",
            "     48        0.0174        0.0516  1.6414\n",
            "     49        \u001b[36m0.0152\u001b[0m        0.0411  1.3226\n",
            "     50        0.0168        0.0547  1.2824\n",
            "     51        0.0204        0.0519  1.2755\n",
            "     52        0.0164        0.0579  1.2912\n",
            "     53        0.0156        0.0454  1.2588\n",
            "     54        0.0154        0.0419  1.2913\n",
            "     55        0.0170        0.0490  1.2781\n",
            "     56        0.0194        0.0480  1.4600\n",
            "     57        0.0181        0.0494  1.6309\n",
            "     58        0.0160        0.0367  1.6892\n",
            "     59        \u001b[36m0.0146\u001b[0m        0.0292  1.2725\n",
            "     60        0.0150        0.0311  1.2807\n",
            "     61        0.0153        0.0372  1.2714\n",
            "     62        0.0156        0.0347  1.2877\n",
            "     63        0.0165        0.0411  1.2889\n",
            "     64        0.0177        0.0480  1.3116\n",
            "     65        0.0185        0.0478  1.2813\n",
            "     66        0.0171        0.0505  1.4315\n",
            "     67        0.0179        \u001b[32m0.0239\u001b[0m  1.6947\n",
            "     68        0.0212        0.0276  1.6821\n",
            "     69        0.0163        0.0321  1.2983\n",
            "     70        \u001b[36m0.0143\u001b[0m        0.0257  1.2899\n",
            "     71        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0232\u001b[0m  1.2709\n",
            "     72        0.0152        \u001b[32m0.0182\u001b[0m  1.2813\n",
            "     73        0.0141        0.0205  1.2879\n",
            "     74        0.0148        0.0224  1.2899\n",
            "     75        0.0142        0.0302  1.2715\n",
            "     76        0.0181        0.0742  1.3987\n",
            "     77        0.0219        0.0466  1.6245\n",
            "     78        0.0177        0.0362  1.7011\n",
            "     79        0.0148        0.0295  1.3119\n",
            "     80        \u001b[36m0.0140\u001b[0m        0.0319  1.3174\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.2034\u001b[0m        \u001b[32m0.2237\u001b[0m  0.7001\n",
            "      2        \u001b[36m0.1670\u001b[0m        \u001b[32m0.2172\u001b[0m  0.6586\n",
            "      3        \u001b[36m0.1565\u001b[0m        \u001b[32m0.2018\u001b[0m  0.6403\n",
            "      4        \u001b[36m0.1488\u001b[0m        \u001b[32m0.1575\u001b[0m  0.6510\n",
            "      5        \u001b[36m0.1404\u001b[0m        0.1729  0.6398\n",
            "      6        \u001b[36m0.1400\u001b[0m        \u001b[32m0.1057\u001b[0m  0.6470\n",
            "      7        \u001b[36m0.1253\u001b[0m        \u001b[32m0.1057\u001b[0m  0.6534\n",
            "      8        \u001b[36m0.1229\u001b[0m        0.1218  0.6490\n",
            "      9        \u001b[36m0.1002\u001b[0m        \u001b[32m0.0516\u001b[0m  0.6595\n",
            "     10        \u001b[36m0.0536\u001b[0m        0.0531  0.6526\n",
            "     11        \u001b[36m0.0252\u001b[0m        \u001b[32m0.0308\u001b[0m  0.7309\n",
            "     12        \u001b[36m0.0200\u001b[0m        0.0522  0.8792\n",
            "     13        \u001b[36m0.0172\u001b[0m        0.0402  0.8517\n",
            "     14        \u001b[36m0.0151\u001b[0m        0.0400  0.9042\n",
            "     15        \u001b[36m0.0140\u001b[0m        0.0351  0.8177\n",
            "     16        \u001b[36m0.0129\u001b[0m        0.0346  0.6511\n",
            "     17        \u001b[36m0.0122\u001b[0m        0.0313  0.6463\n",
            "     18        0.0123        \u001b[32m0.0286\u001b[0m  0.6435\n",
            "     19        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0278\u001b[0m  0.6427\n",
            "     20        \u001b[36m0.0107\u001b[0m        0.0303  0.6457\n",
            "     21        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0266\u001b[0m  0.6410\n",
            "     22        \u001b[36m0.0098\u001b[0m        0.0292  0.6484\n",
            "     23        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0250\u001b[0m  0.6474\n",
            "     24        \u001b[36m0.0091\u001b[0m        0.0261  0.6622\n",
            "     25        \u001b[36m0.0089\u001b[0m        \u001b[32m0.0246\u001b[0m  0.6411\n",
            "     26        0.0099        \u001b[32m0.0210\u001b[0m  0.6440\n",
            "     27        \u001b[36m0.0079\u001b[0m        \u001b[32m0.0183\u001b[0m  0.6431\n",
            "     28        \u001b[36m0.0071\u001b[0m        0.0258  0.6639\n",
            "     29        0.0075        0.0280  0.6510\n",
            "     30        \u001b[36m0.0070\u001b[0m        0.0186  0.6549\n",
            "     31        \u001b[36m0.0064\u001b[0m        \u001b[32m0.0170\u001b[0m  0.8593\n",
            "     32        \u001b[36m0.0061\u001b[0m        0.0181  0.8579\n",
            "     33        \u001b[36m0.0057\u001b[0m        0.0282  0.8612\n",
            "     34        \u001b[36m0.0055\u001b[0m        0.0279  0.9044\n",
            "     35        \u001b[36m0.0054\u001b[0m        0.0314  0.7327\n",
            "     36        \u001b[36m0.0053\u001b[0m        0.0226  0.6614\n",
            "     37        \u001b[36m0.0046\u001b[0m        0.0215  0.6369\n",
            "     38        0.0047        0.0228  0.6657\n",
            "     39        \u001b[36m0.0044\u001b[0m        0.0290  0.6547\n",
            "     40        \u001b[36m0.0043\u001b[0m        0.0322  0.6536\n",
            "     41        0.0044        0.0294  0.6650\n",
            "     42        0.0044        0.0237  0.6997\n",
            "     43        \u001b[36m0.0041\u001b[0m        0.0241  0.6503\n",
            "     44        \u001b[36m0.0038\u001b[0m        0.0287  0.6720\n",
            "     45        0.0039        0.0308  0.6503\n",
            "     46        0.0042        0.0258  0.6371\n",
            "     47        \u001b[36m0.0038\u001b[0m        0.0227  0.6553\n",
            "     48        \u001b[36m0.0036\u001b[0m        0.0282  0.6554\n",
            "     49        0.0039        0.0292  0.6702\n",
            "     50        0.0039        0.0234  0.8143\n",
            "     51        \u001b[36m0.0035\u001b[0m        0.0210  0.8779\n",
            "     52        0.0035        0.0215  0.8470\n",
            "     53        \u001b[36m0.0034\u001b[0m        0.0287  0.8866\n",
            "     54        0.0038        0.0241  0.8094\n",
            "     55        0.0034        0.0181  0.6368\n",
            "     56        \u001b[36m0.0032\u001b[0m        0.0188  0.6435\n",
            "     57        \u001b[36m0.0031\u001b[0m        0.0213  0.6676\n",
            "     58        0.0034        0.0265  0.6543\n",
            "     59        0.0035        0.0179  0.6548\n",
            "     60        \u001b[36m0.0030\u001b[0m        0.0194  0.6475\n",
            "     61        0.0030        \u001b[32m0.0152\u001b[0m  0.6536\n",
            "     62        0.0030        0.0208  0.6476\n",
            "     63        \u001b[36m0.0030\u001b[0m        0.0200  0.6729\n",
            "     64        0.0030        0.0162  0.6676\n",
            "     65        \u001b[36m0.0030\u001b[0m        \u001b[32m0.0151\u001b[0m  0.6479\n",
            "     66        \u001b[36m0.0027\u001b[0m        0.0190  0.6478\n",
            "     67        0.0030        0.0160  0.6475\n",
            "     68        0.0030        0.0233  0.6422\n",
            "     69        0.0031        0.0174  0.7065\n",
            "     70        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0117\u001b[0m  0.8629\n",
            "     71        \u001b[36m0.0024\u001b[0m        \u001b[32m0.0109\u001b[0m  0.8625\n",
            "     72        \u001b[36m0.0021\u001b[0m        0.0125  0.8792\n",
            "     73        0.0022        0.0144  0.8498\n",
            "     74        0.0025        0.0149  0.6486\n",
            "     75        0.0025        0.0133  0.6387\n",
            "     76        \u001b[36m0.0020\u001b[0m        0.0152  0.6400\n",
            "     77        \u001b[36m0.0019\u001b[0m        0.0147  0.6515\n",
            "     78        0.0025        0.0166  0.6398\n",
            "     79        \u001b[36m0.0019\u001b[0m        0.0158  0.6406\n",
            "     80        0.0023        0.0150  0.6363\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  56.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.2197\u001b[0m        \u001b[32m0.1953\u001b[0m  0.6380\n",
            "      2        \u001b[36m0.1598\u001b[0m        \u001b[32m0.1886\u001b[0m  0.6686\n",
            "      3        \u001b[36m0.1460\u001b[0m        \u001b[32m0.1815\u001b[0m  0.6651\n",
            "      4        \u001b[36m0.1351\u001b[0m        \u001b[32m0.1510\u001b[0m  0.6513\n",
            "      5        \u001b[36m0.1204\u001b[0m        \u001b[32m0.1181\u001b[0m  0.6875\n",
            "      6        \u001b[36m0.1022\u001b[0m        0.1307  0.6495\n",
            "      7        \u001b[36m0.0614\u001b[0m        \u001b[32m0.0659\u001b[0m  0.6545\n",
            "      8        \u001b[36m0.0408\u001b[0m        \u001b[32m0.0429\u001b[0m  0.7229\n",
            "      9        \u001b[36m0.0240\u001b[0m        \u001b[32m0.0233\u001b[0m  0.8489\n",
            "     10        \u001b[36m0.0205\u001b[0m        \u001b[32m0.0217\u001b[0m  0.8581\n",
            "     11        \u001b[36m0.0197\u001b[0m        0.0254  0.8582\n",
            "     12        \u001b[36m0.0189\u001b[0m        0.0342  0.8967\n",
            "     13        \u001b[36m0.0178\u001b[0m        0.0328  0.6872\n",
            "     14        \u001b[36m0.0161\u001b[0m        0.0261  0.6495\n",
            "     15        \u001b[36m0.0147\u001b[0m        0.0329  0.6440\n",
            "     16        \u001b[36m0.0140\u001b[0m        0.0319  0.6500\n",
            "     17        \u001b[36m0.0134\u001b[0m        0.0283  0.6337\n",
            "     18        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0198\u001b[0m  0.6498\n",
            "     19        \u001b[36m0.0120\u001b[0m        0.0297  0.6665\n",
            "     20        0.0124        0.0303  0.6540\n",
            "     21        \u001b[36m0.0116\u001b[0m        0.0288  0.6585\n",
            "     22        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0176\u001b[0m  0.6426\n",
            "     23        \u001b[36m0.0085\u001b[0m        \u001b[32m0.0164\u001b[0m  0.6418\n",
            "     24        \u001b[36m0.0084\u001b[0m        \u001b[32m0.0137\u001b[0m  0.6519\n",
            "     25        \u001b[36m0.0076\u001b[0m        0.0142  0.6457\n",
            "     26        \u001b[36m0.0073\u001b[0m        \u001b[32m0.0095\u001b[0m  0.6274\n",
            "     27        \u001b[36m0.0069\u001b[0m        \u001b[32m0.0091\u001b[0m  0.6419\n",
            "     28        \u001b[36m0.0065\u001b[0m        \u001b[32m0.0069\u001b[0m  0.7842\n",
            "     29        \u001b[36m0.0061\u001b[0m        0.0071  0.8857\n",
            "     30        \u001b[36m0.0055\u001b[0m        0.0130  0.8470\n",
            "     31        \u001b[36m0.0053\u001b[0m        0.0179  0.8816\n",
            "     32        \u001b[36m0.0052\u001b[0m        0.0202  0.8620\n",
            "     33        \u001b[36m0.0052\u001b[0m        0.0249  0.6417\n",
            "     34        0.0054        0.0284  0.6551\n",
            "     35        0.0056        0.0415  0.6543\n",
            "     36        0.0056        0.0476  0.6559\n",
            "     37        0.0055        0.0491  0.6563\n",
            "     38        0.0053        0.0515  0.6505\n",
            "     39        0.0054        0.0466  0.6484\n",
            "     40        0.0054        0.0483  0.6370\n",
            "     41        0.0054        0.0469  0.6564\n",
            "     42        0.0053        0.0465  0.6605\n",
            "     43        0.0052        0.0438  0.6700\n",
            "     44        0.0053        0.0463  0.6621\n",
            "     45        \u001b[36m0.0050\u001b[0m        0.0417  0.6597\n",
            "     46        \u001b[36m0.0048\u001b[0m        0.0453  0.6604\n",
            "     47        0.0050        0.0418  0.6600\n",
            "     48        0.0051        0.0360  0.8838\n",
            "     49        0.0051        0.0448  0.8705\n",
            "     50        0.0061        0.0361  0.8481\n",
            "     51        0.0050        0.0428  0.9085\n",
            "     52        \u001b[36m0.0046\u001b[0m        0.0396  0.7673\n",
            "     53        0.0048        0.0390  0.6431\n",
            "     54        0.0051        0.0403  0.6547\n",
            "     55        0.0063        0.0442  0.6846\n",
            "     56        0.0050        0.0312  0.6486\n",
            "     57        \u001b[36m0.0044\u001b[0m        0.0235  0.6538\n",
            "     58        0.0045        0.0234  0.6495\n",
            "     59        \u001b[36m0.0043\u001b[0m        0.0212  0.6556\n",
            "     60        0.0044        0.0222  0.6500\n",
            "     61        \u001b[36m0.0042\u001b[0m        0.0221  0.6450\n",
            "     62        \u001b[36m0.0041\u001b[0m        0.0188  0.6759\n",
            "     63        \u001b[36m0.0036\u001b[0m        0.0177  0.6499\n",
            "     64        \u001b[36m0.0035\u001b[0m        0.0118  0.6458\n",
            "     65        0.0041        0.0153  0.6701\n",
            "     66        \u001b[36m0.0033\u001b[0m        0.0099  0.6773\n",
            "     67        0.0036        0.0091  0.7502\n",
            "     68        \u001b[36m0.0028\u001b[0m        0.0085  0.8483\n",
            "     69        0.0035        0.0116  0.8545\n",
            "     70        0.0028        0.0073  0.8977\n",
            "     71        0.0030        0.0095  0.8144\n",
            "     72        0.0028        \u001b[32m0.0066\u001b[0m  0.6530\n",
            "     73        0.0032        0.0152  0.6533\n",
            "     74        0.0036        0.0147  0.6416\n",
            "     75        0.0035        0.0153  0.6417\n",
            "     76        0.0064        0.0128  0.6447\n",
            "     77        0.0080        0.0201  0.6392\n",
            "     78        0.0028        0.0095  0.6559\n",
            "     79        \u001b[36m0.0025\u001b[0m        0.0068  0.6565\n",
            "     80        \u001b[36m0.0025\u001b[0m        0.0074  0.6435\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  56.5s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.2271\u001b[0m        \u001b[32m0.2152\u001b[0m  0.7436\n",
            "      2        \u001b[36m0.1815\u001b[0m        \u001b[32m0.2082\u001b[0m  0.6988\n",
            "      3        \u001b[36m0.1673\u001b[0m        \u001b[32m0.1993\u001b[0m  0.6917\n",
            "      4        \u001b[36m0.1523\u001b[0m        \u001b[32m0.1805\u001b[0m  0.6840\n",
            "      5        \u001b[36m0.1446\u001b[0m        \u001b[32m0.1697\u001b[0m  0.6896\n",
            "      6        \u001b[36m0.1344\u001b[0m        \u001b[32m0.1572\u001b[0m  0.8801\n",
            "      7        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1054\u001b[0m  0.9261\n",
            "      8        \u001b[36m0.1227\u001b[0m        \u001b[32m0.0888\u001b[0m  0.9659\n",
            "      9        \u001b[36m0.0882\u001b[0m        0.0999  0.9655\n",
            "     10        \u001b[36m0.0657\u001b[0m        \u001b[32m0.0882\u001b[0m  0.8023\n",
            "     11        \u001b[36m0.0462\u001b[0m        0.1154  0.7125\n",
            "     12        \u001b[36m0.0374\u001b[0m        \u001b[32m0.0574\u001b[0m  0.6983\n",
            "     13        \u001b[36m0.0317\u001b[0m        \u001b[32m0.0432\u001b[0m  0.6887\n",
            "     14        \u001b[36m0.0288\u001b[0m        \u001b[32m0.0365\u001b[0m  0.6830\n",
            "     15        \u001b[36m0.0262\u001b[0m        0.0554  0.6770\n",
            "     16        \u001b[36m0.0244\u001b[0m        0.0547  0.6763\n",
            "     17        \u001b[36m0.0237\u001b[0m        0.0477  0.6942\n",
            "     18        \u001b[36m0.0227\u001b[0m        0.0493  0.6918\n",
            "     19        \u001b[36m0.0224\u001b[0m        0.0524  0.6846\n",
            "     20        \u001b[36m0.0211\u001b[0m        0.0505  0.6941\n",
            "     21        \u001b[36m0.0205\u001b[0m        0.0425  0.6939\n",
            "     22        \u001b[36m0.0195\u001b[0m        0.0462  0.6862\n",
            "     23        \u001b[36m0.0191\u001b[0m        0.0477  0.6965\n",
            "     24        \u001b[36m0.0183\u001b[0m        0.0484  0.7410\n",
            "     25        \u001b[36m0.0178\u001b[0m        0.0526  0.9208\n",
            "     26        \u001b[36m0.0165\u001b[0m        0.0513  0.9499\n",
            "     27        \u001b[36m0.0165\u001b[0m        0.0430  0.9166\n",
            "     28        \u001b[36m0.0163\u001b[0m        0.0598  0.9458\n",
            "     29        \u001b[36m0.0158\u001b[0m        0.0495  0.6812\n",
            "     30        \u001b[36m0.0149\u001b[0m        0.0489  0.6696\n",
            "     31        \u001b[36m0.0140\u001b[0m        0.0576  0.6834\n",
            "     32        0.0141        0.0589  0.6942\n",
            "     33        0.0141        0.0556  0.6794\n",
            "     34        \u001b[36m0.0136\u001b[0m        0.0521  0.6771\n",
            "     35        \u001b[36m0.0132\u001b[0m        0.0494  0.7034\n",
            "     36        \u001b[36m0.0130\u001b[0m        0.0467  0.6893\n",
            "     37        \u001b[36m0.0119\u001b[0m        0.0459  0.6858\n",
            "     38        0.0127        \u001b[32m0.0355\u001b[0m  0.6899\n",
            "     39        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0288\u001b[0m  0.6913\n",
            "     40        \u001b[36m0.0104\u001b[0m        0.0316  0.6827\n",
            "     41        0.0108        0.0351  0.6832\n",
            "     42        0.0104        0.0299  0.6902\n",
            "     43        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0264\u001b[0m  0.8453\n",
            "     44        0.0100        0.0328  0.9074\n",
            "     45        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0262\u001b[0m  0.9059\n",
            "     46        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0236\u001b[0m  0.9649\n",
            "     47        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0194\u001b[0m  0.8174\n",
            "     48        \u001b[36m0.0083\u001b[0m        0.0274  0.6993\n",
            "     49        0.0085        0.0268  0.6938\n",
            "     50        \u001b[36m0.0080\u001b[0m        0.0239  0.7171\n",
            "     51        \u001b[36m0.0072\u001b[0m        0.0269  0.6927\n",
            "     52        0.0073        0.0297  0.6841\n",
            "     53        \u001b[36m0.0070\u001b[0m        0.0273  0.6947\n",
            "     54        0.0074        0.0263  0.6827\n",
            "     55        \u001b[36m0.0070\u001b[0m        0.0278  0.6788\n",
            "     56        0.0075        0.0375  0.7112\n",
            "     57        0.0078        0.0303  0.7044\n",
            "     58        \u001b[36m0.0065\u001b[0m        0.0272  0.6916\n",
            "     59        0.0069        \u001b[32m0.0169\u001b[0m  0.6923\n",
            "     60        \u001b[36m0.0064\u001b[0m        0.0208  0.6965\n",
            "     61        0.0066        0.0235  0.7591\n",
            "     62        0.0071        0.0196  0.9119\n",
            "     63        0.0065        0.0208  0.9426\n",
            "     64        \u001b[36m0.0057\u001b[0m        0.0232  0.9454\n",
            "     65        0.0067        0.0249  0.9027\n",
            "     66        0.0068        \u001b[32m0.0126\u001b[0m  0.6992\n",
            "     67        \u001b[36m0.0054\u001b[0m        0.0154  0.6791\n",
            "     68        \u001b[36m0.0048\u001b[0m        \u001b[32m0.0079\u001b[0m  0.7012\n",
            "     69        0.0066        0.0116  0.6895\n",
            "     70        0.0078        0.0195  0.7125\n",
            "     71        0.0058        0.0135  0.6917\n",
            "     72        0.0059        0.0150  0.6997\n",
            "     73        0.0058        0.0234  0.7055\n",
            "     74        0.0064        0.0177  0.6848\n",
            "     75        0.0059        0.0217  0.6876\n",
            "     76        0.0063        0.0134  0.6931\n",
            "     77        0.0054        0.0167  0.6999\n",
            "     78        0.0052        0.0160  0.6933\n",
            "     79        0.0050        0.0116  0.7012\n",
            "     80        0.0049        0.0125  0.9070\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.0min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1752\u001b[0m        \u001b[32m0.2244\u001b[0m  0.9414\n",
            "      2        \u001b[36m0.1613\u001b[0m        0.2251  0.9446\n",
            "      3        \u001b[36m0.1563\u001b[0m        \u001b[32m0.2155\u001b[0m  0.8982\n",
            "      4        \u001b[36m0.1500\u001b[0m        \u001b[32m0.2090\u001b[0m  0.6972\n",
            "      5        \u001b[36m0.1421\u001b[0m        \u001b[32m0.1884\u001b[0m  0.6862\n",
            "      6        \u001b[36m0.1320\u001b[0m        \u001b[32m0.1563\u001b[0m  0.6966\n",
            "      7        \u001b[36m0.1230\u001b[0m        0.1634  0.6848\n",
            "      8        \u001b[36m0.1050\u001b[0m        \u001b[32m0.0722\u001b[0m  0.7024\n",
            "      9        \u001b[36m0.0857\u001b[0m        \u001b[32m0.0342\u001b[0m  0.7125\n",
            "     10        \u001b[36m0.0535\u001b[0m        0.0960  0.6935\n",
            "     11        \u001b[36m0.0441\u001b[0m        \u001b[32m0.0268\u001b[0m  0.7160\n",
            "     12        \u001b[36m0.0388\u001b[0m        0.0668  0.6924\n",
            "     13        \u001b[36m0.0332\u001b[0m        0.0381  0.6909\n",
            "     14        \u001b[36m0.0302\u001b[0m        0.0319  0.7017\n",
            "     15        \u001b[36m0.0287\u001b[0m        0.0395  0.6925\n",
            "     16        \u001b[36m0.0277\u001b[0m        0.0393  0.6928\n",
            "     17        \u001b[36m0.0265\u001b[0m        0.0440  0.7157\n",
            "     18        \u001b[36m0.0254\u001b[0m        0.0410  0.9457\n",
            "     19        \u001b[36m0.0242\u001b[0m        0.0383  0.9283\n",
            "     20        \u001b[36m0.0225\u001b[0m        0.0446  0.9336\n",
            "     21        \u001b[36m0.0199\u001b[0m        0.0497  0.9601\n",
            "     22        \u001b[36m0.0189\u001b[0m        0.0372  0.7028\n",
            "     23        \u001b[36m0.0178\u001b[0m        0.0410  0.6990\n",
            "     24        \u001b[36m0.0175\u001b[0m        0.0371  0.7038\n",
            "     25        \u001b[36m0.0172\u001b[0m        0.0391  0.6857\n",
            "     26        \u001b[36m0.0168\u001b[0m        0.0372  0.6931\n",
            "     27        \u001b[36m0.0161\u001b[0m        0.0361  0.7039\n",
            "     28        0.0166        0.0364  0.6904\n",
            "     29        0.0161        0.0314  0.6894\n",
            "     30        \u001b[36m0.0160\u001b[0m        0.0276  0.6850\n",
            "     31        \u001b[36m0.0153\u001b[0m        0.0325  0.6745\n",
            "     32        \u001b[36m0.0151\u001b[0m        0.0296  0.7246\n",
            "     33        \u001b[36m0.0149\u001b[0m        0.0305  0.6981\n",
            "     34        \u001b[36m0.0142\u001b[0m        0.0379  0.6951\n",
            "     35        \u001b[36m0.0140\u001b[0m        0.0359  0.6828\n",
            "     36        0.0144        0.0344  0.8621\n",
            "     37        \u001b[36m0.0134\u001b[0m        0.0502  1.0220\n",
            "     38        0.0147        0.0387  0.9220\n",
            "     39        0.0140        0.0490  0.9916\n",
            "     40        0.0143        0.0403  0.7503\n",
            "     41        0.0139        0.0403  0.6926\n",
            "     42        0.0138        0.0455  0.6880\n",
            "     43        0.0148        0.0293  0.7017\n",
            "     44        0.0135        0.0296  0.6836\n",
            "     45        \u001b[36m0.0129\u001b[0m        0.0405  0.6873\n",
            "     46        0.0129        0.0336  0.7098\n",
            "     47        \u001b[36m0.0125\u001b[0m        0.0441  0.6927\n",
            "     48        0.0131        0.0478  0.7061\n",
            "     49        0.0130        0.0452  0.7196\n",
            "     50        0.0131        0.0448  0.6817\n",
            "     51        0.0132        0.0483  0.7125\n",
            "     52        0.0129        0.0433  0.6851\n",
            "     53        0.0131        0.0472  0.6940\n",
            "     54        0.0127        0.0420  0.8340\n",
            "     55        0.0136        0.0344  0.9343\n",
            "     56        \u001b[36m0.0124\u001b[0m        0.0397  0.9304\n",
            "     57        0.0128        0.0414  0.9759\n",
            "     58        \u001b[36m0.0124\u001b[0m        0.0331  0.8545\n",
            "     59        \u001b[36m0.0121\u001b[0m        0.0325  0.7182\n",
            "     60        0.0130        0.0465  0.7105\n",
            "     61        0.0132        0.0269  0.7045\n",
            "     62        0.0123        0.0412  0.7308\n",
            "     63        0.0143        0.0422  0.7012\n",
            "     64        \u001b[36m0.0118\u001b[0m        0.0429  0.7190\n",
            "     65        0.0120        0.0391  0.7000\n",
            "     66        0.0121        0.0411  0.7226\n",
            "     67        0.0124        0.0364  0.7049\n",
            "     68        0.0133        0.0401  0.7061\n",
            "     69        0.0122        0.0428  0.6918\n",
            "     70        0.0118        0.0417  0.6963\n",
            "     71        0.0121        0.0507  0.7020\n",
            "     72        0.0123        0.0392  0.8136\n",
            "     73        0.0123        0.0439  0.9291\n",
            "     74        0.0126        0.0367  0.9149\n",
            "     75        0.0125        0.0455  0.9725\n",
            "     76        0.0118        0.0423  0.8733\n",
            "     77        0.0120        0.0374  0.6897\n",
            "     78        0.0122        0.0339  0.6719\n",
            "     79        \u001b[36m0.0116\u001b[0m        0.0324  0.7280\n",
            "     80        0.0127        0.0405  0.7268\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.0min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1610\u001b[0m        \u001b[32m0.2081\u001b[0m  0.6536\n",
            "      2        \u001b[36m0.1404\u001b[0m        \u001b[32m0.1868\u001b[0m  0.6624\n",
            "      3        \u001b[36m0.1265\u001b[0m        \u001b[32m0.1460\u001b[0m  0.6417\n",
            "      4        \u001b[36m0.1124\u001b[0m        \u001b[32m0.1088\u001b[0m  0.6690\n",
            "      5        \u001b[36m0.1026\u001b[0m        \u001b[32m0.0822\u001b[0m  0.6523\n",
            "      6        0.1082        \u001b[32m0.0643\u001b[0m  0.6428\n",
            "      7        \u001b[36m0.0903\u001b[0m        \u001b[32m0.0404\u001b[0m  0.6480\n",
            "      8        \u001b[36m0.0375\u001b[0m        0.0894  0.6693\n",
            "      9        \u001b[36m0.0258\u001b[0m        \u001b[32m0.0227\u001b[0m  0.6510\n",
            "     10        \u001b[36m0.0181\u001b[0m        0.0270  0.6778\n",
            "     11        \u001b[36m0.0159\u001b[0m        0.0389  0.8954\n",
            "     12        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0199\u001b[0m  0.8577\n",
            "     13        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0173\u001b[0m  0.8348\n",
            "     14        \u001b[36m0.0131\u001b[0m        0.0303  0.8991\n",
            "     15        \u001b[36m0.0117\u001b[0m        0.0327  0.7420\n",
            "     16        0.0117        \u001b[32m0.0160\u001b[0m  0.6423\n",
            "     17        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0142\u001b[0m  0.6529\n",
            "     18        \u001b[36m0.0105\u001b[0m        0.0216  0.6442\n",
            "     19        \u001b[36m0.0097\u001b[0m        0.0316  0.6525\n",
            "     20        0.0098        0.0310  0.6361\n",
            "     21        0.0098        \u001b[32m0.0137\u001b[0m  0.6333\n",
            "     22        0.0106        \u001b[32m0.0089\u001b[0m  0.6649\n",
            "     23        \u001b[36m0.0087\u001b[0m        0.0124  0.6495\n",
            "     24        \u001b[36m0.0075\u001b[0m        0.0159  0.6488\n",
            "     25        \u001b[36m0.0073\u001b[0m        0.0201  0.6778\n",
            "     26        0.0074        0.0166  0.6445\n",
            "     27        \u001b[36m0.0070\u001b[0m        0.0191  0.6451\n",
            "     28        \u001b[36m0.0068\u001b[0m        0.0195  0.6535\n",
            "     29        \u001b[36m0.0066\u001b[0m        0.0239  0.6459\n",
            "     30        \u001b[36m0.0065\u001b[0m        0.0218  0.7412\n",
            "     31        \u001b[36m0.0062\u001b[0m        0.0151  0.8584\n",
            "     32        \u001b[36m0.0059\u001b[0m        0.0131  0.8790\n",
            "     33        0.0060        0.0210  0.8930\n",
            "     34        \u001b[36m0.0049\u001b[0m        0.0299  0.8950\n",
            "     35        0.0049        0.0472  0.6471\n",
            "     36        0.0052        0.0503  0.6572\n",
            "     37        0.0056        0.0491  0.6630\n",
            "     38        0.0054        0.0560  0.6600\n",
            "     39        0.0057        0.0615  0.6486\n",
            "     40        0.0053        0.0588  0.6341\n",
            "     41        0.0049        0.0598  0.6533\n",
            "     42        0.0052        0.0635  0.6529\n",
            "     43        0.0058        0.0579  0.6424\n",
            "     44        0.0053        0.0525  0.6531\n",
            "     45        0.0049        0.0517  0.6502\n",
            "     46        0.0049        0.0484  0.6371\n",
            "     47        0.0054        0.0309  0.6469\n",
            "     48        \u001b[36m0.0048\u001b[0m        0.0379  0.6264\n",
            "     49        \u001b[36m0.0043\u001b[0m        0.0356  0.6395\n",
            "     50        0.0045        0.0410  0.7767\n",
            "     51        0.0044        0.0442  0.8554\n",
            "     52        0.0045        0.0319  0.8353\n",
            "     53        \u001b[36m0.0040\u001b[0m        0.0427  0.9034\n",
            "     54        0.0043        0.0372  0.8367\n",
            "     55        0.0041        0.0506  0.6458\n",
            "     56        0.0043        0.0397  0.6442\n",
            "     57        0.0043        0.0403  0.6388\n",
            "     58        0.0043        0.0312  0.6465\n",
            "     59        0.0043        0.0305  0.6374\n",
            "     60        \u001b[36m0.0036\u001b[0m        0.0314  0.6540\n",
            "     61        0.0040        0.0308  0.6541\n",
            "     62        0.0039        0.0355  0.6423\n",
            "     63        0.0038        0.0353  0.6450\n",
            "     64        0.0039        0.0346  0.6395\n",
            "     65        0.0041        0.0327  0.6544\n",
            "     66        0.0038        0.0277  0.6653\n",
            "     67        \u001b[36m0.0035\u001b[0m        0.0251  0.6571\n",
            "     68        0.0036        0.0243  0.6532\n",
            "     69        \u001b[36m0.0034\u001b[0m        0.0241  0.6453\n",
            "     70        0.0035        0.0186  0.8388\n",
            "     71        0.0037        0.0097  0.8530\n",
            "     72        0.0039        0.0176  0.8491\n",
            "     73        0.0038        0.0182  0.8988\n",
            "     74        0.0040        0.0152  0.7503\n",
            "     75        \u001b[36m0.0031\u001b[0m        0.0156  0.6461\n",
            "     76        \u001b[36m0.0030\u001b[0m        0.0179  0.6546\n",
            "     77        0.0031        0.0122  0.6725\n",
            "     78        \u001b[36m0.0026\u001b[0m        0.0178  0.6426\n",
            "     79        0.0031        0.0226  0.6396\n",
            "     80        0.0039        0.0127  0.6342\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1580\u001b[0m        \u001b[32m0.2274\u001b[0m  0.6416\n",
            "      2        \u001b[36m0.1525\u001b[0m        \u001b[32m0.2182\u001b[0m  0.6579\n",
            "      3        \u001b[36m0.1364\u001b[0m        \u001b[32m0.1809\u001b[0m  0.6508\n",
            "      4        \u001b[36m0.1232\u001b[0m        \u001b[32m0.1758\u001b[0m  0.6304\n",
            "      5        \u001b[36m0.1160\u001b[0m        \u001b[32m0.1161\u001b[0m  0.6534\n",
            "      6        \u001b[36m0.1022\u001b[0m        \u001b[32m0.0828\u001b[0m  0.6368\n",
            "      7        \u001b[36m0.0912\u001b[0m        0.1053  0.6545\n",
            "      8        \u001b[36m0.0644\u001b[0m        \u001b[32m0.0518\u001b[0m  0.6437\n",
            "      9        \u001b[36m0.0446\u001b[0m        0.0761  0.7828\n",
            "     10        \u001b[36m0.0242\u001b[0m        \u001b[32m0.0199\u001b[0m  0.8502\n",
            "     11        \u001b[36m0.0197\u001b[0m        0.0290  0.8411\n",
            "     12        \u001b[36m0.0191\u001b[0m        0.0410  0.8917\n",
            "     13        \u001b[36m0.0166\u001b[0m        0.0536  0.8080\n",
            "     14        \u001b[36m0.0156\u001b[0m        0.0267  0.6877\n",
            "     15        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0193\u001b[0m  0.6533\n",
            "     16        \u001b[36m0.0132\u001b[0m        0.0220  0.6993\n",
            "     17        0.0136        0.0392  0.6797\n",
            "     18        \u001b[36m0.0119\u001b[0m        0.0498  0.6607\n",
            "     19        0.0125        0.0366  0.6593\n",
            "     20        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0185\u001b[0m  0.6518\n",
            "     21        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0134\u001b[0m  0.6586\n",
            "     22        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0133\u001b[0m  0.6564\n",
            "     23        \u001b[36m0.0093\u001b[0m        0.0165  0.6627\n",
            "     24        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0132\u001b[0m  0.6543\n",
            "     25        \u001b[36m0.0081\u001b[0m        \u001b[32m0.0131\u001b[0m  0.6645\n",
            "     26        0.0085        \u001b[32m0.0070\u001b[0m  0.6530\n",
            "     27        \u001b[36m0.0070\u001b[0m        0.0158  0.6685\n",
            "     28        0.0071        0.0290  0.7669\n",
            "     29        \u001b[36m0.0065\u001b[0m        0.0564  0.8447\n",
            "     30        0.0074        0.0531  0.8823\n",
            "     31        0.0088        0.0500  0.9035\n",
            "     32        0.0072        0.0462  0.8717\n",
            "     33        0.0071        0.0446  0.6532\n",
            "     34        0.0066        0.0398  0.6429\n",
            "     35        0.0069        0.0419  0.6461\n",
            "     36        0.0068        0.0440  0.6351\n",
            "     37        \u001b[36m0.0064\u001b[0m        0.0530  0.6518\n",
            "     38        \u001b[36m0.0063\u001b[0m        0.0512  0.6517\n",
            "     39        0.0063        0.0600  0.6510\n",
            "     40        \u001b[36m0.0062\u001b[0m        0.0440  0.6521\n",
            "     41        \u001b[36m0.0060\u001b[0m        0.0506  0.6532\n",
            "     42        0.0069        0.0211  0.6459\n",
            "     43        \u001b[36m0.0056\u001b[0m        0.0256  0.6947\n",
            "     44        \u001b[36m0.0055\u001b[0m        0.0241  0.6433\n",
            "     45        \u001b[36m0.0051\u001b[0m        0.0310  0.6358\n",
            "     46        0.0055        0.0147  0.6585\n",
            "     47        \u001b[36m0.0051\u001b[0m        0.0208  0.6470\n",
            "     48        0.0060        0.0513  0.8601\n",
            "     49        0.0052        0.0094  0.8634\n",
            "     50        \u001b[36m0.0045\u001b[0m        0.0128  0.8529\n",
            "     51        \u001b[36m0.0042\u001b[0m        \u001b[32m0.0063\u001b[0m  0.8832\n",
            "     52        \u001b[36m0.0035\u001b[0m        \u001b[32m0.0046\u001b[0m  0.7421\n",
            "     53        \u001b[36m0.0027\u001b[0m        \u001b[32m0.0041\u001b[0m  0.6707\n",
            "     54        \u001b[36m0.0020\u001b[0m        0.0075  0.6474\n",
            "     55        0.0021        \u001b[32m0.0032\u001b[0m  0.6359\n",
            "     56        0.0038        0.0033  0.6496\n",
            "     57        0.0038        0.0129  0.6628\n",
            "     58        0.0023        0.0038  0.6523\n",
            "     59        0.0021        0.0053  0.6577\n",
            "     60        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0025\u001b[0m  0.6817\n",
            "     61        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0009\u001b[0m  0.6781\n",
            "     62        0.0015        0.0033  0.6425\n",
            "     63        0.0018        0.0071  0.6382\n",
            "     64        \u001b[36m0.0013\u001b[0m        0.0017  0.6496\n",
            "     65        0.0022        0.0062  0.6556\n",
            "     66        0.0019        0.0081  0.6451\n",
            "     67        0.0015        0.0011  0.7779\n",
            "     68        \u001b[36m0.0012\u001b[0m        0.0017  0.8572\n",
            "     69        0.0017        0.0012  0.8782\n",
            "     70        \u001b[36m0.0009\u001b[0m        0.0011  0.9121\n",
            "     71        0.0012        0.0033  0.8283\n",
            "     72        0.0080        0.0037  0.6361\n",
            "     73        0.0030        0.0031  0.6564\n",
            "     74        0.0021        0.0047  0.6505\n",
            "     75        0.0020        0.0156  0.6567\n",
            "     76        0.0015        0.0047  0.6494\n",
            "     77        0.0026        0.0021  0.6437\n",
            "     78        0.0014        0.0031  0.6560\n",
            "     79        \u001b[36m0.0009\u001b[0m        0.0034  0.6516\n",
            "     80        \u001b[36m0.0007\u001b[0m        0.0055  0.6353\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  56.6s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1601\u001b[0m        \u001b[32m0.2115\u001b[0m  0.6691\n",
            "      2        \u001b[36m0.1561\u001b[0m        \u001b[32m0.2009\u001b[0m  0.7297\n",
            "      3        \u001b[36m0.1503\u001b[0m        \u001b[32m0.1821\u001b[0m  0.6951\n",
            "      4        \u001b[36m0.1343\u001b[0m        \u001b[32m0.1670\u001b[0m  0.6916\n",
            "      5        \u001b[36m0.1169\u001b[0m        0.1846  0.6966\n",
            "      6        \u001b[36m0.1121\u001b[0m        \u001b[32m0.0777\u001b[0m  0.8445\n",
            "      7        \u001b[36m0.1048\u001b[0m        0.0832  0.9214\n",
            "      8        \u001b[36m0.0887\u001b[0m        \u001b[32m0.0374\u001b[0m  0.8932\n",
            "      9        \u001b[36m0.0459\u001b[0m        0.0578  0.9733\n",
            "     10        \u001b[36m0.0402\u001b[0m        \u001b[32m0.0344\u001b[0m  0.8078\n",
            "     11        \u001b[36m0.0318\u001b[0m        0.0680  0.6902\n",
            "     12        \u001b[36m0.0306\u001b[0m        0.0628  0.6935\n",
            "     13        \u001b[36m0.0266\u001b[0m        0.0618  0.6842\n",
            "     14        \u001b[36m0.0238\u001b[0m        0.0579  0.6841\n",
            "     15        \u001b[36m0.0222\u001b[0m        0.0514  0.6888\n",
            "     16        \u001b[36m0.0215\u001b[0m        0.0505  0.7099\n",
            "     17        \u001b[36m0.0203\u001b[0m        0.0431  0.6888\n",
            "     18        \u001b[36m0.0193\u001b[0m        0.0566  0.6910\n",
            "     19        \u001b[36m0.0191\u001b[0m        0.0574  0.6840\n",
            "     20        \u001b[36m0.0180\u001b[0m        0.0656  0.6984\n",
            "     21        \u001b[36m0.0176\u001b[0m        0.0533  0.6934\n",
            "     22        \u001b[36m0.0168\u001b[0m        0.0480  0.7039\n",
            "     23        \u001b[36m0.0165\u001b[0m        0.0671  0.6977\n",
            "     24        \u001b[36m0.0158\u001b[0m        0.0677  0.8038\n",
            "     25        \u001b[36m0.0157\u001b[0m        0.0786  0.9179\n",
            "     26        \u001b[36m0.0155\u001b[0m        0.0663  0.9186\n",
            "     27        \u001b[36m0.0138\u001b[0m        0.0616  0.9305\n",
            "     28        \u001b[36m0.0131\u001b[0m        0.0559  0.9038\n",
            "     29        \u001b[36m0.0129\u001b[0m        0.0517  0.6909\n",
            "     30        0.0129        0.0418  0.7010\n",
            "     31        \u001b[36m0.0122\u001b[0m        0.0387  0.6922\n",
            "     32        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0332\u001b[0m  0.7069\n",
            "     33        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0271\u001b[0m  0.6837\n",
            "     34        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0249\u001b[0m  0.6698\n",
            "     35        \u001b[36m0.0100\u001b[0m        0.0282  0.6873\n",
            "     36        0.0111        \u001b[32m0.0198\u001b[0m  0.6972\n",
            "     37        \u001b[36m0.0095\u001b[0m        0.0215  0.6870\n",
            "     38        \u001b[36m0.0088\u001b[0m        0.0219  0.6921\n",
            "     39        0.0088        0.0214  0.7123\n",
            "     40        0.0092        0.0305  0.7000\n",
            "     41        \u001b[36m0.0084\u001b[0m        0.0276  0.7086\n",
            "     42        0.0084        0.0304  0.6927\n",
            "     43        \u001b[36m0.0083\u001b[0m        0.0262  0.8774\n",
            "     44        0.0087        \u001b[32m0.0195\u001b[0m  0.9118\n",
            "     45        \u001b[36m0.0078\u001b[0m        \u001b[32m0.0193\u001b[0m  0.9118\n",
            "     46        \u001b[36m0.0075\u001b[0m        0.0275  0.9793\n",
            "     47        \u001b[36m0.0072\u001b[0m        0.0241  0.7505\n",
            "     48        0.0073        0.0205  0.6905\n",
            "     49        \u001b[36m0.0071\u001b[0m        0.0225  0.6815\n",
            "     50        \u001b[36m0.0065\u001b[0m        0.0292  0.6935\n",
            "     51        0.0070        0.0323  0.7008\n",
            "     52        0.0078        0.0357  0.6756\n",
            "     53        0.0075        0.0290  0.7010\n",
            "     54        0.0065        0.0207  0.6922\n",
            "     55        0.0070        0.0301  0.6845\n",
            "     56        0.0069        0.0294  0.6930\n",
            "     57        0.0069        0.0261  0.7010\n",
            "     58        \u001b[36m0.0063\u001b[0m        0.0236  0.6899\n",
            "     59        \u001b[36m0.0059\u001b[0m        0.0273  0.6914\n",
            "     60        0.0071        0.0269  0.6985\n",
            "     61        0.0067        0.0220  0.8019\n",
            "     62        \u001b[36m0.0056\u001b[0m        0.0261  0.9453\n",
            "     63        0.0071        0.0225  0.9125\n",
            "     64        0.0062        \u001b[32m0.0177\u001b[0m  0.9518\n",
            "     65        0.0058        0.0203  0.8300\n",
            "     66        \u001b[36m0.0050\u001b[0m        \u001b[32m0.0150\u001b[0m  0.7067\n",
            "     67        0.0053        \u001b[32m0.0133\u001b[0m  0.6752\n",
            "     68        0.0050        0.0155  0.6931\n",
            "     69        0.0053        \u001b[32m0.0091\u001b[0m  0.6902\n",
            "     70        0.0059        0.0115  0.7021\n",
            "     71        0.0060        0.0132  0.6993\n",
            "     72        0.0065        0.0193  0.6944\n",
            "     73        0.0055        0.0101  0.7059\n",
            "     74        0.0051        \u001b[32m0.0066\u001b[0m  0.6885\n",
            "     75        \u001b[36m0.0047\u001b[0m        0.0084  0.6870\n",
            "     76        0.0053        0.0089  0.6992\n",
            "     77        0.0048        \u001b[32m0.0061\u001b[0m  0.6834\n",
            "     78        \u001b[36m0.0044\u001b[0m        0.0103  0.6991\n",
            "     79        0.0045        0.0062  0.7642\n",
            "     80        0.0046        0.0062  0.9149\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.0min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1648\u001b[0m        \u001b[32m0.2063\u001b[0m  0.9377\n",
            "      2        \u001b[36m0.1588\u001b[0m        \u001b[32m0.1960\u001b[0m  0.9708\n",
            "      3        \u001b[36m0.1474\u001b[0m        \u001b[32m0.1853\u001b[0m  0.8305\n",
            "      4        \u001b[36m0.1371\u001b[0m        \u001b[32m0.1777\u001b[0m  0.6905\n",
            "      5        \u001b[36m0.1257\u001b[0m        \u001b[32m0.1750\u001b[0m  0.6837\n",
            "      6        \u001b[36m0.1135\u001b[0m        \u001b[32m0.1317\u001b[0m  0.6930\n",
            "      7        \u001b[36m0.1132\u001b[0m        \u001b[32m0.0952\u001b[0m  0.6958\n",
            "      8        \u001b[36m0.1003\u001b[0m        0.1190  0.7023\n",
            "      9        0.1080        \u001b[32m0.0467\u001b[0m  0.6991\n",
            "     10        \u001b[36m0.0546\u001b[0m        \u001b[32m0.0415\u001b[0m  0.6800\n",
            "     11        \u001b[36m0.0386\u001b[0m        0.0571  0.6939\n",
            "     12        \u001b[36m0.0353\u001b[0m        0.0708  0.6800\n",
            "     13        \u001b[36m0.0346\u001b[0m        0.0707  0.6819\n",
            "     14        \u001b[36m0.0324\u001b[0m        0.0749  0.7219\n",
            "     15        \u001b[36m0.0301\u001b[0m        0.0794  0.7050\n",
            "     16        \u001b[36m0.0282\u001b[0m        0.0729  0.6866\n",
            "     17        \u001b[36m0.0269\u001b[0m        0.0642  0.7418\n",
            "     18        \u001b[36m0.0260\u001b[0m        0.0881  0.9141\n",
            "     19        \u001b[36m0.0256\u001b[0m        0.0809  0.9409\n",
            "     20        \u001b[36m0.0240\u001b[0m        0.0850  0.9437\n",
            "     21        \u001b[36m0.0228\u001b[0m        0.0970  0.8832\n",
            "     22        0.0248        0.0835  0.6961\n",
            "     23        \u001b[36m0.0219\u001b[0m        0.0755  0.6871\n",
            "     24        \u001b[36m0.0205\u001b[0m        0.0710  0.6996\n",
            "     25        0.0207        0.0469  0.6848\n",
            "     26        \u001b[36m0.0197\u001b[0m        0.0424  0.7028\n",
            "     27        \u001b[36m0.0188\u001b[0m        0.0617  0.6927\n",
            "     28        0.0197        \u001b[32m0.0384\u001b[0m  0.7064\n",
            "     29        \u001b[36m0.0171\u001b[0m        0.0507  0.6780\n",
            "     30        \u001b[36m0.0159\u001b[0m        0.0596  0.6819\n",
            "     31        0.0180        0.0455  0.6739\n",
            "     32        0.0160        \u001b[32m0.0316\u001b[0m  0.6965\n",
            "     33        \u001b[36m0.0155\u001b[0m        0.0327  0.7025\n",
            "     34        \u001b[36m0.0152\u001b[0m        0.0434  0.6861\n",
            "     35        \u001b[36m0.0143\u001b[0m        0.0543  0.7655\n",
            "     36        0.0146        0.0623  0.9141\n",
            "     37        \u001b[36m0.0137\u001b[0m        0.0648  0.9108\n",
            "     38        0.0149        0.0673  0.9160\n",
            "     39        0.0141        0.0624  0.8647\n",
            "     40        0.0142        0.0543  0.6851\n",
            "     41        0.0139        0.0623  0.6869\n",
            "     42        0.0148        0.0652  0.7041\n",
            "     43        0.0145        0.0516  0.6903\n",
            "     44        \u001b[36m0.0137\u001b[0m        0.0625  0.6996\n",
            "     45        0.0141        0.0491  0.6890\n",
            "     46        \u001b[36m0.0131\u001b[0m        0.0654  0.6912\n",
            "     47        0.0145        0.0539  0.7368\n",
            "     48        0.0136        0.0574  0.7327\n",
            "     49        0.0140        0.0522  0.6868\n",
            "     50        0.0132        0.0503  0.6865\n",
            "     51        0.0144        0.0376  0.7015\n",
            "     52        \u001b[36m0.0121\u001b[0m        0.0627  0.7011\n",
            "     53        0.0138        0.0635  0.7419\n",
            "     54        0.0139        0.0482  0.9126\n",
            "     55        0.0138        0.0578  0.9203\n",
            "     56        0.0133        0.0511  0.9389\n",
            "     57        0.0135        0.0530  0.9343\n",
            "     58        0.0139        0.0532  0.6758\n",
            "     59        0.0176        0.0606  0.6819\n",
            "     60        0.0160        0.0524  0.7134\n",
            "     61        0.0128        0.0404  0.6840\n",
            "     62        \u001b[36m0.0121\u001b[0m        0.0517  0.7102\n",
            "     63        0.0125        0.0537  0.7208\n",
            "     64        0.0131        0.0496  0.6842\n",
            "     65        0.0130        0.0516  0.7143\n",
            "     66        0.0135        0.0405  0.7123\n",
            "     67        0.0127        0.0397  0.7316\n",
            "     68        0.0129        0.0468  0.6887\n",
            "     69        0.0128        0.0419  0.6830\n",
            "     70        0.0123        0.0482  0.7158\n",
            "     71        0.0128        0.0531  0.7164\n",
            "     72        0.0128        0.0502  0.9915\n",
            "     73        0.0147        0.0486  0.9845\n",
            "     74        0.0127        0.0433  0.9782\n",
            "     75        0.0123        0.0431  0.9577\n",
            "     76        0.0131        0.0357  0.7064\n",
            "     77        \u001b[36m0.0119\u001b[0m        0.0367  0.6929\n",
            "     78        0.0119        0.0389  0.7004\n",
            "     79        0.0129        0.0411  0.7171\n",
            "     80        0.0120        \u001b[32m0.0283\u001b[0m  0.7313\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.0min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1783\u001b[0m        \u001b[32m0.2149\u001b[0m  0.6458\n",
            "      2        \u001b[36m0.1400\u001b[0m        \u001b[32m0.1835\u001b[0m  0.6552\n",
            "      3        \u001b[36m0.1315\u001b[0m        \u001b[32m0.1397\u001b[0m  0.6569\n",
            "      4        \u001b[36m0.1201\u001b[0m        \u001b[32m0.1186\u001b[0m  0.6577\n",
            "      5        \u001b[36m0.1111\u001b[0m        \u001b[32m0.0887\u001b[0m  0.6521\n",
            "      6        \u001b[36m0.1097\u001b[0m        0.0904  0.6565\n",
            "      7        \u001b[36m0.0958\u001b[0m        \u001b[32m0.0799\u001b[0m  0.6466\n",
            "      8        \u001b[36m0.0706\u001b[0m        \u001b[32m0.0645\u001b[0m  0.6559\n",
            "      9        \u001b[36m0.0436\u001b[0m        0.0797  0.6591\n",
            "     10        \u001b[36m0.0247\u001b[0m        \u001b[32m0.0551\u001b[0m  0.7461\n",
            "     11        0.0259        \u001b[32m0.0468\u001b[0m  0.8552\n",
            "     12        \u001b[36m0.0195\u001b[0m        0.0557  0.8528\n",
            "     13        \u001b[36m0.0172\u001b[0m        0.0482  0.8806\n",
            "     14        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0318\u001b[0m  0.8014\n",
            "     15        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0304\u001b[0m  0.6599\n",
            "     16        0.0148        0.0376  0.6472\n",
            "     17        \u001b[36m0.0142\u001b[0m        0.0664  0.6445\n",
            "     18        \u001b[36m0.0138\u001b[0m        0.0484  0.6581\n",
            "     19        \u001b[36m0.0131\u001b[0m        0.0466  0.6494\n",
            "     20        \u001b[36m0.0119\u001b[0m        0.0438  0.6475\n",
            "     21        \u001b[36m0.0112\u001b[0m        0.0470  0.6573\n",
            "     22        \u001b[36m0.0108\u001b[0m        0.0489  0.6651\n",
            "     23        0.0111        0.0487  0.6752\n",
            "     24        0.0110        0.0452  0.6500\n",
            "     25        \u001b[36m0.0106\u001b[0m        0.0554  0.6662\n",
            "     26        \u001b[36m0.0103\u001b[0m        0.0458  0.6445\n",
            "     27        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0283\u001b[0m  0.6579\n",
            "     28        \u001b[36m0.0086\u001b[0m        0.0376  0.6632\n",
            "     29        \u001b[36m0.0083\u001b[0m        0.0462  0.7142\n",
            "     30        \u001b[36m0.0078\u001b[0m        0.0336  0.8499\n",
            "     31        0.0079        0.0489  0.8670\n",
            "     32        \u001b[36m0.0075\u001b[0m        0.0423  0.8692\n",
            "     33        \u001b[36m0.0073\u001b[0m        0.0470  0.8597\n",
            "     34        0.0080        0.0602  0.6444\n",
            "     35        0.0074        0.0419  0.6568\n",
            "     36        \u001b[36m0.0071\u001b[0m        0.0450  0.6456\n",
            "     37        \u001b[36m0.0070\u001b[0m        0.0468  0.6528\n",
            "     38        0.0073        0.0409  0.6587\n",
            "     39        \u001b[36m0.0068\u001b[0m        0.0411  0.6513\n",
            "     40        \u001b[36m0.0060\u001b[0m        0.0408  0.6542\n",
            "     41        \u001b[36m0.0058\u001b[0m        0.0520  0.6640\n",
            "     42        0.0061        0.0552  0.6700\n",
            "     43        0.0061        0.0565  0.6660\n",
            "     44        0.0063        0.0414  0.6716\n",
            "     45        0.0061        0.0423  0.6918\n",
            "     46        \u001b[36m0.0057\u001b[0m        0.0425  0.7101\n",
            "     47        \u001b[36m0.0055\u001b[0m        0.0434  0.6943\n",
            "     48        \u001b[36m0.0055\u001b[0m        0.0426  0.7639\n",
            "     49        0.0058        0.0401  0.8553\n",
            "     50        0.0056        0.0431  0.8642\n",
            "     51        \u001b[36m0.0055\u001b[0m        0.0440  0.8826\n",
            "     52        0.0055        0.0387  0.9063\n",
            "     53        0.0055        0.0317  0.6474\n",
            "     54        \u001b[36m0.0049\u001b[0m        0.0340  0.6578\n",
            "     55        0.0050        0.0294  0.6535\n",
            "     56        0.0053        \u001b[32m0.0199\u001b[0m  0.6511\n",
            "     57        \u001b[36m0.0029\u001b[0m        0.0450  0.6577\n",
            "     58        0.0063        0.0243  0.6486\n",
            "     59        0.0053        0.0324  0.6482\n",
            "     60        0.0048        0.0217  0.6442\n",
            "     61        0.0044        0.0224  0.6355\n",
            "     62        0.0039        0.0287  0.6533\n",
            "     63        0.0045        0.0354  0.6677\n",
            "     64        0.0063        0.0240  0.6459\n",
            "     65        0.0031        0.0341  0.6493\n",
            "     66        0.0065        \u001b[32m0.0149\u001b[0m  0.6866\n",
            "     67        0.0044        0.0245  0.6675\n",
            "     68        0.0061        0.0272  0.8288\n",
            "     69        0.0043        0.0243  0.8578\n",
            "     70        0.0044        0.0160  0.8430\n",
            "     71        0.0033        0.0224  0.9232\n",
            "     72        0.0042        0.0188  0.7261\n",
            "     73        0.0039        0.0152  0.6603\n",
            "     74        0.0037        0.0172  0.6580\n",
            "     75        0.0039        0.0275  0.6555\n",
            "     76        0.0046        0.0309  0.6692\n",
            "     77        0.0052        0.0251  0.6539\n",
            "     78        0.0042        0.0231  0.6636\n",
            "     79        0.0056        0.0184  0.6627\n",
            "     80        0.0064        \u001b[32m0.0090\u001b[0m  0.6622\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  56.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1669\u001b[0m        \u001b[32m0.2491\u001b[0m  0.6924\n",
            "      2        \u001b[36m0.1438\u001b[0m        \u001b[32m0.1991\u001b[0m  0.6606\n",
            "      3        \u001b[36m0.1334\u001b[0m        \u001b[32m0.1723\u001b[0m  0.6603\n",
            "      4        \u001b[36m0.1179\u001b[0m        \u001b[32m0.1598\u001b[0m  0.6376\n",
            "      5        \u001b[36m0.1112\u001b[0m        \u001b[32m0.1276\u001b[0m  0.6561\n",
            "      6        0.1119        \u001b[32m0.1173\u001b[0m  0.6627\n",
            "      7        \u001b[36m0.1026\u001b[0m        0.1680  0.8658\n",
            "      8        \u001b[36m0.0877\u001b[0m        \u001b[32m0.0625\u001b[0m  0.9018\n",
            "      9        \u001b[36m0.0563\u001b[0m        \u001b[32m0.0529\u001b[0m  0.8514\n",
            "     10        \u001b[36m0.0330\u001b[0m        \u001b[32m0.0428\u001b[0m  0.9121\n",
            "     11        \u001b[36m0.0277\u001b[0m        0.0607  0.7207\n",
            "     12        0.0282        0.0795  0.6668\n",
            "     13        \u001b[36m0.0259\u001b[0m        0.1131  0.6959\n",
            "     14        \u001b[36m0.0218\u001b[0m        0.1112  0.6847\n",
            "     15        \u001b[36m0.0206\u001b[0m        0.0824  0.6775\n",
            "     16        \u001b[36m0.0189\u001b[0m        0.0594  0.6598\n",
            "     17        \u001b[36m0.0170\u001b[0m        0.0592  0.6493\n",
            "     18        \u001b[36m0.0164\u001b[0m        0.0785  0.6529\n",
            "     19        \u001b[36m0.0161\u001b[0m        0.0671  0.6710\n",
            "     20        \u001b[36m0.0148\u001b[0m        0.0743  0.6552\n",
            "     21        0.0151        0.0767  0.6337\n",
            "     22        0.0153        0.0650  0.6407\n",
            "     23        \u001b[36m0.0146\u001b[0m        0.0477  0.6425\n",
            "     24        0.0151        \u001b[32m0.0358\u001b[0m  0.6470\n",
            "     25        \u001b[36m0.0139\u001b[0m        0.0394  0.6552\n",
            "     26        0.0144        \u001b[32m0.0217\u001b[0m  0.8468\n",
            "     27        \u001b[36m0.0117\u001b[0m        0.0366  0.8829\n",
            "     28        0.0118        0.0679  0.8823\n",
            "     29        \u001b[36m0.0115\u001b[0m        0.0377  0.9136\n",
            "     30        0.0117        0.0411  0.7619\n",
            "     31        \u001b[36m0.0111\u001b[0m        0.0302  0.6505\n",
            "     32        \u001b[36m0.0088\u001b[0m        0.0784  0.6550\n",
            "     33        0.0114        0.0404  0.6696\n",
            "     34        \u001b[36m0.0087\u001b[0m        0.0667  0.6893\n",
            "     35        0.0101        0.0478  0.6588\n",
            "     36        \u001b[36m0.0075\u001b[0m        0.0552  0.6469\n",
            "     37        0.0088        0.0458  0.6527\n",
            "     38        0.0078        0.0553  0.6536\n",
            "     39        0.0092        0.0472  0.6610\n",
            "     40        0.0084        0.0733  0.6400\n",
            "     41        0.0107        0.0443  0.6539\n",
            "     42        0.0101        0.0730  0.6666\n",
            "     43        0.0092        0.0474  0.6482\n",
            "     44        0.0088        0.0449  0.6717\n",
            "     45        \u001b[36m0.0067\u001b[0m        0.0926  0.7638\n",
            "     46        0.0114        0.0654  0.8937\n",
            "     47        0.0091        0.0416  0.8640\n",
            "     48        0.0074        0.0681  0.9019\n",
            "     49        0.0109        0.0320  0.8690\n",
            "     50        0.0082        0.0425  0.6547\n",
            "     51        0.0079        0.0492  0.6489\n",
            "     52        0.0073        0.0660  0.6585\n",
            "     53        0.0105        0.0455  0.6549\n",
            "     54        0.0090        0.0336  0.6550\n",
            "     55        \u001b[36m0.0062\u001b[0m        0.0642  0.6631\n",
            "     56        0.0098        0.0450  0.6597\n",
            "     57        0.0078        0.0804  0.6534\n",
            "     58        0.0102        0.0406  0.6487\n",
            "     59        \u001b[36m0.0061\u001b[0m        0.0445  0.6517\n",
            "     60        0.0093        0.0341  0.6576\n",
            "     61        0.0076        0.0467  0.6472\n",
            "     62        0.0078        0.0939  0.6473\n",
            "     63        0.0101        0.0950  0.6538\n",
            "     64        0.0070        0.1080  0.6652\n",
            "     65        0.0103        0.0758  0.8448\n",
            "     66        0.0088        0.0620  0.8678\n",
            "     67        0.0066        0.0625  0.8570\n",
            "     68        0.0074        0.0470  0.9134\n",
            "     69        0.0081        0.0639  0.6854\n",
            "     70        0.0068        0.0883  0.6716\n",
            "     71        0.0086        0.0468  0.6766\n",
            "     72        0.0086        0.0427  0.6629\n",
            "     73        \u001b[36m0.0056\u001b[0m        0.0660  0.6431\n",
            "     74        0.0075        0.0536  0.6583\n",
            "     75        0.0070        0.0615  0.6467\n",
            "     76        0.0070        0.0853  0.6752\n",
            "     77        0.0081        0.0920  0.6744\n",
            "     78        0.0066        0.0732  0.6361\n",
            "     79        0.0066        0.0721  0.6497\n",
            "     80        0.0073        0.0829  0.6656\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  56.9s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1713\u001b[0m        \u001b[32m0.2192\u001b[0m  0.7491\n",
            "      2        \u001b[36m0.1547\u001b[0m        \u001b[32m0.2097\u001b[0m  0.7728\n",
            "      3        \u001b[36m0.1497\u001b[0m        \u001b[32m0.2030\u001b[0m  0.8868\n",
            "      4        \u001b[36m0.1425\u001b[0m        \u001b[32m0.1791\u001b[0m  0.9801\n",
            "      5        \u001b[36m0.1300\u001b[0m        \u001b[32m0.1624\u001b[0m  0.9813\n",
            "      6        \u001b[36m0.1190\u001b[0m        \u001b[32m0.1387\u001b[0m  0.9992\n",
            "      7        \u001b[36m0.0954\u001b[0m        \u001b[32m0.0635\u001b[0m  0.8515\n",
            "      8        \u001b[36m0.0705\u001b[0m        0.1089  0.7469\n",
            "      9        \u001b[36m0.0512\u001b[0m        \u001b[32m0.0367\u001b[0m  0.7282\n",
            "     10        \u001b[36m0.0455\u001b[0m        0.1176  0.7410\n",
            "     11        \u001b[36m0.0318\u001b[0m        0.0802  0.7551\n",
            "     12        \u001b[36m0.0260\u001b[0m        0.0842  0.7569\n",
            "     13        \u001b[36m0.0246\u001b[0m        0.0665  0.7911\n",
            "     14        \u001b[36m0.0202\u001b[0m        0.0648  0.7603\n",
            "     15        \u001b[36m0.0188\u001b[0m        0.0602  0.7594\n",
            "     16        \u001b[36m0.0182\u001b[0m        0.0567  0.7679\n",
            "     17        \u001b[36m0.0168\u001b[0m        0.0658  0.7358\n",
            "     18        \u001b[36m0.0159\u001b[0m        0.0847  0.7654\n",
            "     19        0.0160        0.0576  0.7518\n",
            "     20        \u001b[36m0.0150\u001b[0m        0.0689  0.8652\n",
            "     21        \u001b[36m0.0143\u001b[0m        0.0592  0.9886\n",
            "     22        0.0145        0.0585  0.9938\n",
            "     23        \u001b[36m0.0128\u001b[0m        0.0564  0.9963\n",
            "     24        0.0129        0.0571  0.8574\n",
            "     25        \u001b[36m0.0117\u001b[0m        0.0643  0.7523\n",
            "     26        0.0120        0.0537  0.7494\n",
            "     27        0.0123        0.0583  0.7472\n",
            "     28        0.0122        0.0556  0.7665\n",
            "     29        \u001b[36m0.0115\u001b[0m        0.0479  0.7642\n",
            "     30        \u001b[36m0.0108\u001b[0m        0.0375  0.7408\n",
            "     31        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0282\u001b[0m  0.7492\n",
            "     32        \u001b[36m0.0103\u001b[0m        0.0288  0.7529\n",
            "     33        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0273\u001b[0m  0.7530\n",
            "     34        \u001b[36m0.0095\u001b[0m        0.0303  0.7418\n",
            "     35        0.0101        \u001b[32m0.0243\u001b[0m  0.7401\n",
            "     36        \u001b[36m0.0095\u001b[0m        0.0251  0.7519\n",
            "     37        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0225\u001b[0m  0.8296\n",
            "     38        0.0104        0.0235  0.9632\n",
            "     39        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0170\u001b[0m  0.9585\n",
            "     40        \u001b[36m0.0083\u001b[0m        0.0262  1.0152\n",
            "     41        0.0088        0.0295  0.8501\n",
            "     42        0.0091        0.0234  0.7468\n",
            "     43        0.0093        0.0257  0.7620\n",
            "     44        \u001b[36m0.0078\u001b[0m        0.0237  0.7499\n",
            "     45        \u001b[36m0.0078\u001b[0m        0.0387  0.7418\n",
            "     46        \u001b[36m0.0076\u001b[0m        0.0377  0.7433\n",
            "     47        0.0079        0.0346  0.7523\n",
            "     48        0.0092        0.0379  0.7513\n",
            "     49        0.0076        0.0310  0.7369\n",
            "     50        \u001b[36m0.0074\u001b[0m        0.0281  0.7496\n",
            "     51        0.0076        0.0352  0.7426\n",
            "     52        \u001b[36m0.0073\u001b[0m        0.0294  0.7519\n",
            "     53        \u001b[36m0.0073\u001b[0m        0.0331  0.7600\n",
            "     54        0.0075        0.0299  0.8231\n",
            "     55        \u001b[36m0.0072\u001b[0m        0.0341  0.9564\n",
            "     56        \u001b[36m0.0070\u001b[0m        0.0371  0.9447\n",
            "     57        \u001b[36m0.0065\u001b[0m        0.0348  1.0411\n",
            "     58        0.0066        0.0471  0.8898\n",
            "     59        \u001b[36m0.0060\u001b[0m        0.0438  0.7414\n",
            "     60        0.0062        0.0310  0.7423\n",
            "     61        \u001b[36m0.0060\u001b[0m        0.0225  0.7491\n",
            "     62        0.0062        0.0178  0.7470\n",
            "     63        \u001b[36m0.0058\u001b[0m        0.0216  0.7361\n",
            "     64        \u001b[36m0.0052\u001b[0m        0.0238  0.7463\n",
            "     65        \u001b[36m0.0048\u001b[0m        0.0182  0.7448\n",
            "     66        0.0067        0.0236  0.7499\n",
            "     67        0.0095        0.0184  0.7650\n",
            "     68        0.0142        \u001b[32m0.0150\u001b[0m  0.7478\n",
            "     69        0.0061        0.0270  0.7502\n",
            "     70        0.0049        0.0246  0.7660\n",
            "     71        0.0067        0.0205  0.7901\n",
            "     72        0.0055        0.0195  0.9743\n",
            "     73        0.0049        0.0258  0.9724\n",
            "     74        0.0053        0.0251  0.9975\n",
            "     75        0.0051        0.0162  0.9359\n",
            "     76        \u001b[36m0.0047\u001b[0m        \u001b[32m0.0115\u001b[0m  0.7590\n",
            "     77        \u001b[36m0.0040\u001b[0m        0.0193  0.7524\n",
            "     78        0.0050        0.0187  0.7366\n",
            "     79        0.0048        0.0120  0.7537\n",
            "     80        \u001b[36m0.0038\u001b[0m        \u001b[32m0.0088\u001b[0m  0.7450\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1673\u001b[0m        \u001b[32m0.2335\u001b[0m  0.7616\n",
            "      2        \u001b[36m0.1527\u001b[0m        \u001b[32m0.2092\u001b[0m  0.7390\n",
            "      3        0.1547        \u001b[32m0.2043\u001b[0m  0.7525\n",
            "      4        \u001b[36m0.1525\u001b[0m        \u001b[32m0.2000\u001b[0m  0.7463\n",
            "      5        \u001b[36m0.1482\u001b[0m        \u001b[32m0.1940\u001b[0m  0.7477\n",
            "      6        \u001b[36m0.1466\u001b[0m        \u001b[32m0.1840\u001b[0m  0.7415\n",
            "      7        \u001b[36m0.1371\u001b[0m        \u001b[32m0.1715\u001b[0m  0.7654\n",
            "      8        \u001b[36m0.1240\u001b[0m        \u001b[32m0.1431\u001b[0m  0.8308\n",
            "      9        \u001b[36m0.1028\u001b[0m        \u001b[32m0.1071\u001b[0m  0.9567\n",
            "     10        \u001b[36m0.0670\u001b[0m        0.2090  0.9546\n",
            "     11        \u001b[36m0.0505\u001b[0m        0.1141  0.9854\n",
            "     12        0.0512        \u001b[32m0.0998\u001b[0m  0.8418\n",
            "     13        \u001b[36m0.0385\u001b[0m        0.1065  0.7428\n",
            "     14        0.0388        \u001b[32m0.0603\u001b[0m  0.7446\n",
            "     15        \u001b[36m0.0317\u001b[0m        0.0608  0.7437\n",
            "     16        \u001b[36m0.0313\u001b[0m        \u001b[32m0.0584\u001b[0m  0.7504\n",
            "     17        \u001b[36m0.0289\u001b[0m        0.0656  0.7574\n",
            "     18        \u001b[36m0.0244\u001b[0m        0.0679  0.7539\n",
            "     19        \u001b[36m0.0233\u001b[0m        \u001b[32m0.0466\u001b[0m  0.7490\n",
            "     20        \u001b[36m0.0212\u001b[0m        \u001b[32m0.0418\u001b[0m  0.7398\n",
            "     21        \u001b[36m0.0200\u001b[0m        0.0448  0.7517\n",
            "     22        \u001b[36m0.0197\u001b[0m        \u001b[32m0.0415\u001b[0m  0.7580\n",
            "     23        \u001b[36m0.0175\u001b[0m        0.0576  0.7592\n",
            "     24        0.0179        \u001b[32m0.0237\u001b[0m  0.7666\n",
            "     25        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0220\u001b[0m  0.8398\n",
            "     26        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0135\u001b[0m  0.9759\n",
            "     27        \u001b[36m0.0147\u001b[0m        0.0359  0.9526\n",
            "     28        0.0150        0.0418  1.0222\n",
            "     29        \u001b[36m0.0145\u001b[0m        0.0347  0.8315\n",
            "     30        \u001b[36m0.0135\u001b[0m        0.0474  0.7491\n",
            "     31        0.0135        0.0613  0.7421\n",
            "     32        0.0143        0.0767  0.7471\n",
            "     33        0.0139        0.0574  0.7435\n",
            "     34        \u001b[36m0.0128\u001b[0m        0.0723  0.7423\n",
            "     35        0.0141        0.0438  0.7516\n",
            "     36        0.0131        0.0670  0.7466\n",
            "     37        0.0190        0.0316  0.7494\n",
            "     38        \u001b[36m0.0126\u001b[0m        0.0355  0.7425\n",
            "     39        \u001b[36m0.0123\u001b[0m        0.0408  0.7494\n",
            "     40        \u001b[36m0.0122\u001b[0m        0.0330  0.7372\n",
            "     41        \u001b[36m0.0107\u001b[0m        0.0398  0.7503\n",
            "     42        0.0121        0.0407  0.8512\n",
            "     43        0.0129        0.0470  0.9782\n",
            "     44        0.0119        0.0369  0.9418\n",
            "     45        0.0117        0.0428  1.0003\n",
            "     46        0.0114        0.0394  0.8519\n",
            "     47        0.0116        0.0524  0.7513\n",
            "     48        0.0192        0.0520  0.7426\n",
            "     49        0.0125        0.0455  0.7477\n",
            "     50        0.0123        0.0574  0.7491\n",
            "     51        0.0122        0.0435  0.7469\n",
            "     52        0.0110        0.0394  0.7427\n",
            "     53        0.0121        0.0488  0.7492\n",
            "     54        0.0214        0.0307  0.7514\n",
            "     55        0.0152        0.0489  0.7537\n",
            "     56        0.0139        0.0415  0.7411\n",
            "     57        0.0120        0.0356  0.7487\n",
            "     58        0.0126        0.0470  0.7450\n",
            "     59        0.0128        0.0406  0.8004\n",
            "     60        0.0116        0.0440  0.9624\n",
            "     61        0.0114        0.0522  0.9533\n",
            "     62        0.0113        0.0444  0.9826\n",
            "     63        0.0107        0.0381  0.9195\n",
            "     64        \u001b[36m0.0103\u001b[0m        0.0499  0.7434\n",
            "     65        0.0114        0.0494  0.7389\n",
            "     66        0.0177        0.0352  0.7527\n",
            "     67        0.0129        0.0395  0.7444\n",
            "     68        0.0103        0.0372  0.7777\n",
            "     69        \u001b[36m0.0101\u001b[0m        0.0351  0.7509\n",
            "     70        0.0115        0.0295  0.7358\n",
            "     71        0.0102        0.0360  0.7383\n",
            "     72        0.0111        0.0349  0.7502\n",
            "     73        \u001b[36m0.0100\u001b[0m        0.0317  0.7535\n",
            "     74        0.0102        0.0278  0.7424\n",
            "     75        0.0100        0.0389  0.7481\n",
            "     76        0.0151        0.0377  0.7472\n",
            "     77        \u001b[36m0.0087\u001b[0m        0.0294  0.9654\n",
            "     78        0.0115        0.0438  0.9604\n",
            "     79        0.0112        0.0391  0.9870\n",
            "     80        0.0093        0.0417  0.9358\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.1min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
              "  module=<class 'GRU_model.GRUNet'>,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={'batch_size': [16, 32, 64],\n",
              "                         'iterator_train__shuffle': [False], 'max_epochs': [80],\n",
              "                         'module__drop_prob': [0.5],\n",
              "                         'module__hidden_dim': [16, 32, 64],\n",
              "                         'module__n_layers': [1, 2], 'optimizer__lr': [0.001],\n",
              "                         'optimizer__weight_decay': [0.0001, 0.001]},\n",
              "             refit=False, scoring='neg_mean_squared_error', verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={&#x27;batch_size&#x27;: [16, 32, 64],\n",
              "                         &#x27;iterator_train__shuffle&#x27;: [False], &#x27;max_epochs&#x27;: [80],\n",
              "                         &#x27;module__drop_prob&#x27;: [0.5],\n",
              "                         &#x27;module__hidden_dim&#x27;: [16, 32, 64],\n",
              "                         &#x27;module__n_layers&#x27;: [1, 2], &#x27;optimizer__lr&#x27;: [0.001],\n",
              "                         &#x27;optimizer__weight_decay&#x27;: [0.0001, 0.001]},\n",
              "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={&#x27;batch_size&#x27;: [16, 32, 64],\n",
              "                         &#x27;iterator_train__shuffle&#x27;: [False], &#x27;max_epochs&#x27;: [80],\n",
              "                         &#x27;module__drop_prob&#x27;: [0.5],\n",
              "                         &#x27;module__hidden_dim&#x27;: [16, 32, 64],\n",
              "                         &#x27;module__n_layers&#x27;: [1, 2], &#x27;optimizer__lr&#x27;: [0.001],\n",
              "                         &#x27;optimizer__weight_decay&#x27;: [0.0001, 0.001]},\n",
              "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NeuralNetRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NeuralNetRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              ")</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters_wind = gs.best_params_\n",
        "best_score_wind = gs.best_score_\n",
        "print(\"Best parameters:\", best_parameters_wind)\n",
        "print(\"Best score (negative MSE):\", best_score_wind)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Qw8RTsK-XYZ",
        "outputId": "1e3f2f22-30b9-4e7e-81d2-b3a18fb03870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'batch_size': 16, 'iterator_train__shuffle': False, 'max_epochs': 80, 'module__drop_prob': 0.5, 'module__hidden_dim': 16, 'module__n_layers': 1, 'optimizer__lr': 0.001, 'optimizer__weight_decay': 0.0001}\n",
            "Best score (negative MSE): -0.001339736278168857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Best parameters: batch_size': 16, 'iterator_train__shuffle': False, 'max_epochs': 80, 'module__drop_prob': 0.5, 'module__hidden_dim': 16, 'module__n_layers': 1, 'optimizer__lr': 0.001, 'optimizer__weight_decay': 0.0001\n",
        "gs_results_wind = pd.DataFrame(gs.cv_results_)\n",
        "gs_results_wind\n",
        "gs_results_wind.to_csv('/content/drive/MyDrive/solar_data/gird_search_results_wind.csv')"
      ],
      "metadata": {
        "id": "PP3NTlQ4-ueF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "2fcdf273-6239-4efd-c85a-e2f2347effb1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-23bad02f962d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Best parameters: batch_size': 16, 'iterator_train__shuffle': False, 'max_epochs': 80, 'module__drop_prob': 0.5, 'module__hidden_dim': 16, 'module__n_layers': 1, 'optimizer__lr': 0.001, 'optimizer__weight_decay': 0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgs_results_wind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgs_results_wind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgs_results_wind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/solar_data/gird_search_results_wind.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solar_24_X, solar_24_y = move_sliding_window_2(solar_data, WINDOW_SIZE, range(11), 0, forecast_horizon=24)\n",
        "solar_24_val_X, solar_24_val_y =  move_sliding_window_2(solar_data_val, WINDOW_SIZE, range(11), 0, forecast_horizon=24)\n",
        "wind_24_X, wind_24_y = move_sliding_window_2(wind_data, WINDOW_SIZE, range(11), 0, forecast_horizon=24)\n",
        "wind_24_val_X, wind_24_val_y = move_sliding_window_2(wind_data_val, WINDOW_SIZE, range(11), 0, forecast_horizon=24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3P469vkAJTl",
        "outputId": "17b2f43e-419c-4d18-b411-fe11292290b9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17496, 25, 11) (17496, 1)\n",
            "(8712, 25, 11) (8712, 1)\n",
            "(17496, 25, 11) (17496, 1)\n",
            "(8712, 25, 11) (8712, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gs_24_solar = GridSearchCV(net, params, refit=False, cv=ps_solar_24, scoring='neg_mean_squared_error', verbose=2)\n",
        "gs_24_solar.fit(solar_X_combined_24, solar_y_combined_24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "Yn9Qa0AADUGt",
        "outputId": "171b1167-a215-420f-928d-85df56986976"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'net' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-8ec8396f44c2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgs_24_solar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mps_solar_24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgs_24_solar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolar_X_combined_24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolar_y_combined_24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters_solar_24 = gs_24_solar.best_params_\n",
        "best_score_solar_24 = gs_24_solar.best_score_\n",
        "print(\"Best parameters:\", best_parameters_solar_24)\n",
        "print(\"Best score (negative MSE):\", best_score_solar_24)"
      ],
      "metadata": {
        "id": "gTsWSJiDDbim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2a7d1b-c2fe-49b1-b6b7-555dede07cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'batch_size': 64, 'iterator_train__shuffle': False, 'max_epochs': 80, 'module__drop_prob': 0.5, 'module__hidden_dim': 16, 'module__n_layers': 1, 'optimizer__lr': 0.001, 'optimizer__weight_decay': 0.0001}\n",
            "Best score (negative MSE): -0.03249602019786835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gs_results = pd.DataFrame(gs_24_solar.cv_results_)\n",
        "gs_results\n",
        "gs_results.to_csv('/content/drive/MyDrive/solar_data/gird_search_results_solar_24.csv')"
      ],
      "metadata": {
        "id": "WF_t1JWIT8Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_wind_24 = GridSearchCV(net, params, refit=False, cv=ps_wind_24, scoring='neg_mean_squared_error', verbose=2)\n",
        "gs_wind_24.fit(wind_X_combined_24, wind_y_combined_24)"
      ],
      "metadata": {
        "id": "PNE2Gmz9XQ87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5e6caab-f3aa-4e36-8cb6-652fccce20ed",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 36 candidates, totalling 36 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1735\u001b[0m        \u001b[32m0.1965\u001b[0m  2.6803\n",
            "      2        \u001b[36m0.1516\u001b[0m        0.1971  2.1412\n",
            "      3        \u001b[36m0.1402\u001b[0m        0.2018  2.1740\n",
            "      4        \u001b[36m0.1368\u001b[0m        0.1978  2.7525\n",
            "      5        \u001b[36m0.1354\u001b[0m        0.1969  2.4575\n",
            "      6        \u001b[36m0.1346\u001b[0m        0.1969  2.1409\n",
            "      7        \u001b[36m0.1336\u001b[0m        0.1975  2.1378\n",
            "      8        \u001b[36m0.1330\u001b[0m        \u001b[32m0.1935\u001b[0m  2.1358\n",
            "      9        \u001b[36m0.1330\u001b[0m        0.1941  2.0920\n",
            "     10        \u001b[36m0.1321\u001b[0m        \u001b[32m0.1887\u001b[0m  2.7428\n",
            "     11        \u001b[36m0.1314\u001b[0m        \u001b[32m0.1863\u001b[0m  2.4259\n",
            "     12        \u001b[36m0.1313\u001b[0m        \u001b[32m0.1819\u001b[0m  2.1197\n",
            "     13        \u001b[36m0.1312\u001b[0m        \u001b[32m0.1779\u001b[0m  2.1276\n",
            "     14        \u001b[36m0.1308\u001b[0m        \u001b[32m0.1740\u001b[0m  2.1411\n",
            "     15        \u001b[36m0.1304\u001b[0m        \u001b[32m0.1698\u001b[0m  2.1304\n",
            "     16        0.1323        0.1726  2.6836\n",
            "     17        0.1337        \u001b[32m0.1633\u001b[0m  2.4277\n",
            "     18        0.1320        \u001b[32m0.1590\u001b[0m  2.1313\n",
            "     19        0.1315        \u001b[32m0.1565\u001b[0m  2.1850\n",
            "     20        0.1314        \u001b[32m0.1551\u001b[0m  2.1542\n",
            "     21        0.1319        0.1653  2.1596\n",
            "     22        0.1317        \u001b[32m0.1510\u001b[0m  2.7493\n",
            "     23        0.1304        0.1559  2.3910\n",
            "     24        0.1306        \u001b[32m0.1466\u001b[0m  2.1868\n",
            "     25        \u001b[36m0.1301\u001b[0m        0.1513  2.4409\n",
            "     26        \u001b[36m0.1294\u001b[0m        0.1520  2.6383\n",
            "     27        0.1294        0.1509  2.7366\n",
            "     28        \u001b[36m0.1293\u001b[0m        0.1470  2.6788\n",
            "     29        \u001b[36m0.1293\u001b[0m        0.1536  2.1588\n",
            "     30        0.1295        0.1470  2.1341\n",
            "     31        \u001b[36m0.1291\u001b[0m        0.1474  2.1469\n",
            "     32        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1457\u001b[0m  2.1268\n",
            "     33        \u001b[36m0.1283\u001b[0m        \u001b[32m0.1416\u001b[0m  2.5057\n",
            "     34        \u001b[36m0.1274\u001b[0m        0.1596  2.6378\n",
            "     35        \u001b[36m0.1246\u001b[0m        0.2044  2.1372\n",
            "     36        0.1337        0.1984  2.1476\n",
            "     37        0.1322        0.1842  2.1427\n",
            "     38        0.1273        0.1643  2.1406\n",
            "     39        0.1282        0.1579  2.5201\n",
            "     40        \u001b[36m0.1230\u001b[0m        0.1926  2.6286\n",
            "     41        \u001b[36m0.1209\u001b[0m        0.1718  2.1456\n",
            "     42        0.1268        0.1711  2.1464\n",
            "     43        \u001b[36m0.1164\u001b[0m        0.1681  2.1422\n",
            "     44        \u001b[36m0.1059\u001b[0m        0.1589  2.1589\n",
            "     45        \u001b[36m0.1051\u001b[0m        0.1569  2.5632\n",
            "     46        \u001b[36m0.1034\u001b[0m        0.1655  2.5532\n",
            "     47        \u001b[36m0.1025\u001b[0m        0.1593  2.1526\n",
            "     48        \u001b[36m0.0974\u001b[0m        0.1554  2.1319\n",
            "     49        \u001b[36m0.0954\u001b[0m        0.1495  2.1380\n",
            "     50        \u001b[36m0.0937\u001b[0m        0.1451  2.1556\n",
            "     51        \u001b[36m0.0923\u001b[0m        0.1512  3.0194\n",
            "     52        0.0929        0.1486  3.2081\n",
            "     53        \u001b[36m0.0897\u001b[0m        0.1523  2.1629\n",
            "     54        0.0925        \u001b[32m0.1331\u001b[0m  2.1648\n",
            "     55        0.0923        0.1559  2.1465\n",
            "     56        \u001b[36m0.0857\u001b[0m        0.1510  2.1571\n",
            "     57        0.0862        0.1481  2.7573\n",
            "     58        0.0868        0.1361  2.3717\n",
            "     59        \u001b[36m0.0842\u001b[0m        0.1409  2.1439\n",
            "     60        \u001b[36m0.0837\u001b[0m        0.1385  2.1481\n",
            "     61        0.0858        0.1630  2.1299\n",
            "     62        0.0860        \u001b[32m0.1241\u001b[0m  2.1946\n",
            "     63        0.0847        0.1412  2.7681\n",
            "     64        0.0853        \u001b[32m0.1217\u001b[0m  2.3414\n",
            "     65        0.0846        0.1362  2.1290\n",
            "     66        0.0849        0.1336  2.1153\n",
            "     67        \u001b[36m0.0837\u001b[0m        0.1786  2.1460\n",
            "     68        0.0848        0.1664  2.1968\n",
            "     69        0.0849        0.1443  2.7796\n",
            "     70        0.0838        0.1745  2.3608\n",
            "     71        0.0840        0.1399  2.1671\n",
            "     72        \u001b[36m0.0830\u001b[0m        0.1779  2.1455\n",
            "     73        0.0836        0.1777  2.1519\n",
            "     74        0.0845        0.1803  2.2488\n",
            "     75        0.0836        0.1675  2.7838\n",
            "     76        0.0831        0.1693  2.3760\n",
            "     77        \u001b[36m0.0829\u001b[0m        0.1836  2.6458\n",
            "     78        0.0839        0.1851  2.4663\n",
            "     79        0.0834        0.1654  2.1432\n",
            "     80        \u001b[36m0.0827\u001b[0m        0.1709  2.6632\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.1min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1531\u001b[0m        \u001b[32m0.2097\u001b[0m  2.3938\n",
            "      2        \u001b[36m0.1495\u001b[0m        0.2143  2.1488\n",
            "      3        0.1499        \u001b[32m0.2079\u001b[0m  2.1271\n",
            "      4        \u001b[36m0.1450\u001b[0m        \u001b[32m0.2071\u001b[0m  2.1272\n",
            "      5        \u001b[36m0.1422\u001b[0m        \u001b[32m0.2050\u001b[0m  2.1711\n",
            "      6        \u001b[36m0.1420\u001b[0m        \u001b[32m0.2040\u001b[0m  2.7562\n",
            "      7        \u001b[36m0.1388\u001b[0m        0.2053  2.3354\n",
            "      8        \u001b[36m0.1367\u001b[0m        \u001b[32m0.2034\u001b[0m  2.1359\n",
            "      9        \u001b[36m0.1353\u001b[0m        \u001b[32m0.2021\u001b[0m  2.1376\n",
            "     10        \u001b[36m0.1351\u001b[0m        \u001b[32m0.1978\u001b[0m  2.1287\n",
            "     11        \u001b[36m0.1340\u001b[0m        \u001b[32m0.1976\u001b[0m  2.1779\n",
            "     12        0.1344        \u001b[32m0.1973\u001b[0m  2.8892\n",
            "     13        \u001b[36m0.1335\u001b[0m        \u001b[32m0.1938\u001b[0m  2.4202\n",
            "     14        0.1336        \u001b[32m0.1919\u001b[0m  2.1911\n",
            "     15        \u001b[36m0.1331\u001b[0m        \u001b[32m0.1907\u001b[0m  2.2866\n",
            "     16        0.1334        \u001b[32m0.1853\u001b[0m  2.3894\n",
            "     17        \u001b[36m0.1321\u001b[0m        \u001b[32m0.1845\u001b[0m  2.5346\n",
            "     18        \u001b[36m0.1318\u001b[0m        \u001b[32m0.1798\u001b[0m  3.1040\n",
            "     19        \u001b[36m0.1313\u001b[0m        0.1810  2.3977\n",
            "     20        0.1314        0.1857  2.3192\n",
            "     21        0.1319        \u001b[32m0.1797\u001b[0m  2.4248\n",
            "     22        0.1320        0.1803  2.6416\n",
            "     23        0.1316        0.1826  3.2213\n",
            "     24        \u001b[36m0.1311\u001b[0m        0.1826  2.5858\n",
            "     25        0.1335        0.1914  2.2990\n",
            "     26        0.1331        0.1866  2.2927\n",
            "     27        0.1330        0.1826  2.2456\n",
            "     28        0.1326        \u001b[32m0.1793\u001b[0m  2.3178\n",
            "     29        0.1327        0.1826  2.8698\n",
            "     30        0.1316        \u001b[32m0.1765\u001b[0m  2.3267\n",
            "     31        0.1315        0.1790  2.1235\n",
            "     32        \u001b[36m0.1310\u001b[0m        \u001b[32m0.1757\u001b[0m  2.1348\n",
            "     33        0.1315        \u001b[32m0.1745\u001b[0m  2.1333\n",
            "     34        0.1312        0.1753  2.2269\n",
            "     35        \u001b[36m0.1306\u001b[0m        0.1766  2.7795\n",
            "     36        \u001b[36m0.1302\u001b[0m        0.1746  2.2080\n",
            "     37        \u001b[36m0.1301\u001b[0m        0.1752  2.1353\n",
            "     38        0.1302        0.1761  2.1290\n",
            "     39        0.1303        \u001b[32m0.1692\u001b[0m  2.1573\n",
            "     40        \u001b[36m0.1300\u001b[0m        0.1736  2.3093\n",
            "     41        \u001b[36m0.1291\u001b[0m        0.1750  2.7629\n",
            "     42        0.1304        \u001b[32m0.1660\u001b[0m  2.1804\n",
            "     43        0.1299        0.1723  2.1315\n",
            "     44        0.1296        0.1692  2.1298\n",
            "     45        0.1295        0.1717  2.1459\n",
            "     46        0.1295        0.1730  2.3165\n",
            "     47        0.1296        0.1679  3.3592\n",
            "     48        0.1293        0.1729  2.7401\n",
            "     49        0.1295        0.1729  2.1407\n",
            "     50        0.1294        0.1709  2.1352\n",
            "     51        0.1299        0.1749  2.1361\n",
            "     52        0.1301        0.1696  2.5143\n",
            "     53        0.1299        0.1711  2.6686\n",
            "     54        0.1298        0.1684  2.1181\n",
            "     55        \u001b[36m0.1282\u001b[0m        0.1724  2.1145\n",
            "     56        0.1315        0.1828  2.1156\n",
            "     57        0.1286        0.1692  2.1476\n",
            "     58        \u001b[36m0.1276\u001b[0m        0.1698  2.4807\n",
            "     59        0.1290        0.1673  2.6466\n",
            "     60        0.1292        0.1714  2.1302\n",
            "     61        0.1294        0.1707  2.1296\n",
            "     62        0.1299        0.1695  2.1318\n",
            "     63        0.1297        0.1700  2.1379\n",
            "     64        0.1309        0.1739  2.5046\n",
            "     65        0.1310        0.1713  2.5732\n",
            "     66        0.1308        0.1714  2.1328\n",
            "     67        0.1311        0.1729  2.1603\n",
            "     68        0.1310        0.1741  2.1322\n",
            "     69        0.1328        0.1719  2.1389\n",
            "     70        0.1326        0.1685  2.5581\n",
            "     71        0.1327        \u001b[32m0.1642\u001b[0m  2.5174\n",
            "     72        0.1319        0.1679  2.3137\n",
            "     73        0.1319        \u001b[32m0.1637\u001b[0m  2.6471\n",
            "     74        0.1315        0.1670  2.5557\n",
            "     75        0.1317        0.1666  2.4149\n",
            "     76        0.1315        0.1639  2.7001\n",
            "     77        0.1332        \u001b[32m0.1636\u001b[0m  2.1475\n",
            "     78        0.1319        0.1689  2.1463\n",
            "     79        0.1313        0.1702  2.1565\n",
            "     80        0.1334        0.1716  2.1340\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.2min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1533\u001b[0m        \u001b[32m0.2140\u001b[0m  2.7988\n",
            "      2        \u001b[36m0.1509\u001b[0m        0.2149  2.6693\n",
            "      3        0.1534        \u001b[32m0.2131\u001b[0m  2.2901\n",
            "      4        0.1519        \u001b[32m0.2104\u001b[0m  2.2957\n",
            "      5        0.1523        \u001b[32m0.2101\u001b[0m  2.3074\n",
            "      6        0.1509        \u001b[32m0.2097\u001b[0m  2.4458\n",
            "      7        0.1510        \u001b[32m0.2079\u001b[0m  3.0063\n",
            "      8        \u001b[36m0.1486\u001b[0m        0.2093  2.2830\n",
            "      9        \u001b[36m0.1470\u001b[0m        \u001b[32m0.2050\u001b[0m  2.2931\n",
            "     10        \u001b[36m0.1421\u001b[0m        0.2057  2.2888\n",
            "     11        \u001b[36m0.1404\u001b[0m        \u001b[32m0.2041\u001b[0m  2.3392\n",
            "     12        \u001b[36m0.1388\u001b[0m        \u001b[32m0.2025\u001b[0m  2.7956\n",
            "     13        0.1410        \u001b[32m0.1944\u001b[0m  2.6662\n",
            "     14        \u001b[36m0.1364\u001b[0m        \u001b[32m0.1725\u001b[0m  2.2940\n",
            "     15        \u001b[36m0.1360\u001b[0m        \u001b[32m0.1631\u001b[0m  2.2954\n",
            "     16        \u001b[36m0.1359\u001b[0m        \u001b[32m0.1595\u001b[0m  2.2987\n",
            "     17        \u001b[36m0.1353\u001b[0m        0.1702  2.9705\n",
            "     18        \u001b[36m0.1350\u001b[0m        0.1719  3.6054\n",
            "     19        0.1359        0.1602  2.2844\n",
            "     20        0.1364        0.1596  2.2906\n",
            "     21        0.1352        0.1665  2.2919\n",
            "     22        0.1354        \u001b[32m0.1588\u001b[0m  2.2865\n",
            "     23        0.1356        0.1649  3.0112\n",
            "     24        0.1359        0.1704  2.3957\n",
            "     25        0.1359        0.1707  2.2943\n",
            "     26        0.1354        0.1639  2.2860\n",
            "     27        0.1369        0.1865  2.2932\n",
            "     28        0.1363        0.1616  2.6902\n",
            "     29        0.1363        0.1812  2.7055\n",
            "     30        0.1361        0.1766  2.2969\n",
            "     31        0.1368        0.1838  2.2819\n",
            "     32        0.1360        0.1751  2.2920\n",
            "     33        0.1365        0.1764  2.3869\n",
            "     34        0.1357        0.1655  3.0306\n",
            "     35        0.1358        0.1736  2.3360\n",
            "     36        0.1357        0.1772  2.2829\n",
            "     37        0.1356        0.1706  2.2972\n",
            "     38        \u001b[36m0.1349\u001b[0m        0.1743  2.2886\n",
            "     39        0.1356        0.1841  2.7418\n",
            "     40        0.1356        0.1686  2.6504\n",
            "     41        0.1357        0.1748  2.8013\n",
            "     42        0.1351        0.1615  2.7990\n",
            "     43        0.1356        0.1805  2.2900\n",
            "     44        \u001b[36m0.1343\u001b[0m        0.1849  2.7879\n",
            "     45        \u001b[36m0.1343\u001b[0m        0.1824  2.5985\n",
            "     46        0.1372        0.1785  2.2904\n",
            "     47        \u001b[36m0.1336\u001b[0m        0.1635  2.3219\n",
            "     48        0.1348        0.1647  2.2816\n",
            "     49        0.1351        0.1641  2.6091\n",
            "     50        \u001b[36m0.1328\u001b[0m        0.1841  2.8373\n",
            "     51        0.1343        0.1803  2.2904\n",
            "     52        0.1339        0.1656  2.2898\n",
            "     53        0.1336        0.1732  2.2809\n",
            "     54        0.1336        \u001b[32m0.1576\u001b[0m  2.2923\n",
            "     55        0.1329        0.1665  2.9452\n",
            "     56        0.1341        0.1899  2.4250\n",
            "     57        0.1345        \u001b[32m0.1569\u001b[0m  2.2808\n",
            "     58        0.1340        \u001b[32m0.1503\u001b[0m  2.2833\n",
            "     59        0.1345        \u001b[32m0.1497\u001b[0m  2.2889\n",
            "     60        0.1336        0.1833  2.6834\n",
            "     61        0.1344        0.1589  2.7785\n",
            "     62        0.1332        0.1529  2.2872\n",
            "     63        \u001b[36m0.1322\u001b[0m        0.1718  2.2925\n",
            "     64        0.1330        0.1885  2.2959\n",
            "     65        0.1339        0.1757  2.8493\n",
            "     66        0.1340        0.1882  3.6827\n",
            "     67        0.1359        0.1847  2.2893\n",
            "     68        0.1323        0.1853  2.2875\n",
            "     69        0.1329        0.1936  2.2860\n",
            "     70        0.1335        0.1591  2.2956\n",
            "     71        0.1324        0.1937  2.9349\n",
            "     72        0.1333        0.1897  2.4575\n",
            "     73        0.1350        0.1865  2.2689\n",
            "     74        0.1331        0.1793  2.2682\n",
            "     75        \u001b[36m0.1311\u001b[0m        0.1992  2.2756\n",
            "     76        0.1319        0.1598  2.6160\n",
            "     77        0.1313        0.1638  2.7378\n",
            "     78        0.1317        0.1804  2.2729\n",
            "     79        \u001b[36m0.1309\u001b[0m        0.1841  2.2856\n",
            "     80        0.1320        0.1806  2.2872\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.3min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1642\u001b[0m        \u001b[32m0.2194\u001b[0m  2.4724\n",
            "      2        \u001b[36m0.1515\u001b[0m        \u001b[32m0.2159\u001b[0m  2.8546\n",
            "      3        \u001b[36m0.1504\u001b[0m        \u001b[32m0.2141\u001b[0m  2.3185\n",
            "      4        0.1525        0.2148  2.3176\n",
            "      5        \u001b[36m0.1501\u001b[0m        \u001b[32m0.2126\u001b[0m  2.3141\n",
            "      6        0.1534        0.2145  2.2792\n",
            "      7        0.1531        0.2129  2.9388\n",
            "      8        0.1532        0.2144  2.6049\n",
            "      9        0.1513        0.2138  2.8570\n",
            "     10        0.1537        0.2164  2.6821\n",
            "     11        0.1503        0.2165  2.2869\n",
            "     12        0.1532        0.2157  2.9070\n",
            "     13        0.1513        0.2178  2.5260\n",
            "     14        0.1511        0.2198  2.2870\n",
            "     15        0.1510        0.2193  2.3157\n",
            "     16        0.1502        0.2183  2.2833\n",
            "     17        \u001b[36m0.1499\u001b[0m        0.2207  2.5257\n",
            "     18        0.1500        0.2176  2.8984\n",
            "     19        \u001b[36m0.1489\u001b[0m        0.2182  2.2855\n",
            "     20        0.1493        0.2158  2.3137\n",
            "     21        \u001b[36m0.1489\u001b[0m        0.2179  2.2828\n",
            "     22        \u001b[36m0.1480\u001b[0m        0.2195  2.2805\n",
            "     23        0.1490        0.2220  2.8387\n",
            "     24        0.1488        0.2207  2.5849\n",
            "     25        0.1502        0.2238  2.2791\n",
            "     26        0.1499        0.2197  2.2843\n",
            "     27        0.1487        0.2175  2.2829\n",
            "     28        0.1501        0.2206  2.5208\n",
            "     29        \u001b[36m0.1471\u001b[0m        0.2214  2.9463\n",
            "     30        0.1473        0.2180  2.2887\n",
            "     31        0.1479        0.2186  2.2910\n",
            "     32        0.1482        0.2190  2.3060\n",
            "     33        0.1480        0.2190  2.7676\n",
            "     34        \u001b[36m0.1467\u001b[0m        0.2184  3.7905\n",
            "     35        0.1470        0.2179  2.3343\n",
            "     36        \u001b[36m0.1461\u001b[0m        0.2148  2.3169\n",
            "     37        0.1467        \u001b[32m0.2097\u001b[0m  2.3138\n",
            "     38        \u001b[36m0.1437\u001b[0m        \u001b[32m0.2050\u001b[0m  2.3058\n",
            "     39        \u001b[36m0.1418\u001b[0m        \u001b[32m0.2024\u001b[0m  2.8151\n",
            "     40        \u001b[36m0.1414\u001b[0m        0.2030  2.6715\n",
            "     41        \u001b[36m0.1407\u001b[0m        \u001b[32m0.2017\u001b[0m  2.3146\n",
            "     42        0.1409        \u001b[32m0.1897\u001b[0m  2.2968\n",
            "     43        0.1417        \u001b[32m0.1860\u001b[0m  2.2974\n",
            "     44        0.1416        \u001b[32m0.1845\u001b[0m  2.4622\n",
            "     45        \u001b[36m0.1404\u001b[0m        0.1884  2.9837\n",
            "     46        0.1421        \u001b[32m0.1799\u001b[0m  2.2899\n",
            "     47        0.1424        \u001b[32m0.1775\u001b[0m  2.2930\n",
            "     48        0.1417        \u001b[32m0.1668\u001b[0m  2.2901\n",
            "     49        0.1412        0.1846  2.2974\n",
            "     50        0.1410        \u001b[32m0.1663\u001b[0m  2.7603\n",
            "     51        0.1411        0.1938  2.7000\n",
            "     52        0.1419        0.1764  2.2986\n",
            "     53        0.1407        0.1767  2.2949\n",
            "     54        0.1409        0.1669  2.3117\n",
            "     55        0.1409        0.1746  2.4548\n",
            "     56        0.1415        0.1824  3.2028\n",
            "     57        \u001b[36m0.1401\u001b[0m        0.1896  2.8422\n",
            "     58        0.1413        0.1931  2.5598\n",
            "     59        0.1411        0.1937  2.2818\n",
            "     60        \u001b[36m0.1396\u001b[0m        0.1761  2.4086\n",
            "     61        0.1406        0.1735  3.0423\n",
            "     62        0.1398        0.1689  2.2945\n",
            "     63        0.1401        0.1772  2.3119\n",
            "     64        0.1428        0.1945  2.3073\n",
            "     65        \u001b[36m0.1392\u001b[0m        0.1988  2.3013\n",
            "     66        0.1417        0.1751  2.7584\n",
            "     67        0.1414        0.1897  2.6987\n",
            "     68        0.1400        0.1951  2.2899\n",
            "     69        0.1423        0.1895  2.2859\n",
            "     70        0.1418        0.1842  2.2689\n",
            "     71        0.1401        0.1901  2.4141\n",
            "     72        0.1400        0.1907  3.0066\n",
            "     73        0.1410        0.1969  2.2927\n",
            "     74        0.1393        0.2012  2.2953\n",
            "     75        0.1408        0.1726  2.3104\n",
            "     76        0.1401        0.1888  2.3352\n",
            "     77        0.1411        0.1779  2.7938\n",
            "     78        0.1407        0.1945  2.6714\n",
            "     79        \u001b[36m0.1383\u001b[0m        0.1949  2.2924\n",
            "     80        0.1395        0.1807  2.3799\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.3min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1536\u001b[0m        \u001b[32m0.2251\u001b[0m  2.6272\n",
            "      2        \u001b[36m0.1508\u001b[0m        \u001b[32m0.2154\u001b[0m  2.9592\n",
            "      3        \u001b[36m0.1448\u001b[0m        \u001b[32m0.2150\u001b[0m  2.4785\n",
            "      4        0.1450        \u001b[32m0.2108\u001b[0m  2.1238\n",
            "      5        \u001b[36m0.1425\u001b[0m        0.2190  2.1257\n",
            "      6        \u001b[36m0.1415\u001b[0m        \u001b[32m0.2089\u001b[0m  2.1161\n",
            "      7        \u001b[36m0.1408\u001b[0m        \u001b[32m0.2071\u001b[0m  2.1205\n",
            "      8        \u001b[36m0.1380\u001b[0m        0.2079  2.5829\n",
            "      9        \u001b[36m0.1365\u001b[0m        \u001b[32m0.2060\u001b[0m  2.5554\n",
            "     10        \u001b[36m0.1347\u001b[0m        \u001b[32m0.2021\u001b[0m  2.1036\n",
            "     11        0.1347        0.2045  2.1251\n",
            "     12        \u001b[36m0.1335\u001b[0m        \u001b[32m0.1906\u001b[0m  2.1204\n",
            "     13        \u001b[36m0.1321\u001b[0m        0.1917  2.1195\n",
            "     14        \u001b[36m0.1317\u001b[0m        \u001b[32m0.1899\u001b[0m  2.5664\n",
            "     15        \u001b[36m0.1311\u001b[0m        \u001b[32m0.1822\u001b[0m  2.4387\n",
            "     16        \u001b[36m0.1299\u001b[0m        \u001b[32m0.1780\u001b[0m  2.1324\n",
            "     17        \u001b[36m0.1297\u001b[0m        \u001b[32m0.1741\u001b[0m  2.1237\n",
            "     18        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1690\u001b[0m  2.1301\n",
            "     19        0.1311        0.1755  2.1179\n",
            "     20        \u001b[36m0.1265\u001b[0m        \u001b[32m0.1603\u001b[0m  2.6088\n",
            "     21        \u001b[36m0.1230\u001b[0m        \u001b[32m0.1547\u001b[0m  2.4370\n",
            "     22        \u001b[36m0.1219\u001b[0m        0.1570  2.1207\n",
            "     23        \u001b[36m0.1204\u001b[0m        \u001b[32m0.1466\u001b[0m  2.1171\n",
            "     24        \u001b[36m0.1184\u001b[0m        \u001b[32m0.1464\u001b[0m  2.1322\n",
            "     25        0.1204        \u001b[32m0.1455\u001b[0m  2.1499\n",
            "     26        \u001b[36m0.1138\u001b[0m        0.1656  2.8172\n",
            "     27        0.1221        0.1457  2.8625\n",
            "     28        0.1147        0.1626  2.5179\n",
            "     29        \u001b[36m0.1124\u001b[0m        \u001b[32m0.1454\u001b[0m  2.1247\n",
            "     30        0.1130        \u001b[32m0.1444\u001b[0m  2.1274\n",
            "     31        \u001b[36m0.1087\u001b[0m        \u001b[32m0.1411\u001b[0m  2.4209\n",
            "     32        \u001b[36m0.1067\u001b[0m        0.1460  2.6264\n",
            "     33        0.1083        \u001b[32m0.1405\u001b[0m  2.1192\n",
            "     34        \u001b[36m0.1063\u001b[0m        \u001b[32m0.1299\u001b[0m  2.1469\n",
            "     35        \u001b[36m0.1034\u001b[0m        0.1460  2.1233\n",
            "     36        \u001b[36m0.1033\u001b[0m        0.1431  2.1133\n",
            "     37        \u001b[36m0.0998\u001b[0m        0.1422  2.4512\n",
            "     38        \u001b[36m0.0989\u001b[0m        0.1353  2.6114\n",
            "     39        \u001b[36m0.0960\u001b[0m        0.1588  2.1209\n",
            "     40        \u001b[36m0.0933\u001b[0m        0.1469  2.1253\n",
            "     41        0.0935        0.1644  2.1280\n",
            "     42        0.0942        0.1435  2.1194\n",
            "     43        \u001b[36m0.0906\u001b[0m        0.1321  2.4854\n",
            "     44        \u001b[36m0.0900\u001b[0m        0.1369  2.5337\n",
            "     45        \u001b[36m0.0890\u001b[0m        0.1363  2.1137\n",
            "     46        0.0906        0.1375  2.0944\n",
            "     47        0.0903        0.1357  2.1306\n",
            "     48        0.0910        0.1483  2.1064\n",
            "     49        0.0893        0.1426  2.5456\n",
            "     50        \u001b[36m0.0875\u001b[0m        0.1328  2.5317\n",
            "     51        0.0895        \u001b[32m0.1239\u001b[0m  2.1437\n",
            "     52        \u001b[36m0.0868\u001b[0m        \u001b[32m0.1192\u001b[0m  2.2945\n",
            "     53        0.0887        0.1493  2.6129\n",
            "     54        0.0886        \u001b[32m0.1122\u001b[0m  2.6390\n",
            "     55        0.0878        0.1348  2.7333\n",
            "     56        \u001b[36m0.0862\u001b[0m        0.1564  2.2131\n",
            "     57        0.0868        0.1508  2.1287\n",
            "     58        0.0872        0.1377  2.1261\n",
            "     59        0.0863        0.1614  2.1263\n",
            "     60        \u001b[36m0.0854\u001b[0m        0.2153  2.3106\n",
            "     61        0.0889        0.1323  2.7888\n",
            "     62        0.0870        0.1435  2.1297\n",
            "     63        0.0873        0.1248  2.1212\n",
            "     64        \u001b[36m0.0852\u001b[0m        0.1723  2.1481\n",
            "     65        0.0852        0.1481  2.1276\n",
            "     66        \u001b[36m0.0846\u001b[0m        0.1563  2.3210\n",
            "     67        0.0850        0.1488  2.7408\n",
            "     68        0.0848        0.1549  2.1280\n",
            "     69        0.0852        0.1224  2.1299\n",
            "     70        \u001b[36m0.0832\u001b[0m        0.1534  2.1311\n",
            "     71        0.0845        0.1458  2.1390\n",
            "     72        0.0850        0.1398  2.3667\n",
            "     73        0.0845        0.1445  2.7211\n",
            "     74        0.0844        0.1315  2.1217\n",
            "     75        0.0834        0.1329  2.1286\n",
            "     76        0.0835        0.1329  2.1294\n",
            "     77        0.0841        0.1570  2.1447\n",
            "     78        0.0842        0.1391  2.5512\n",
            "     79        0.0841        0.1522  3.3855\n",
            "     80        0.0861        0.1470  2.4099\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.1min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1489\u001b[0m        \u001b[32m0.2148\u001b[0m  2.1387\n",
            "      2        \u001b[36m0.1479\u001b[0m        0.2160  2.1263\n",
            "      3        \u001b[36m0.1462\u001b[0m        0.2175  2.1230\n",
            "      4        \u001b[36m0.1451\u001b[0m        0.2208  2.7285\n",
            "      5        0.1462        0.2220  2.2697\n",
            "      6        \u001b[36m0.1447\u001b[0m        0.2264  2.1663\n",
            "      7        0.1457        0.2288  2.1550\n",
            "      8        0.1458        0.2267  2.1508\n",
            "      9        \u001b[36m0.1435\u001b[0m        0.2323  2.3221\n",
            "     10        0.1458        0.2359  2.8014\n",
            "     11        0.1440        0.2336  2.1556\n",
            "     12        0.1446        0.2416  2.1643\n",
            "     13        0.1453        0.2465  2.1187\n",
            "     14        0.1448        0.2520  2.1085\n",
            "     15        0.1449        0.2480  2.3032\n",
            "     16        0.1451        0.2486  2.7693\n",
            "     17        \u001b[36m0.1434\u001b[0m        0.2440  2.1185\n",
            "     18        0.1443        0.2522  2.1121\n",
            "     19        0.1455        0.2663  2.1573\n",
            "     20        0.1442        0.2448  2.1050\n",
            "     21        0.1458        0.2618  2.3162\n",
            "     22        0.1439        0.2632  2.7287\n",
            "     23        0.1451        0.2788  2.1288\n",
            "     24        0.1455        0.2739  2.5417\n",
            "     25        0.1461        0.2491  2.6633\n",
            "     26        \u001b[36m0.1427\u001b[0m        0.2573  2.1786\n",
            "     27        0.1442        0.2648  2.6249\n",
            "     28        0.1438        0.2650  2.4104\n",
            "     29        \u001b[36m0.1422\u001b[0m        0.2621  2.1207\n",
            "     30        0.1425        0.2701  2.1188\n",
            "     31        \u001b[36m0.1404\u001b[0m        0.2528  2.1176\n",
            "     32        \u001b[36m0.1398\u001b[0m        0.2611  2.1480\n",
            "     33        \u001b[36m0.1393\u001b[0m        0.2601  2.6931\n",
            "     34        \u001b[36m0.1390\u001b[0m        0.2575  2.3068\n",
            "     35        0.1390        0.2590  2.1017\n",
            "     36        \u001b[36m0.1385\u001b[0m        0.2448  2.1140\n",
            "     37        \u001b[36m0.1381\u001b[0m        0.2707  2.1098\n",
            "     38        \u001b[36m0.1372\u001b[0m        0.2660  2.1308\n",
            "     39        0.1375        0.2657  2.7629\n",
            "     40        \u001b[36m0.1368\u001b[0m        0.2639  2.2195\n",
            "     41        \u001b[36m0.1342\u001b[0m        0.2566  2.1273\n",
            "     42        0.1352        0.2411  2.1198\n",
            "     43        0.1377        0.2423  2.1183\n",
            "     44        0.1387        0.2483  2.2542\n",
            "     45        0.1343        0.2489  2.7443\n",
            "     46        0.1344        0.2578  2.2026\n",
            "     47        0.1348        0.2538  2.1237\n",
            "     48        0.1359        0.2471  2.1333\n",
            "     49        \u001b[36m0.1339\u001b[0m        0.2485  2.1218\n",
            "     50        0.1352        0.2421  2.5008\n",
            "     51        0.1345        0.2457  3.5176\n",
            "     52        0.1346        0.2508  2.2839\n",
            "     53        0.1350        0.2412  2.1240\n",
            "     54        0.1357        0.2472  2.1248\n",
            "     55        \u001b[36m0.1332\u001b[0m        0.2576  2.1169\n",
            "     56        0.1334        0.2516  2.4420\n",
            "     57        \u001b[36m0.1326\u001b[0m        0.2462  2.6118\n",
            "     58        0.1353        0.2442  2.1505\n",
            "     59        0.1343        0.2484  2.1231\n",
            "     60        0.1373        0.2498  2.1197\n",
            "     61        0.1357        0.2563  2.1193\n",
            "     62        0.1329        0.2519  2.4690\n",
            "     63        \u001b[36m0.1315\u001b[0m        0.2426  2.5737\n",
            "     64        0.1420        0.2445  2.1214\n",
            "     65        0.1370        0.2588  2.1073\n",
            "     66        0.1359        0.2555  2.1182\n",
            "     67        0.1395        0.2546  2.1306\n",
            "     68        0.1351        0.2486  2.5605\n",
            "     69        0.1387        0.2675  2.4939\n",
            "     70        0.1328        0.2409  2.1170\n",
            "     71        0.1352        0.2280  2.1437\n",
            "     72        \u001b[36m0.1312\u001b[0m        0.2428  2.1170\n",
            "     73        0.1315        0.2450  2.1101\n",
            "     74        0.1335        0.2460  2.5937\n",
            "     75        0.1354        0.2481  2.4964\n",
            "     76        0.1355        0.2484  2.6147\n",
            "     77        0.1347        0.2539  2.6767\n",
            "     78        0.1362        0.2566  2.1643\n",
            "     79        0.1335        0.2600  2.3227\n",
            "     80        0.1373        0.2610  2.7423\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1581\u001b[0m        \u001b[32m0.2156\u001b[0m  2.2970\n",
            "      2        \u001b[36m0.1510\u001b[0m        \u001b[32m0.2155\u001b[0m  2.2883\n",
            "      3        0.1512        0.2171  2.3101\n",
            "      4        \u001b[36m0.1508\u001b[0m        0.2163  2.3179\n",
            "      5        0.1509        0.2171  2.9305\n",
            "      6        0.1510        \u001b[32m0.2144\u001b[0m  2.4629\n",
            "      7        \u001b[36m0.1498\u001b[0m        \u001b[32m0.2134\u001b[0m  2.3062\n",
            "      8        0.1503        0.2137  2.2926\n",
            "      9        \u001b[36m0.1493\u001b[0m        0.2171  2.2999\n",
            "     10        0.1498        0.2150  2.6354\n",
            "     11        \u001b[36m0.1489\u001b[0m        0.2153  2.7341\n",
            "     12        0.1492        \u001b[32m0.2124\u001b[0m  2.3100\n",
            "     13        \u001b[36m0.1480\u001b[0m        \u001b[32m0.2123\u001b[0m  2.3097\n",
            "     14        \u001b[36m0.1461\u001b[0m        0.2137  2.2944\n",
            "     15        \u001b[36m0.1456\u001b[0m        0.2153  2.3737\n",
            "     16        \u001b[36m0.1432\u001b[0m        0.2244  3.0392\n",
            "     17        \u001b[36m0.1414\u001b[0m        0.2345  2.2954\n",
            "     18        \u001b[36m0.1397\u001b[0m        0.2165  2.2943\n",
            "     19        0.1403        0.2133  2.3311\n",
            "     20        0.1441        0.2308  2.5145\n",
            "     21        0.1491        \u001b[32m0.1987\u001b[0m  3.7554\n",
            "     22        0.1435        \u001b[32m0.1987\u001b[0m  2.5362\n",
            "     23        \u001b[36m0.1395\u001b[0m        \u001b[32m0.1921\u001b[0m  2.2792\n",
            "     24        \u001b[36m0.1352\u001b[0m        0.1932  2.3084\n",
            "     25        \u001b[36m0.1335\u001b[0m        \u001b[32m0.1719\u001b[0m  2.2778\n",
            "     26        \u001b[36m0.1256\u001b[0m        0.1762  2.6246\n",
            "     27        \u001b[36m0.1232\u001b[0m        \u001b[32m0.1588\u001b[0m  2.7515\n",
            "     28        0.1232        0.1601  2.3221\n",
            "     29        \u001b[36m0.1220\u001b[0m        0.1631  2.2966\n",
            "     30        \u001b[36m0.1212\u001b[0m        \u001b[32m0.1567\u001b[0m  2.2901\n",
            "     31        0.1263        \u001b[32m0.1480\u001b[0m  2.3210\n",
            "     32        0.1237        \u001b[32m0.1442\u001b[0m  3.0136\n",
            "     33        0.1224        0.1501  2.3682\n",
            "     34        0.1244        \u001b[32m0.1404\u001b[0m  2.2989\n",
            "     35        0.1216        \u001b[32m0.1384\u001b[0m  2.3057\n",
            "     36        0.1223        0.1418  2.2999\n",
            "     37        0.1224        \u001b[32m0.1253\u001b[0m  2.7167\n",
            "     38        0.1215        0.1499  2.7047\n",
            "     39        0.1245        0.1300  2.3077\n",
            "     40        0.1223        0.1305  2.3243\n",
            "     41        0.1213        \u001b[32m0.1168\u001b[0m  2.2823\n",
            "     42        \u001b[36m0.1209\u001b[0m        0.1264  2.3967\n",
            "     43        \u001b[36m0.1196\u001b[0m        0.1314  3.0307\n",
            "     44        \u001b[36m0.1186\u001b[0m        0.1326  2.5799\n",
            "     45        0.1190        0.1294  2.8777\n",
            "     46        0.1192        0.1312  2.5271\n",
            "     47        0.1197        0.1286  2.3798\n",
            "     48        0.1186        0.1527  3.0106\n",
            "     49        \u001b[36m0.1118\u001b[0m        \u001b[32m0.1162\u001b[0m  2.3170\n",
            "     50        0.1133        \u001b[32m0.1052\u001b[0m  2.2949\n",
            "     51        \u001b[36m0.1080\u001b[0m        0.1198  2.2777\n",
            "     52        \u001b[36m0.1069\u001b[0m        0.1258  2.3253\n",
            "     53        \u001b[36m0.1051\u001b[0m        0.1259  2.7731\n",
            "     54        \u001b[36m0.1043\u001b[0m        0.1205  2.6639\n",
            "     55        0.1063        0.1331  2.3446\n",
            "     56        \u001b[36m0.1041\u001b[0m        0.1308  2.3449\n",
            "     57        \u001b[36m0.1026\u001b[0m        0.1341  2.3406\n",
            "     58        0.1041        0.1332  2.5743\n",
            "     59        \u001b[36m0.1026\u001b[0m        0.1324  2.9891\n",
            "     60        \u001b[36m0.1022\u001b[0m        0.1427  2.3296\n",
            "     61        0.1033        0.1282  2.3157\n",
            "     62        0.1035        0.1305  2.3083\n",
            "     63        0.1042        0.1297  2.2982\n",
            "     64        \u001b[36m0.0994\u001b[0m        0.1325  2.9135\n",
            "     65        0.1006        0.1333  2.5712\n",
            "     66        0.1008        0.1398  2.2938\n",
            "     67        \u001b[36m0.0979\u001b[0m        0.1411  2.3194\n",
            "     68        0.1002        0.1439  2.6751\n",
            "     69        0.1005        0.1443  3.4813\n",
            "     70        0.1021        0.1431  2.6915\n",
            "     71        0.0994        0.1456  2.3040\n",
            "     72        0.0988        0.1417  2.2999\n",
            "     73        \u001b[36m0.0977\u001b[0m        0.1410  2.2999\n",
            "     74        0.0987        0.1490  2.4705\n",
            "     75        0.0986        0.1399  3.0009\n",
            "     76        0.0988        0.1443  2.3148\n",
            "     77        0.1014        0.1306  2.3053\n",
            "     78        \u001b[36m0.0969\u001b[0m        0.1580  2.2984\n",
            "     79        0.0997        0.1322  2.3068\n",
            "     80        0.0995        0.1535  2.8190\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.3min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1510\u001b[0m        \u001b[32m0.2166\u001b[0m  2.3773\n",
            "      2        0.1526        \u001b[32m0.2149\u001b[0m  2.2911\n",
            "      3        0.1529        0.2164  2.3026\n",
            "      4        0.1518        0.2173  2.2951\n",
            "      5        0.1521        0.2158  2.6943\n",
            "      6        \u001b[36m0.1510\u001b[0m        0.2165  2.7435\n",
            "      7        0.1520        0.2150  2.2723\n",
            "      8        \u001b[36m0.1508\u001b[0m        \u001b[32m0.2126\u001b[0m  2.3087\n",
            "      9        \u001b[36m0.1505\u001b[0m        0.2140  2.2896\n",
            "     10        \u001b[36m0.1503\u001b[0m        0.2134  2.3235\n",
            "     11        0.1505        0.2149  3.0792\n",
            "     12        \u001b[36m0.1503\u001b[0m        0.2192  2.8180\n",
            "     13        0.1508        0.2147  2.8328\n",
            "     14        \u001b[36m0.1500\u001b[0m        0.2164  2.3038\n",
            "     15        \u001b[36m0.1494\u001b[0m        0.2187  2.3814\n",
            "     16        0.1507        0.2181  3.0127\n",
            "     17        \u001b[36m0.1484\u001b[0m        0.2191  2.3290\n",
            "     18        0.1484        0.2187  2.2972\n",
            "     19        \u001b[36m0.1483\u001b[0m        0.2187  2.3007\n",
            "     20        \u001b[36m0.1474\u001b[0m        0.2174  2.3254\n",
            "     21        0.1479        0.2188  2.7390\n",
            "     22        0.1481        0.2194  2.7103\n",
            "     23        0.1476        0.2223  2.2931\n",
            "     24        0.1478        0.2216  2.2909\n",
            "     25        0.1477        0.2203  2.2941\n",
            "     26        \u001b[36m0.1474\u001b[0m        0.2200  2.4083\n",
            "     27        0.1481        0.2215  3.0175\n",
            "     28        0.1479        0.2211  2.2930\n",
            "     29        0.1475        0.2222  2.3011\n",
            "     30        \u001b[36m0.1471\u001b[0m        0.2220  2.3054\n",
            "     31        0.1482        0.2217  2.2988\n",
            "     32        0.1473        0.2232  2.7579\n",
            "     33        0.1477        0.2221  2.7009\n",
            "     34        0.1479        0.2242  2.2843\n",
            "     35        \u001b[36m0.1465\u001b[0m        0.2240  2.3955\n",
            "     36        0.1480        0.2254  2.8289\n",
            "     37        0.1466        0.2272  3.1491\n",
            "     38        0.1475        0.2243  2.7657\n",
            "     39        0.1469        0.2295  2.3093\n",
            "     40        0.1466        0.2304  2.2939\n",
            "     41        0.1473        0.2316  2.2880\n",
            "     42        0.1475        0.2279  2.3503\n",
            "     43        0.1481        0.2379  3.0300\n",
            "     44        0.1486        0.2350  2.3886\n",
            "     45        \u001b[36m0.1462\u001b[0m        0.2356  2.2854\n",
            "     46        0.1476        0.2424  2.2938\n",
            "     47        0.1472        0.2402  2.2951\n",
            "     48        0.1482        0.2395  2.6981\n",
            "     49        0.1479        0.2431  2.7310\n",
            "     50        0.1492        0.2307  2.3049\n",
            "     51        0.1497        0.2408  2.2952\n",
            "     52        0.1486        0.2394  2.3297\n",
            "     53        0.1494        0.2355  2.4048\n",
            "     54        0.1499        0.2421  3.0189\n",
            "     55        0.1482        0.2456  2.3115\n",
            "     56        0.1499        0.2354  2.3397\n",
            "     57        0.1493        0.2533  2.2911\n",
            "     58        0.1495        0.2358  2.2983\n",
            "     59        0.1490        0.2535  2.8086\n",
            "     60        0.1476        0.2514  3.2460\n",
            "     61        0.1489        0.2445  2.7180\n",
            "     62        0.1488        0.2545  2.3137\n",
            "     63        0.1490        0.2588  2.3064\n",
            "     64        0.1487        0.2528  2.7751\n",
            "     65        \u001b[36m0.1462\u001b[0m        0.2534  2.7069\n",
            "     66        0.1477        0.2498  2.4346\n",
            "     67        0.1497        0.2551  2.3068\n",
            "     68        0.1486        0.2575  2.3273\n",
            "     69        0.1480        0.2619  2.4773\n",
            "     70        0.1480        0.2588  2.9603\n",
            "     71        0.1487        0.2452  2.2965\n",
            "     72        0.1474        0.2424  2.2895\n",
            "     73        0.1493        0.2541  2.3003\n",
            "     74        0.1486        0.2516  2.2859\n",
            "     75        0.1479        0.2546  2.8145\n",
            "     76        0.1483        0.2566  2.6516\n",
            "     77        0.1476        0.2540  2.3044\n",
            "     78        0.1472        0.2523  2.3102\n",
            "     79        0.1483        0.2578  2.3079\n",
            "     80        0.1485        0.2553  2.5438\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.3min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1532\u001b[0m        \u001b[32m0.2376\u001b[0m  2.6028\n",
            "      2        \u001b[36m0.1530\u001b[0m        \u001b[32m0.2247\u001b[0m  2.1141\n",
            "      3        \u001b[36m0.1519\u001b[0m        \u001b[32m0.2220\u001b[0m  2.1611\n",
            "      4        \u001b[36m0.1497\u001b[0m        \u001b[32m0.2210\u001b[0m  2.5851\n",
            "      5        \u001b[36m0.1477\u001b[0m        0.2233  2.7499\n",
            "      6        \u001b[36m0.1466\u001b[0m        0.2230  2.7746\n",
            "      7        \u001b[36m0.1464\u001b[0m        \u001b[32m0.2177\u001b[0m  2.1805\n",
            "      8        \u001b[36m0.1447\u001b[0m        0.2236  2.1318\n",
            "      9        \u001b[36m0.1440\u001b[0m        0.2267  2.1289\n",
            "     10        \u001b[36m0.1436\u001b[0m        0.2208  2.1328\n",
            "     11        \u001b[36m0.1415\u001b[0m        0.2391  2.2954\n",
            "     12        \u001b[36m0.1401\u001b[0m        0.2392  2.8062\n",
            "     13        \u001b[36m0.1383\u001b[0m        0.2380  2.1294\n",
            "     14        \u001b[36m0.1369\u001b[0m        0.2320  2.1240\n",
            "     15        \u001b[36m0.1353\u001b[0m        0.2251  2.1365\n",
            "     16        \u001b[36m0.1349\u001b[0m        0.2207  2.1318\n",
            "     17        \u001b[36m0.1343\u001b[0m        \u001b[32m0.2156\u001b[0m  2.3153\n",
            "     18        \u001b[36m0.1337\u001b[0m        \u001b[32m0.2099\u001b[0m  2.7112\n",
            "     19        \u001b[36m0.1327\u001b[0m        \u001b[32m0.2001\u001b[0m  2.1337\n",
            "     20        \u001b[36m0.1318\u001b[0m        \u001b[32m0.1956\u001b[0m  2.1689\n",
            "     21        \u001b[36m0.1314\u001b[0m        \u001b[32m0.1901\u001b[0m  2.1723\n",
            "     22        \u001b[36m0.1310\u001b[0m        \u001b[32m0.1853\u001b[0m  2.1717\n",
            "     23        \u001b[36m0.1307\u001b[0m        \u001b[32m0.1812\u001b[0m  2.4562\n",
            "     24        \u001b[36m0.1303\u001b[0m        \u001b[32m0.1780\u001b[0m  2.6577\n",
            "     25        \u001b[36m0.1301\u001b[0m        \u001b[32m0.1761\u001b[0m  2.1627\n",
            "     26        \u001b[36m0.1298\u001b[0m        \u001b[32m0.1740\u001b[0m  2.1451\n",
            "     27        \u001b[36m0.1296\u001b[0m        \u001b[32m0.1718\u001b[0m  2.1272\n",
            "     28        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1700\u001b[0m  2.1252\n",
            "     29        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1673\u001b[0m  2.7059\n",
            "     30        \u001b[36m0.1287\u001b[0m        \u001b[32m0.1654\u001b[0m  3.2662\n",
            "     31        0.1292        \u001b[32m0.1546\u001b[0m  2.3000\n",
            "     32        \u001b[36m0.1278\u001b[0m        0.1869  2.1495\n",
            "     33        0.1291        0.1620  2.1321\n",
            "     34        \u001b[36m0.1267\u001b[0m        0.1566  2.1288\n",
            "     35        \u001b[36m0.1258\u001b[0m        0.1937  2.6618\n",
            "     36        0.1273        0.1761  2.4059\n",
            "     37        \u001b[36m0.1228\u001b[0m        0.1683  2.1282\n",
            "     38        \u001b[36m0.1148\u001b[0m        0.1690  2.1707\n",
            "     39        \u001b[36m0.1133\u001b[0m        0.1610  2.1239\n",
            "     40        \u001b[36m0.1114\u001b[0m        0.1586  2.1288\n",
            "     41        \u001b[36m0.1105\u001b[0m        \u001b[32m0.1500\u001b[0m  2.7343\n",
            "     42        0.1107        0.1515  2.3011\n",
            "     43        0.1109        0.1531  2.1326\n",
            "     44        \u001b[36m0.1058\u001b[0m        0.1548  2.1259\n",
            "     45        \u001b[36m0.1039\u001b[0m        0.1551  2.1281\n",
            "     46        \u001b[36m0.1018\u001b[0m        \u001b[32m0.1462\u001b[0m  2.2068\n",
            "     47        \u001b[36m0.0961\u001b[0m        0.1723  2.7383\n",
            "     48        \u001b[36m0.0959\u001b[0m        0.1690  2.2252\n",
            "     49        \u001b[36m0.0927\u001b[0m        0.1779  2.1291\n",
            "     50        0.0954        \u001b[32m0.1369\u001b[0m  2.1467\n",
            "     51        0.0932        0.1545  2.1367\n",
            "     52        \u001b[36m0.0904\u001b[0m        0.1447  2.3054\n",
            "     53        0.0907        0.1663  2.7473\n",
            "     54        0.0924        0.1384  2.1731\n",
            "     55        0.0923        0.1738  2.3083\n",
            "     56        \u001b[36m0.0889\u001b[0m        0.1620  2.6390\n",
            "     57        0.0924        0.1670  2.4448\n",
            "     58        0.0915        0.1475  2.6195\n",
            "     59        0.0906        0.1502  2.5088\n",
            "     60        0.0892        \u001b[32m0.1351\u001b[0m  2.1174\n",
            "     61        \u001b[36m0.0883\u001b[0m        \u001b[32m0.1342\u001b[0m  2.1237\n",
            "     62        0.0888        0.1629  2.1316\n",
            "     63        0.0886        0.1546  2.1233\n",
            "     64        0.0891        0.1753  2.6527\n",
            "     65        0.0890        0.1816  2.4623\n",
            "     66        0.0886        0.1430  2.1304\n",
            "     67        0.0884        0.1389  2.1483\n",
            "     68        \u001b[36m0.0880\u001b[0m        0.1481  2.1399\n",
            "     69        \u001b[36m0.0874\u001b[0m        0.1617  2.1332\n",
            "     70        \u001b[36m0.0870\u001b[0m        0.1591  2.6684\n",
            "     71        0.0874        0.1521  2.3945\n",
            "     72        \u001b[36m0.0869\u001b[0m        0.1735  2.1444\n",
            "     73        0.0873        0.1519  2.1471\n",
            "     74        \u001b[36m0.0858\u001b[0m        0.1454  2.1322\n",
            "     75        0.0880        0.1664  2.1480\n",
            "     76        \u001b[36m0.0856\u001b[0m        \u001b[32m0.1318\u001b[0m  2.7391\n",
            "     77        0.0878        0.1580  2.3435\n",
            "     78        \u001b[36m0.0852\u001b[0m        \u001b[32m0.1294\u001b[0m  2.1372\n",
            "     79        0.0869        0.1439  2.1283\n",
            "     80        \u001b[36m0.0848\u001b[0m        0.1411  2.1537\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.1min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1567\u001b[0m        \u001b[32m0.2787\u001b[0m  2.8897\n",
            "      2        \u001b[36m0.1566\u001b[0m        \u001b[32m0.2608\u001b[0m  3.2393\n",
            "      3        \u001b[36m0.1548\u001b[0m        \u001b[32m0.2584\u001b[0m  2.1470\n",
            "      4        \u001b[36m0.1541\u001b[0m        0.2626  2.1358\n",
            "      5        \u001b[36m0.1523\u001b[0m        0.2649  2.1373\n",
            "      6        \u001b[36m0.1518\u001b[0m        \u001b[32m0.2458\u001b[0m  2.1197\n",
            "      7        \u001b[36m0.1495\u001b[0m        \u001b[32m0.2405\u001b[0m  2.5020\n",
            "      8        \u001b[36m0.1486\u001b[0m        \u001b[32m0.2387\u001b[0m  2.5131\n",
            "      9        \u001b[36m0.1472\u001b[0m        \u001b[32m0.2295\u001b[0m  2.1342\n",
            "     10        \u001b[36m0.1455\u001b[0m        \u001b[32m0.2188\u001b[0m  2.1561\n",
            "     11        \u001b[36m0.1443\u001b[0m        \u001b[32m0.2174\u001b[0m  2.1267\n",
            "     12        \u001b[36m0.1431\u001b[0m        \u001b[32m0.2141\u001b[0m  2.1298\n",
            "     13        0.1431        0.2190  2.5982\n",
            "     14        \u001b[36m0.1411\u001b[0m        \u001b[32m0.2029\u001b[0m  2.4734\n",
            "     15        \u001b[36m0.1393\u001b[0m        \u001b[32m0.2000\u001b[0m  2.1101\n",
            "     16        \u001b[36m0.1383\u001b[0m        \u001b[32m0.1957\u001b[0m  2.1221\n",
            "     17        \u001b[36m0.1383\u001b[0m        \u001b[32m0.1905\u001b[0m  2.1184\n",
            "     18        \u001b[36m0.1378\u001b[0m        \u001b[32m0.1844\u001b[0m  2.1206\n",
            "     19        \u001b[36m0.1365\u001b[0m        \u001b[32m0.1838\u001b[0m  2.5985\n",
            "     20        \u001b[36m0.1354\u001b[0m        0.1843  2.4595\n",
            "     21        0.1361        0.1896  2.1446\n",
            "     22        0.1365        0.1937  2.1131\n",
            "     23        0.1356        0.1909  2.1478\n",
            "     24        0.1357        0.1942  2.1397\n",
            "     25        0.1357        0.1898  2.6611\n",
            "     26        0.1360        0.1891  2.4383\n",
            "     27        \u001b[36m0.1349\u001b[0m        0.1931  2.5666\n",
            "     28        0.1356        0.1942  2.6630\n",
            "     29        0.1377        0.1975  2.1819\n",
            "     30        0.1374        0.2000  2.3992\n",
            "     31        0.1355        0.1934  2.6873\n",
            "     32        0.1361        0.1942  2.1426\n",
            "     33        0.1363        0.1939  2.1273\n",
            "     34        0.1369        0.2031  2.1335\n",
            "     35        0.1355        0.1896  2.1391\n",
            "     36        0.1355        0.1930  2.4831\n",
            "     37        0.1352        0.1877  2.6331\n",
            "     38        0.1350        0.1896  2.1249\n",
            "     39        \u001b[36m0.1342\u001b[0m        0.1862  2.1371\n",
            "     40        0.1356        0.1945  2.1209\n",
            "     41        0.1348        0.1853  2.1315\n",
            "     42        0.1361        0.1875  2.4754\n",
            "     43        0.1346        \u001b[32m0.1779\u001b[0m  2.5795\n",
            "     44        \u001b[36m0.1332\u001b[0m        0.1792  2.1267\n",
            "     45        0.1356        0.1869  2.1408\n",
            "     46        0.1345        0.1838  2.1321\n",
            "     47        0.1336        0.1792  2.1264\n",
            "     48        \u001b[36m0.1329\u001b[0m        0.1794  2.5107\n",
            "     49        0.1355        0.1870  2.6073\n",
            "     50        0.1383        0.1934  2.1199\n",
            "     51        0.1367        0.1784  2.1144\n",
            "     52        0.1353        0.1797  2.1993\n",
            "     53        0.1344        0.1833  2.5493\n",
            "     54        0.1350        0.1874  3.3163\n",
            "     55        0.1347        0.1886  2.3444\n",
            "     56        0.1344        0.1868  2.1266\n",
            "     57        0.1343        0.1860  2.1391\n",
            "     58        0.1352        0.1813  2.1277\n",
            "     59        0.1359        0.1870  2.2318\n",
            "     60        0.1358        0.1811  2.7422\n",
            "     61        0.1339        \u001b[32m0.1776\u001b[0m  2.2282\n",
            "     62        0.1343        \u001b[32m0.1768\u001b[0m  2.1588\n",
            "     63        0.1363        \u001b[32m0.1768\u001b[0m  2.1254\n",
            "     64        0.1358        \u001b[32m0.1727\u001b[0m  2.1363\n",
            "     65        0.1340        \u001b[32m0.1711\u001b[0m  2.2836\n",
            "     66        0.1333        0.1734  2.7504\n",
            "     67        0.1338        \u001b[32m0.1700\u001b[0m  2.1555\n",
            "     68        0.1334        0.1751  2.1268\n",
            "     69        0.1340        0.1728  2.1324\n",
            "     70        0.1337        0.1735  2.1370\n",
            "     71        0.1343        0.1783  2.3221\n",
            "     72        0.1341        0.1755  2.7033\n",
            "     73        0.1367        0.1831  2.1421\n",
            "     74        0.1376        0.1935  2.1618\n",
            "     75        0.1375        0.1919  2.2023\n",
            "     76        0.1376        0.1925  2.1578\n",
            "     77        0.1345        0.1809  2.4621\n",
            "     78        0.1362        0.1773  2.8020\n",
            "     79        0.1357        0.1863  2.6392\n",
            "     80        0.1337        0.1775  2.5565\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1605\u001b[0m        \u001b[32m0.2201\u001b[0m  2.3357\n",
            "      2        \u001b[36m0.1516\u001b[0m        \u001b[32m0.2177\u001b[0m  2.6000\n",
            "      3        \u001b[36m0.1510\u001b[0m        0.2193  2.8629\n",
            "      4        0.1523        0.2224  2.3067\n",
            "      5        \u001b[36m0.1502\u001b[0m        0.2186  2.2854\n",
            "      6        0.1513        0.2206  2.2964\n",
            "      7        0.1524        0.2188  2.3233\n",
            "      8        0.1509        \u001b[32m0.2171\u001b[0m  2.9565\n",
            "      9        0.1519        0.2178  2.4436\n",
            "     10        0.1515        0.2177  2.3006\n",
            "     11        0.1502        0.2174  2.3109\n",
            "     12        0.1511        \u001b[32m0.2165\u001b[0m  2.2977\n",
            "     13        0.1514        \u001b[32m0.2162\u001b[0m  2.6738\n",
            "     14        0.1509        \u001b[32m0.2135\u001b[0m  2.7786\n",
            "     15        \u001b[36m0.1500\u001b[0m        0.2141  2.2992\n",
            "     16        0.1545        \u001b[32m0.2121\u001b[0m  2.3141\n",
            "     17        0.1515        0.2127  2.2984\n",
            "     18        0.1531        \u001b[32m0.2064\u001b[0m  2.3589\n",
            "     19        0.1520        \u001b[32m0.2040\u001b[0m  3.0207\n",
            "     20        0.1514        0.2055  2.3932\n",
            "     21        0.1560        0.2209  2.3160\n",
            "     22        0.1529        0.2152  2.3372\n",
            "     23        0.1528        0.2136  2.8090\n",
            "     24        0.1524        0.2137  3.4717\n",
            "     25        0.1527        0.2102  2.4582\n",
            "     26        0.1520        0.2074  2.3184\n",
            "     27        0.1504        0.2077  2.3062\n",
            "     28        0.1517        0.2135  2.3003\n",
            "     29        0.1524        0.2090  2.6863\n",
            "     30        0.1537        0.2070  2.7490\n",
            "     31        0.1530        0.2073  2.3257\n",
            "     32        0.1518        \u001b[32m0.2036\u001b[0m  2.3123\n",
            "     33        0.1531        0.2069  2.3284\n",
            "     34        \u001b[36m0.1485\u001b[0m        \u001b[32m0.1992\u001b[0m  2.3952\n",
            "     35        \u001b[36m0.1469\u001b[0m        \u001b[32m0.1950\u001b[0m  3.0049\n",
            "     36        \u001b[36m0.1450\u001b[0m        \u001b[32m0.1890\u001b[0m  2.3517\n",
            "     37        \u001b[36m0.1433\u001b[0m        \u001b[32m0.1838\u001b[0m  2.2868\n",
            "     38        \u001b[36m0.1411\u001b[0m        \u001b[32m0.1776\u001b[0m  2.3040\n",
            "     39        \u001b[36m0.1392\u001b[0m        \u001b[32m0.1638\u001b[0m  2.3041\n",
            "     40        0.1395        0.1656  2.7299\n",
            "     41        \u001b[36m0.1389\u001b[0m        \u001b[32m0.1440\u001b[0m  2.7190\n",
            "     42        0.1424        0.2070  2.3136\n",
            "     43        0.1413        0.1919  2.3120\n",
            "     44        \u001b[36m0.1319\u001b[0m        0.1681  2.3598\n",
            "     45        \u001b[36m0.1319\u001b[0m        0.1731  2.4219\n",
            "     46        \u001b[36m0.1239\u001b[0m        0.1740  3.4382\n",
            "     47        \u001b[36m0.1223\u001b[0m        0.1623  2.8827\n",
            "     48        0.1223        0.1560  2.4074\n",
            "     49        \u001b[36m0.1194\u001b[0m        0.1535  2.3321\n",
            "     50        \u001b[36m0.1178\u001b[0m        \u001b[32m0.1402\u001b[0m  2.3251\n",
            "     51        \u001b[36m0.1174\u001b[0m        \u001b[32m0.1390\u001b[0m  3.0061\n",
            "     52        \u001b[36m0.1156\u001b[0m        \u001b[32m0.1374\u001b[0m  2.4169\n",
            "     53        \u001b[36m0.1139\u001b[0m        0.1404  2.3154\n",
            "     54        \u001b[36m0.1133\u001b[0m        \u001b[32m0.1351\u001b[0m  2.3148\n",
            "     55        \u001b[36m0.1133\u001b[0m        0.1366  2.3263\n",
            "     56        \u001b[36m0.1129\u001b[0m        0.1359  2.6917\n",
            "     57        \u001b[36m0.1122\u001b[0m        \u001b[32m0.1329\u001b[0m  2.7621\n",
            "     58        \u001b[36m0.1117\u001b[0m        0.1342  2.2992\n",
            "     59        \u001b[36m0.1113\u001b[0m        0.1389  2.3207\n",
            "     60        \u001b[36m0.1112\u001b[0m        0.1341  2.3038\n",
            "     61        \u001b[36m0.1087\u001b[0m        \u001b[32m0.1322\u001b[0m  2.3607\n",
            "     62        \u001b[36m0.1080\u001b[0m        0.1342  3.0226\n",
            "     63        \u001b[36m0.1079\u001b[0m        0.1395  2.3332\n",
            "     64        \u001b[36m0.1073\u001b[0m        0.1401  2.3548\n",
            "     65        \u001b[36m0.1064\u001b[0m        \u001b[32m0.1316\u001b[0m  2.3034\n",
            "     66        0.1070        0.1329  2.3120\n",
            "     67        \u001b[36m0.1040\u001b[0m        \u001b[32m0.1316\u001b[0m  2.8020\n",
            "     68        \u001b[36m0.1033\u001b[0m        0.1378  2.7022\n",
            "     69        \u001b[36m0.1029\u001b[0m        0.1354  2.3101\n",
            "     70        \u001b[36m0.1006\u001b[0m        \u001b[32m0.1303\u001b[0m  2.5444\n",
            "     71        0.1032        0.1317  2.8073\n",
            "     72        \u001b[36m0.0998\u001b[0m        0.1332  2.9736\n",
            "     73        \u001b[36m0.0991\u001b[0m        0.1369  2.7042\n",
            "     74        0.1034        0.1389  2.2912\n",
            "     75        \u001b[36m0.0986\u001b[0m        0.1356  2.3034\n",
            "     76        0.1101        0.1422  2.3150\n",
            "     77        \u001b[36m0.0974\u001b[0m        0.1344  2.4505\n",
            "     78        \u001b[36m0.0968\u001b[0m        0.1339  3.0096\n",
            "     79        0.1145        0.2026  2.3307\n",
            "     80        0.1076        0.1359  2.3227\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 3.4min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1608\u001b[0m        \u001b[32m0.2173\u001b[0m  2.3112\n",
            "      2        \u001b[36m0.1527\u001b[0m        0.2182  2.2956\n",
            "      3        \u001b[36m0.1522\u001b[0m        0.2220  2.9418\n",
            "      4        0.1526        0.2173  2.4349\n",
            "      5        0.1546        0.2203  2.3041\n",
            "      6        0.1537        0.2187  2.2974\n",
            "      7        \u001b[36m0.1519\u001b[0m        0.2180  2.2908\n",
            "      8        0.1520        0.2180  2.6548\n",
            "      9        \u001b[36m0.1514\u001b[0m        0.2191  2.7671\n",
            "     10        \u001b[36m0.1513\u001b[0m        0.2180  2.3128\n",
            "     11        0.1519        0.2180  2.3399\n",
            "     12        \u001b[36m0.1503\u001b[0m        0.2183  2.3001\n",
            "     13        \u001b[36m0.1501\u001b[0m        0.2176  2.3733\n",
            "     14        0.1519        0.2190  3.8338\n",
            "     15        \u001b[36m0.1499\u001b[0m        0.2181  2.7101\n",
            "     16        \u001b[36m0.1493\u001b[0m        0.2187  2.3651\n",
            "     17        0.1580        0.2206  2.3069\n",
            "     18        0.1505        0.2180  2.3036\n",
            "     19        0.1514        0.2177  2.9178\n",
            "     20        0.1509        0.2182  2.5275\n",
            "     21        0.1515        0.2175  2.2855\n",
            "     22        0.1513        \u001b[32m0.2170\u001b[0m  2.2899\n",
            "     23        0.1508        0.2173  2.2941\n",
            "     24        0.1495        0.2175  2.5751\n",
            "     25        0.1507        0.2191  2.8970\n",
            "     26        \u001b[36m0.1484\u001b[0m        0.2176  2.3078\n",
            "     27        0.1486        0.2171  2.3023\n",
            "     28        0.1526        0.2175  2.3026\n",
            "     29        0.1488        0.2185  2.2981\n",
            "     30        0.1532        0.2179  2.9058\n",
            "     31        0.1520        0.2174  2.5398\n",
            "     32        \u001b[36m0.1483\u001b[0m        0.2175  2.2838\n",
            "     33        0.1486        0.2190  2.3135\n",
            "     34        0.1536        0.2173  2.3179\n",
            "     35        0.1515        0.2177  2.6169\n",
            "     36        0.1518        \u001b[32m0.2167\u001b[0m  2.8242\n",
            "     37        0.1507        \u001b[32m0.2163\u001b[0m  2.3317\n",
            "     38        0.1491        \u001b[32m0.2150\u001b[0m  2.8465\n",
            "     39        0.1505        0.2159  2.8879\n",
            "     40        0.1511        \u001b[32m0.2146\u001b[0m  2.6523\n",
            "     41        0.1509        0.2165  2.8388\n",
            "     42        0.1510        0.2153  2.3164\n",
            "     43        0.1510        0.2152  2.3444\n",
            "     44        \u001b[36m0.1480\u001b[0m        \u001b[32m0.2131\u001b[0m  2.3014\n",
            "     45        0.1504        0.2133  2.3056\n",
            "     46        0.1516        0.2149  3.0150\n",
            "     47        0.1507        0.2185  2.4367\n",
            "     48        0.1527        0.2142  2.3184\n",
            "     49        0.1582        \u001b[32m0.2104\u001b[0m  2.3118\n",
            "     50        0.1526        0.2149  2.3143\n",
            "     51        0.1584        0.2126  2.7004\n",
            "     52        0.1521        0.2152  2.7514\n",
            "     53        0.1591        0.2124  2.3073\n",
            "     54        0.1578        0.2155  2.3013\n",
            "     55        0.1617        0.2140  2.2888\n",
            "     56        0.1575        0.2151  2.3511\n",
            "     57        0.1621        0.2139  3.0245\n",
            "     58        0.1632        0.2131  2.3860\n",
            "     59        0.1640        0.2127  2.3358\n",
            "     60        0.1648        0.2138  2.3043\n",
            "     61        0.1649        0.2154  2.4474\n",
            "     62        0.1642        0.2155  3.6772\n",
            "     63        0.1644        0.2166  2.7568\n",
            "     64        0.1634        0.2157  2.3187\n",
            "     65        0.1628        0.2158  2.3084\n",
            "     66        0.1645        0.2164  2.3042\n",
            "     67        0.1669        0.2155  2.6258\n",
            "     68        0.1705        0.2105  2.7596\n",
            "     69        0.1651        0.2171  2.2944\n",
            "     70        0.1587        0.2185  2.3182\n",
            "     71        0.1610        0.2173  2.3353\n",
            "     72        0.1595        0.2160  2.3904\n",
            "     73        0.1624        0.2176  3.0181\n",
            "     74        0.1614        0.2176  2.3810\n",
            "     75        0.1633        0.2195  2.3079\n",
            "     76        0.1634        0.2179  2.3276\n",
            "     77        0.1638        0.2181  2.3093\n",
            "     78        0.1642        0.2258  2.7200\n",
            "     79        0.1706        0.2113  2.7216\n",
            "     80        0.1662        0.2179  2.2997\n",
            "[CV] END batch_size=16, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 3.4min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1546\u001b[0m        \u001b[32m0.2339\u001b[0m  1.1383\n",
            "      2        \u001b[36m0.1425\u001b[0m        \u001b[32m0.2254\u001b[0m  1.1415\n",
            "      3        \u001b[36m0.1398\u001b[0m        \u001b[32m0.2194\u001b[0m  1.1388\n",
            "      4        \u001b[36m0.1356\u001b[0m        \u001b[32m0.2127\u001b[0m  1.1336\n",
            "      5        \u001b[36m0.1297\u001b[0m        \u001b[32m0.2120\u001b[0m  1.1385\n",
            "      6        \u001b[36m0.1259\u001b[0m        \u001b[32m0.2068\u001b[0m  1.4049\n",
            "      7        \u001b[36m0.1233\u001b[0m        0.2103  1.4880\n",
            "      8        \u001b[36m0.1217\u001b[0m        \u001b[32m0.2027\u001b[0m  1.4079\n",
            "      9        0.1219        \u001b[32m0.1952\u001b[0m  1.1587\n",
            "     10        0.1241        0.1998  1.3897\n",
            "     11        \u001b[36m0.1192\u001b[0m        0.2031  1.3526\n",
            "     12        \u001b[36m0.1166\u001b[0m        \u001b[32m0.1739\u001b[0m  1.4666\n",
            "     13        \u001b[36m0.1154\u001b[0m        \u001b[32m0.1709\u001b[0m  1.3362\n",
            "     14        \u001b[36m0.1140\u001b[0m        \u001b[32m0.1463\u001b[0m  1.1312\n",
            "     15        \u001b[36m0.1137\u001b[0m        \u001b[32m0.1354\u001b[0m  1.1384\n",
            "     16        \u001b[36m0.1136\u001b[0m        0.1413  1.3290\n",
            "     17        \u001b[36m0.1107\u001b[0m        0.1378  1.4696\n",
            "     18        \u001b[36m0.1075\u001b[0m        0.1355  1.4827\n",
            "     19        \u001b[36m0.1058\u001b[0m        0.1399  1.1365\n",
            "     20        \u001b[36m0.1020\u001b[0m        0.1411  1.1377\n",
            "     21        \u001b[36m0.0972\u001b[0m        0.1394  1.1443\n",
            "     22        \u001b[36m0.0949\u001b[0m        0.1468  1.1347\n",
            "     23        \u001b[36m0.0942\u001b[0m        0.1390  1.1443\n",
            "     24        \u001b[36m0.0913\u001b[0m        0.1356  1.1381\n",
            "     25        \u001b[36m0.0898\u001b[0m        \u001b[32m0.1307\u001b[0m  1.1374\n",
            "     26        \u001b[36m0.0889\u001b[0m        \u001b[32m0.1242\u001b[0m  1.1453\n",
            "     27        \u001b[36m0.0877\u001b[0m        0.1343  1.2897\n",
            "     28        \u001b[36m0.0868\u001b[0m        0.1320  1.4681\n",
            "     29        \u001b[36m0.0859\u001b[0m        0.1355  1.5330\n",
            "     30        \u001b[36m0.0848\u001b[0m        0.1292  1.1390\n",
            "     31        \u001b[36m0.0839\u001b[0m        \u001b[32m0.1207\u001b[0m  1.1523\n",
            "     32        \u001b[36m0.0830\u001b[0m        \u001b[32m0.1192\u001b[0m  1.1346\n",
            "     33        0.0831        0.1340  1.1282\n",
            "     34        \u001b[36m0.0822\u001b[0m        0.1450  1.1404\n",
            "     35        \u001b[36m0.0821\u001b[0m        0.1325  1.1310\n",
            "     36        \u001b[36m0.0816\u001b[0m        0.1285  1.1320\n",
            "     37        \u001b[36m0.0802\u001b[0m        0.1372  1.1384\n",
            "     38        \u001b[36m0.0801\u001b[0m        0.1680  1.2529\n",
            "     39        0.0807        0.1437  1.4790\n",
            "     40        \u001b[36m0.0800\u001b[0m        0.1329  1.5163\n",
            "     41        \u001b[36m0.0789\u001b[0m        0.1412  1.1525\n",
            "     42        0.0815        0.1505  1.1502\n",
            "     43        0.0802        0.1606  1.1456\n",
            "     44        0.0800        0.1442  1.1356\n",
            "     45        0.0791        0.1384  1.1379\n",
            "     46        \u001b[36m0.0786\u001b[0m        0.1492  1.1236\n",
            "     47        \u001b[36m0.0783\u001b[0m        0.1432  1.1434\n",
            "     48        \u001b[36m0.0778\u001b[0m        0.1461  1.1396\n",
            "     49        0.0790        0.1342  1.2334\n",
            "     50        0.0779        0.1724  1.4607\n",
            "     51        0.0790        0.1575  1.5198\n",
            "     52        \u001b[36m0.0771\u001b[0m        0.1371  1.1693\n",
            "     53        0.0812        0.1231  1.1401\n",
            "     54        0.0779        0.1678  1.1733\n",
            "     55        0.0781        0.1492  1.1440\n",
            "     56        \u001b[36m0.0765\u001b[0m        0.1253  1.1447\n",
            "     57        \u001b[36m0.0762\u001b[0m        0.1390  1.1421\n",
            "     58        0.0763        0.1505  1.3817\n",
            "     59        0.0765        0.1490  1.4012\n",
            "     60        0.0776        0.1318  1.8433\n",
            "     61        0.0769        0.1299  1.6928\n",
            "     62        0.0774        0.1542  1.3118\n",
            "     63        0.0789        0.1473  1.1371\n",
            "     64        \u001b[36m0.0760\u001b[0m        0.1491  1.1618\n",
            "     65        0.0762        0.1273  1.1461\n",
            "     66        \u001b[36m0.0758\u001b[0m        0.1420  1.1464\n",
            "     67        0.0766        0.1649  1.1436\n",
            "     68        0.0758        0.1612  1.1572\n",
            "     69        0.0768        0.1261  1.1427\n",
            "     70        \u001b[36m0.0757\u001b[0m        0.1328  1.1376\n",
            "     71        0.0758        0.1359  1.4524\n",
            "     72        0.0764        0.1427  1.4835\n",
            "     73        0.0760        0.1725  1.3098\n",
            "     74        0.0766        0.1211  1.1508\n",
            "     75        0.0763        0.1364  1.1332\n",
            "     76        \u001b[36m0.0750\u001b[0m        0.1627  1.1343\n",
            "     77        0.0770        0.1550  1.1288\n",
            "     78        0.0759        0.1534  1.1529\n",
            "     79        0.0754        0.1577  1.1290\n",
            "     80        0.0769        0.1521  1.1439\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.7min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1616\u001b[0m        \u001b[32m0.2056\u001b[0m  1.1786\n",
            "      2        \u001b[36m0.1551\u001b[0m        \u001b[32m0.2015\u001b[0m  1.4869\n",
            "      3        0.1554        \u001b[32m0.2008\u001b[0m  1.5027\n",
            "      4        \u001b[36m0.1461\u001b[0m        \u001b[32m0.1968\u001b[0m  1.1997\n",
            "      5        0.1498        \u001b[32m0.1941\u001b[0m  1.1392\n",
            "      6        0.1476        \u001b[32m0.1899\u001b[0m  1.1561\n",
            "      7        \u001b[36m0.1421\u001b[0m        0.1999  1.1393\n",
            "      8        0.1426        0.1904  1.1360\n",
            "      9        \u001b[36m0.1407\u001b[0m        \u001b[32m0.1836\u001b[0m  1.1336\n",
            "     10        \u001b[36m0.1396\u001b[0m        \u001b[32m0.1825\u001b[0m  1.1538\n",
            "     11        \u001b[36m0.1387\u001b[0m        \u001b[32m0.1825\u001b[0m  1.1384\n",
            "     12        \u001b[36m0.1382\u001b[0m        \u001b[32m0.1798\u001b[0m  1.2071\n",
            "     13        \u001b[36m0.1379\u001b[0m        \u001b[32m0.1773\u001b[0m  1.4648\n",
            "     14        \u001b[36m0.1375\u001b[0m        \u001b[32m0.1752\u001b[0m  1.5017\n",
            "     15        \u001b[36m0.1372\u001b[0m        \u001b[32m0.1733\u001b[0m  1.2210\n",
            "     16        \u001b[36m0.1369\u001b[0m        \u001b[32m0.1722\u001b[0m  1.1416\n",
            "     17        \u001b[36m0.1366\u001b[0m        \u001b[32m0.1718\u001b[0m  1.1395\n",
            "     18        \u001b[36m0.1364\u001b[0m        0.1721  1.1426\n",
            "     19        \u001b[36m0.1363\u001b[0m        0.1722  1.1374\n",
            "     20        \u001b[36m0.1361\u001b[0m        0.1725  1.1372\n",
            "     21        \u001b[36m0.1359\u001b[0m        0.1725  1.1355\n",
            "     22        \u001b[36m0.1357\u001b[0m        0.1724  1.1598\n",
            "     23        \u001b[36m0.1356\u001b[0m        0.1725  1.2003\n",
            "     24        \u001b[36m0.1354\u001b[0m        0.1722  1.4832\n",
            "     25        \u001b[36m0.1352\u001b[0m        0.1727  1.5118\n",
            "     26        \u001b[36m0.1351\u001b[0m        \u001b[32m0.1716\u001b[0m  1.5068\n",
            "     27        \u001b[36m0.1350\u001b[0m        0.1717  1.3792\n",
            "     28        \u001b[36m0.1348\u001b[0m        \u001b[32m0.1708\u001b[0m  1.4553\n",
            "     29        \u001b[36m0.1347\u001b[0m        \u001b[32m0.1704\u001b[0m  1.3925\n",
            "     30        \u001b[36m0.1346\u001b[0m        \u001b[32m0.1702\u001b[0m  1.1488\n",
            "     31        \u001b[36m0.1345\u001b[0m        \u001b[32m0.1693\u001b[0m  1.1395\n",
            "     32        \u001b[36m0.1343\u001b[0m        0.1703  1.1623\n",
            "     33        \u001b[36m0.1341\u001b[0m        0.1697  1.1465\n",
            "     34        0.1342        0.1701  1.4789\n",
            "     35        \u001b[36m0.1339\u001b[0m        0.1716  1.4920\n",
            "     36        0.1340        \u001b[32m0.1683\u001b[0m  1.3084\n",
            "     37        \u001b[36m0.1338\u001b[0m        0.1735  1.1353\n",
            "     38        \u001b[36m0.1336\u001b[0m        0.1687  1.1439\n",
            "     39        0.1343        0.1778  1.1449\n",
            "     40        0.1338        0.1762  1.1373\n",
            "     41        0.1341        0.1755  1.1370\n",
            "     42        0.1340        0.1913  1.1376\n",
            "     43        0.1345        \u001b[32m0.1651\u001b[0m  1.1395\n",
            "     44        0.1350        0.1847  1.1425\n",
            "     45        0.1341        \u001b[32m0.1621\u001b[0m  1.4550\n",
            "     46        \u001b[36m0.1331\u001b[0m        0.1754  1.5097\n",
            "     47        \u001b[36m0.1331\u001b[0m        0.1801  1.3355\n",
            "     48        \u001b[36m0.1325\u001b[0m        0.1755  1.1451\n",
            "     49        0.1352        0.1857  1.1394\n",
            "     50        0.1342        \u001b[32m0.1614\u001b[0m  1.1395\n",
            "     51        0.1350        0.1807  1.1367\n",
            "     52        0.1342        0.1659  1.1364\n",
            "     53        0.1334        0.1717  1.1416\n",
            "     54        0.1333        0.1787  1.1376\n",
            "     55        0.1335        \u001b[32m0.1581\u001b[0m  1.1661\n",
            "     56        0.1336        0.1810  1.4318\n",
            "     57        0.1337        0.1687  1.4844\n",
            "     58        0.1327        0.1840  1.3463\n",
            "     59        0.1332        0.1997  1.1248\n",
            "     60        0.1328        0.1812  1.1312\n",
            "     61        0.1336        0.1873  1.1337\n",
            "     62        0.1340        0.1587  1.1338\n",
            "     63        0.1347        0.1744  1.1310\n",
            "     64        0.1334        0.2232  1.1319\n",
            "     65        0.1348        0.2110  1.1344\n",
            "     66        0.1376        0.2118  1.1470\n",
            "     67        0.1365        0.1607  1.3801\n",
            "     68        0.1357        0.1841  1.4901\n",
            "     69        0.1344        0.1794  1.4059\n",
            "     70        0.1339        0.1761  1.1636\n",
            "     71        0.1356        0.1733  1.1500\n",
            "     72        0.1332        \u001b[32m0.1573\u001b[0m  1.1411\n",
            "     73        0.1325        0.1835  1.1451\n",
            "     74        0.1331        0.1891  1.1586\n",
            "     75        0.1329        0.2025  1.4224\n",
            "     76        0.1334        0.1779  1.3789\n",
            "     77        0.1333        0.1582  1.6475\n",
            "     78        0.1327        0.1664  1.6692\n",
            "     79        0.1330        0.1846  1.5281\n",
            "     80        0.1326        0.1909  1.1673\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.7min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1724\u001b[0m        \u001b[32m0.2146\u001b[0m  1.2297\n",
            "      2        \u001b[36m0.1520\u001b[0m        \u001b[32m0.2095\u001b[0m  1.2462\n",
            "      3        \u001b[36m0.1520\u001b[0m        0.2113  1.2363\n",
            "      4        \u001b[36m0.1483\u001b[0m        \u001b[32m0.2094\u001b[0m  1.2381\n",
            "      5        \u001b[36m0.1463\u001b[0m        0.2111  1.2310\n",
            "      6        \u001b[36m0.1427\u001b[0m        0.2117  1.2349\n",
            "      7        \u001b[36m0.1375\u001b[0m        0.2215  1.3294\n",
            "      8        \u001b[36m0.1359\u001b[0m        0.2289  1.6241\n",
            "      9        \u001b[36m0.1351\u001b[0m        0.2330  1.5965\n",
            "     10        \u001b[36m0.1322\u001b[0m        0.2290  1.2310\n",
            "     11        \u001b[36m0.1298\u001b[0m        0.2205  1.2293\n",
            "     12        \u001b[36m0.1262\u001b[0m        0.2115  1.2334\n",
            "     13        \u001b[36m0.1254\u001b[0m        \u001b[32m0.1918\u001b[0m  1.2477\n",
            "     14        \u001b[36m0.1232\u001b[0m        \u001b[32m0.1772\u001b[0m  1.2290\n",
            "     15        \u001b[36m0.1225\u001b[0m        \u001b[32m0.1650\u001b[0m  1.2401\n",
            "     16        \u001b[36m0.1218\u001b[0m        \u001b[32m0.1534\u001b[0m  1.2311\n",
            "     17        \u001b[36m0.1213\u001b[0m        \u001b[32m0.1388\u001b[0m  1.2731\n",
            "     18        \u001b[36m0.1205\u001b[0m        \u001b[32m0.1336\u001b[0m  1.6007\n",
            "     19        \u001b[36m0.1179\u001b[0m        0.1362  1.6439\n",
            "     20        \u001b[36m0.1145\u001b[0m        \u001b[32m0.1274\u001b[0m  1.2623\n",
            "     21        \u001b[36m0.1135\u001b[0m        0.1329  1.2233\n",
            "     22        0.1147        0.1435  1.2387\n",
            "     23        0.1150        0.1390  1.2176\n",
            "     24        0.1141        0.1311  1.2398\n",
            "     25        0.1156        0.1400  1.2144\n",
            "     26        \u001b[36m0.1100\u001b[0m        0.1300  1.2128\n",
            "     27        \u001b[36m0.1099\u001b[0m        0.1280  1.2243\n",
            "     28        \u001b[36m0.1048\u001b[0m        0.1349  1.5283\n",
            "     29        0.1075        0.1374  1.6207\n",
            "     30        0.1057        0.1399  1.3996\n",
            "     31        0.1065        0.1481  1.2188\n",
            "     32        0.1061        0.1423  1.2262\n",
            "     33        \u001b[36m0.1029\u001b[0m        0.1439  1.2288\n",
            "     34        \u001b[36m0.1005\u001b[0m        0.1473  1.2291\n",
            "     35        \u001b[36m0.0990\u001b[0m        0.1410  1.2288\n",
            "     36        0.0992        0.1485  1.2434\n",
            "     37        \u001b[36m0.0962\u001b[0m        0.1358  1.2302\n",
            "     38        0.0965        0.1506  1.4187\n",
            "     39        \u001b[36m0.0954\u001b[0m        0.1439  1.9229\n",
            "     40        \u001b[36m0.0940\u001b[0m        0.1505  1.8388\n",
            "     41        \u001b[36m0.0936\u001b[0m        0.1590  1.5734\n",
            "     42        0.0938        0.1615  1.3602\n",
            "     43        0.0937        0.1626  1.2133\n",
            "     44        0.0944        0.1785  1.2248\n",
            "     45        \u001b[36m0.0922\u001b[0m        0.1907  1.2448\n",
            "     46        \u001b[36m0.0920\u001b[0m        0.1985  1.2382\n",
            "     47        0.0942        0.1716  1.2325\n",
            "     48        \u001b[36m0.0906\u001b[0m        0.1879  1.6001\n",
            "     49        \u001b[36m0.0893\u001b[0m        0.1984  1.6351\n",
            "     50        \u001b[36m0.0880\u001b[0m        0.2135  1.3111\n",
            "     51        0.0883        0.2097  1.2164\n",
            "     52        \u001b[36m0.0878\u001b[0m        0.2153  1.2046\n",
            "     53        0.0902        0.2011  1.2313\n",
            "     54        0.0959        0.1576  1.2206\n",
            "     55        0.0933        0.1622  1.2185\n",
            "     56        0.0901        0.1838  1.2074\n",
            "     57        0.0889        0.1824  1.2185\n",
            "     58        \u001b[36m0.0876\u001b[0m        0.2018  1.4753\n",
            "     59        0.0885        0.1848  1.6094\n",
            "     60        \u001b[36m0.0871\u001b[0m        0.1786  1.5007\n",
            "     61        0.0872        0.1816  1.2295\n",
            "     62        \u001b[36m0.0871\u001b[0m        0.1724  1.2286\n",
            "     63        \u001b[36m0.0859\u001b[0m        0.1738  1.2151\n",
            "     64        \u001b[36m0.0858\u001b[0m        0.1836  1.2348\n",
            "     65        \u001b[36m0.0856\u001b[0m        0.1819  1.2318\n",
            "     66        \u001b[36m0.0834\u001b[0m        0.1717  1.2237\n",
            "     67        0.0845        0.1953  1.2288\n",
            "     68        0.0834        0.1798  1.3663\n",
            "     69        0.0838        0.1812  1.5956\n",
            "     70        \u001b[36m0.0834\u001b[0m        0.1828  1.6042\n",
            "     71        \u001b[36m0.0832\u001b[0m        0.2027  1.2225\n",
            "     72        0.0837        0.2187  1.2213\n",
            "     73        \u001b[36m0.0823\u001b[0m        0.1963  1.2223\n",
            "     74        \u001b[36m0.0818\u001b[0m        0.1885  1.2172\n",
            "     75        0.0828        0.1931  1.2130\n",
            "     76        0.0822        0.2071  1.2168\n",
            "     77        0.0819        0.1903  1.2263\n",
            "     78        \u001b[36m0.0813\u001b[0m        0.1917  1.2223\n",
            "     79        \u001b[36m0.0797\u001b[0m        0.1940  1.6182\n",
            "     80        0.0821        0.1689  1.6377\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1689\u001b[0m        \u001b[32m0.2149\u001b[0m  1.2394\n",
            "      2        \u001b[36m0.1563\u001b[0m        \u001b[32m0.2108\u001b[0m  1.2268\n",
            "      3        0.1590        0.2114  1.2365\n",
            "      4        0.1605        0.2142  1.4666\n",
            "      5        0.1570        0.2132  1.4994\n",
            "      6        0.1638        \u001b[32m0.2095\u001b[0m  1.5929\n",
            "      7        0.1652        0.2102  1.4362\n",
            "      8        0.1580        0.2219  1.6010\n",
            "      9        0.1586        0.2215  1.6397\n",
            "     10        0.1569        0.2266  1.3155\n",
            "     11        \u001b[36m0.1530\u001b[0m        0.2294  1.2353\n",
            "     12        \u001b[36m0.1518\u001b[0m        0.2283  1.2199\n",
            "     13        0.1532        0.2283  1.2231\n",
            "     14        \u001b[36m0.1517\u001b[0m        0.2320  1.2197\n",
            "     15        0.1534        0.2321  1.2250\n",
            "     16        0.1522        0.2372  1.2282\n",
            "     17        0.1529        0.2375  1.2186\n",
            "     18        \u001b[36m0.1507\u001b[0m        0.2382  1.4753\n",
            "     19        \u001b[36m0.1495\u001b[0m        0.2390  1.6305\n",
            "     20        0.1497        0.2474  1.4044\n",
            "     21        \u001b[36m0.1474\u001b[0m        0.2465  1.2213\n",
            "     22        0.1486        0.2403  1.2158\n",
            "     23        \u001b[36m0.1469\u001b[0m        0.2444  1.2335\n",
            "     24        \u001b[36m0.1455\u001b[0m        0.2613  1.2174\n",
            "     25        0.1465        0.2688  1.2191\n",
            "     26        0.1456        0.2335  1.2135\n",
            "     27        \u001b[36m0.1450\u001b[0m        0.2601  1.2239\n",
            "     28        \u001b[36m0.1444\u001b[0m        0.2659  1.4317\n",
            "     29        \u001b[36m0.1433\u001b[0m        0.2612  1.5888\n",
            "     30        0.1455        0.2633  1.5326\n",
            "     31        0.1446        0.2764  1.2181\n",
            "     32        0.1454        0.2620  1.2199\n",
            "     33        \u001b[36m0.1432\u001b[0m        0.2444  1.2503\n",
            "     34        0.1432        0.2345  1.2253\n",
            "     35        \u001b[36m0.1427\u001b[0m        0.2698  1.2198\n",
            "     36        0.1434        0.2901  1.2202\n",
            "     37        0.1428        0.2273  1.2179\n",
            "     38        \u001b[36m0.1427\u001b[0m        0.2537  1.3141\n",
            "     39        0.1430        0.2811  1.5812\n",
            "     40        0.1431        0.2516  1.6518\n",
            "     41        0.1447        0.2567  1.2222\n",
            "     42        \u001b[36m0.1409\u001b[0m        0.2100  1.2178\n",
            "     43        0.1412        0.2557  1.2193\n",
            "     44        \u001b[36m0.1408\u001b[0m        0.2366  1.2337\n",
            "     45        0.1411        0.2529  1.2164\n",
            "     46        0.1421        0.2614  1.2510\n",
            "     47        0.1418        0.2356  1.2377\n",
            "     48        \u001b[36m0.1408\u001b[0m        0.2601  1.2789\n",
            "     49        0.1428        0.2404  2.0915\n",
            "     50        \u001b[36m0.1401\u001b[0m        0.2488  2.1996\n",
            "     51        \u001b[36m0.1395\u001b[0m        0.2252  1.3821\n",
            "     52        0.1403        0.2546  1.2305\n",
            "     53        0.1398        0.2512  1.2227\n",
            "     54        0.1403        0.2620  1.2342\n",
            "     55        0.1409        0.2806  1.2555\n",
            "     56        0.1414        0.2627  1.2361\n",
            "     57        0.1414        0.2206  1.2184\n",
            "     58        0.1401        \u001b[32m0.2007\u001b[0m  1.2900\n",
            "     59        0.1413        0.2305  1.6164\n",
            "     60        0.1409        0.2358  1.6383\n",
            "     61        0.1401        0.2244  1.2148\n",
            "     62        \u001b[36m0.1393\u001b[0m        0.2272  1.2282\n",
            "     63        \u001b[36m0.1388\u001b[0m        0.2370  1.2321\n",
            "     64        0.1398        0.2689  1.2191\n",
            "     65        0.1398        0.2669  1.2195\n",
            "     66        0.1426        0.2498  1.2354\n",
            "     67        0.1403        0.2151  1.2256\n",
            "     68        0.1399        0.2237  1.2444\n",
            "     69        0.1393        0.2206  1.6094\n",
            "     70        \u001b[36m0.1376\u001b[0m        0.2068  1.6153\n",
            "     71        0.1401        0.2173  1.3309\n",
            "     72        \u001b[36m0.1371\u001b[0m        0.2184  1.2364\n",
            "     73        0.1403        0.2091  1.2225\n",
            "     74        0.1383        0.2221  1.2256\n",
            "     75        0.1397        0.2725  1.2238\n",
            "     76        0.1404        0.2153  1.2220\n",
            "     77        0.1418        0.2244  1.2289\n",
            "     78        0.1405        0.2228  1.2237\n",
            "     79        0.1407        \u001b[32m0.2006\u001b[0m  1.4829\n",
            "     80        0.1396        0.2080  1.6364\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1767\u001b[0m        \u001b[32m0.1935\u001b[0m  1.2455\n",
            "      2        \u001b[36m0.1440\u001b[0m        0.2002  1.1372\n",
            "      3        \u001b[36m0.1402\u001b[0m        0.1955  1.1307\n",
            "      4        \u001b[36m0.1331\u001b[0m        \u001b[32m0.1915\u001b[0m  1.1335\n",
            "      5        \u001b[36m0.1281\u001b[0m        \u001b[32m0.1882\u001b[0m  1.1323\n",
            "      6        \u001b[36m0.1247\u001b[0m        \u001b[32m0.1855\u001b[0m  1.1280\n",
            "      7        \u001b[36m0.1224\u001b[0m        \u001b[32m0.1822\u001b[0m  1.1394\n",
            "      8        \u001b[36m0.1202\u001b[0m        0.1857  1.1287\n",
            "      9        \u001b[36m0.1197\u001b[0m        0.1823  1.1625\n",
            "     10        \u001b[36m0.1186\u001b[0m        \u001b[32m0.1770\u001b[0m  1.4888\n",
            "     11        \u001b[36m0.1178\u001b[0m        \u001b[32m0.1634\u001b[0m  1.5143\n",
            "     12        \u001b[36m0.1164\u001b[0m        0.1673  1.2225\n",
            "     13        \u001b[36m0.1154\u001b[0m        0.1670  1.1523\n",
            "     14        \u001b[36m0.1150\u001b[0m        0.1653  1.3766\n",
            "     15        \u001b[36m0.1138\u001b[0m        \u001b[32m0.1361\u001b[0m  1.3799\n",
            "     16        \u001b[36m0.1130\u001b[0m        0.1995  1.4815\n",
            "     17        0.1155        0.1520  1.4007\n",
            "     18        0.1148        0.1653  1.1604\n",
            "     19        \u001b[36m0.1126\u001b[0m        0.1471  1.2039\n",
            "     20        0.1189        0.2331  1.4823\n",
            "     21        0.1167        0.2324  1.4972\n",
            "     22        0.1147        0.1889  1.2444\n",
            "     23        0.1130        0.1629  1.1566\n",
            "     24        0.1185        0.2398  1.1439\n",
            "     25        0.1150        0.1445  1.1415\n",
            "     26        \u001b[36m0.1106\u001b[0m        \u001b[32m0.1306\u001b[0m  1.1460\n",
            "     27        \u001b[36m0.1060\u001b[0m        0.1403  1.1430\n",
            "     28        \u001b[36m0.1051\u001b[0m        0.1378  1.1384\n",
            "     29        \u001b[36m0.1013\u001b[0m        \u001b[32m0.1300\u001b[0m  1.1383\n",
            "     30        \u001b[36m0.0998\u001b[0m        \u001b[32m0.1209\u001b[0m  1.1927\n",
            "     31        \u001b[36m0.0979\u001b[0m        \u001b[32m0.1192\u001b[0m  1.4660\n",
            "     32        \u001b[36m0.0959\u001b[0m        0.1324  1.4893\n",
            "     33        \u001b[36m0.0952\u001b[0m        \u001b[32m0.1090\u001b[0m  1.2460\n",
            "     34        \u001b[36m0.0919\u001b[0m        0.1186  1.1676\n",
            "     35        \u001b[36m0.0916\u001b[0m        0.1184  1.1488\n",
            "     36        0.0916        0.1133  1.1458\n",
            "     37        \u001b[36m0.0912\u001b[0m        0.1103  1.1321\n",
            "     38        \u001b[36m0.0911\u001b[0m        \u001b[32m0.1031\u001b[0m  1.1377\n",
            "     39        \u001b[36m0.0881\u001b[0m        0.1068  1.1371\n",
            "     40        \u001b[36m0.0873\u001b[0m        \u001b[32m0.1030\u001b[0m  1.1375\n",
            "     41        \u001b[36m0.0860\u001b[0m        0.1055  1.1525\n",
            "     42        \u001b[36m0.0855\u001b[0m        0.1047  1.4857\n",
            "     43        \u001b[36m0.0850\u001b[0m        0.1104  1.4817\n",
            "     44        0.0857        0.1071  1.2862\n",
            "     45        \u001b[36m0.0842\u001b[0m        0.1076  1.1356\n",
            "     46        \u001b[36m0.0832\u001b[0m        0.1141  1.1431\n",
            "     47        \u001b[36m0.0831\u001b[0m        0.1171  1.1480\n",
            "     48        0.0835        0.1097  1.1382\n",
            "     49        0.0833        0.1142  1.1233\n",
            "     50        \u001b[36m0.0817\u001b[0m        0.1160  1.1405\n",
            "     51        \u001b[36m0.0811\u001b[0m        0.1138  1.1373\n",
            "     52        \u001b[36m0.0805\u001b[0m        0.1169  1.1406\n",
            "     53        \u001b[36m0.0799\u001b[0m        0.1173  1.4714\n",
            "     54        \u001b[36m0.0796\u001b[0m        0.1185  1.4785\n",
            "     55        \u001b[36m0.0791\u001b[0m        0.1120  1.3145\n",
            "     56        \u001b[36m0.0790\u001b[0m        0.1208  1.1390\n",
            "     57        \u001b[36m0.0788\u001b[0m        0.1246  1.1645\n",
            "     58        0.0815        0.1073  1.1432\n",
            "     59        0.0790        0.1126  1.1743\n",
            "     60        0.0789        0.1199  1.1474\n",
            "     61        0.0790        0.1141  1.1462\n",
            "     62        \u001b[36m0.0785\u001b[0m        0.1092  1.4042\n",
            "     63        \u001b[36m0.0785\u001b[0m        0.1095  1.5703\n",
            "     64        \u001b[36m0.0782\u001b[0m        0.1169  1.9717\n",
            "     65        0.0789        0.1137  1.5546\n",
            "     66        0.0783        0.1137  1.1402\n",
            "     67        0.0790        0.1080  1.1323\n",
            "     68        0.0783        0.1256  1.1603\n",
            "     69        0.0784        0.1160  1.1414\n",
            "     70        0.0784        0.1122  1.1392\n",
            "     71        0.0785        0.1165  1.1526\n",
            "     72        0.0786        0.1230  1.1331\n",
            "     73        \u001b[36m0.0782\u001b[0m        0.1160  1.1751\n",
            "     74        \u001b[36m0.0778\u001b[0m        0.1152  1.3500\n",
            "     75        \u001b[36m0.0775\u001b[0m        0.1070  1.4673\n",
            "     76        \u001b[36m0.0775\u001b[0m        0.1243  1.4459\n",
            "     77        0.0781        0.1185  1.1542\n",
            "     78        \u001b[36m0.0773\u001b[0m        0.1198  1.1586\n",
            "     79        0.0774        0.1227  1.1582\n",
            "     80        0.0780        0.1186  1.1524\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.7min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1696\u001b[0m        \u001b[32m0.2166\u001b[0m  1.1556\n",
            "      2        \u001b[36m0.1483\u001b[0m        \u001b[32m0.2112\u001b[0m  1.1522\n",
            "      3        \u001b[36m0.1479\u001b[0m        \u001b[32m0.2067\u001b[0m  1.1779\n",
            "      4        \u001b[36m0.1460\u001b[0m        \u001b[32m0.2032\u001b[0m  1.1615\n",
            "      5        \u001b[36m0.1431\u001b[0m        \u001b[32m0.2001\u001b[0m  1.4980\n",
            "      6        \u001b[36m0.1411\u001b[0m        \u001b[32m0.1995\u001b[0m  1.4921\n",
            "      7        \u001b[36m0.1409\u001b[0m        \u001b[32m0.1980\u001b[0m  1.3084\n",
            "      8        \u001b[36m0.1393\u001b[0m        0.1997  1.1612\n",
            "      9        \u001b[36m0.1385\u001b[0m        \u001b[32m0.1947\u001b[0m  1.1499\n",
            "     10        \u001b[36m0.1377\u001b[0m        0.1998  1.1521\n",
            "     11        0.1401        \u001b[32m0.1910\u001b[0m  1.1482\n",
            "     12        \u001b[36m0.1374\u001b[0m        0.1958  1.1358\n",
            "     13        \u001b[36m0.1367\u001b[0m        \u001b[32m0.1897\u001b[0m  1.1381\n",
            "     14        \u001b[36m0.1362\u001b[0m        \u001b[32m0.1866\u001b[0m  1.1466\n",
            "     15        \u001b[36m0.1307\u001b[0m        0.1939  1.1386\n",
            "     16        0.1351        \u001b[32m0.1816\u001b[0m  1.4814\n",
            "     17        0.1340        \u001b[32m0.1748\u001b[0m  1.4752\n",
            "     18        \u001b[36m0.1269\u001b[0m        0.1777  1.3279\n",
            "     19        0.1333        0.1749  1.1437\n",
            "     20        0.1336        \u001b[32m0.1730\u001b[0m  1.1428\n",
            "     21        0.1341        \u001b[32m0.1654\u001b[0m  1.1334\n",
            "     22        0.1336        0.1798  1.1408\n",
            "     23        0.1352        0.1784  1.1372\n",
            "     24        0.1358        0.1733  1.1399\n",
            "     25        0.1345        \u001b[32m0.1626\u001b[0m  1.1427\n",
            "     26        0.1315        \u001b[32m0.1576\u001b[0m  1.1393\n",
            "     27        0.1316        0.1746  1.4204\n",
            "     28        0.1339        0.1770  1.5157\n",
            "     29        0.1339        0.1691  1.4460\n",
            "     30        0.1333        0.1697  1.4321\n",
            "     31        0.1291        0.1676  1.3812\n",
            "     32        0.1324        0.1613  1.4587\n",
            "     33        0.1303        0.1772  1.2938\n",
            "     34        0.1325        0.1695  1.1442\n",
            "     35        0.1312        0.1620  1.1387\n",
            "     36        0.1296        0.1662  1.1581\n",
            "     37        0.1285        0.1633  1.3476\n",
            "     38        0.1275        0.1621  1.4689\n",
            "     39        0.1275        \u001b[32m0.1544\u001b[0m  1.4413\n",
            "     40        \u001b[36m0.1233\u001b[0m        0.1554  1.1418\n",
            "     41        \u001b[36m0.1226\u001b[0m        \u001b[32m0.1473\u001b[0m  1.1417\n",
            "     42        \u001b[36m0.1215\u001b[0m        0.1495  1.1476\n",
            "     43        0.1271        0.1659  1.1356\n",
            "     44        0.1304        0.1698  1.1373\n",
            "     45        0.1297        0.1654  1.1355\n",
            "     46        0.1297        0.1674  1.1493\n",
            "     47        0.1298        0.1613  1.1430\n",
            "     48        0.1308        0.1623  1.3213\n",
            "     49        0.1307        0.1561  1.4701\n",
            "     50        0.1316        0.1638  1.4661\n",
            "     51        0.1314        0.1571  1.1608\n",
            "     52        0.1309        0.1777  1.1467\n",
            "     53        0.1307        0.1607  1.1444\n",
            "     54        0.1322        0.1792  1.1383\n",
            "     55        0.1319        0.1605  1.1335\n",
            "     56        0.1306        0.1635  1.1318\n",
            "     57        0.1308        0.1547  1.1376\n",
            "     58        0.1305        0.1639  1.1563\n",
            "     59        0.1295        0.1575  1.3076\n",
            "     60        0.1296        0.1671  1.4716\n",
            "     61        0.1296        0.1603  1.5085\n",
            "     62        0.1308        0.1790  1.1452\n",
            "     63        0.1307        0.1597  1.1478\n",
            "     64        0.1311        0.1755  1.1425\n",
            "     65        0.1314        0.1501  1.1604\n",
            "     66        0.1310        0.1637  1.1403\n",
            "     67        0.1306        0.1505  1.1374\n",
            "     68        0.1307        0.1678  1.1363\n",
            "     69        0.1306        0.1546  1.1395\n",
            "     70        0.1307        0.1773  1.2985\n",
            "     71        0.1308        0.1562  1.4475\n",
            "     72        0.1308        0.1767  1.4822\n",
            "     73        0.1310        0.1512  1.1423\n",
            "     74        0.1308        0.1660  1.1428\n",
            "     75        0.1308        0.1486  1.1313\n",
            "     76        0.1306        0.1651  1.1723\n",
            "     77        0.1303        0.1552  1.1462\n",
            "     78        0.1305        0.1753  1.2544\n",
            "     79        0.1308        0.1561  1.4336\n",
            "     80        0.1317        0.1746  1.4107\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.7min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1598\u001b[0m        \u001b[32m0.2196\u001b[0m  1.9457\n",
            "      2        \u001b[36m0.1510\u001b[0m        \u001b[32m0.2132\u001b[0m  1.6091\n",
            "      3        0.1531        \u001b[32m0.2125\u001b[0m  1.2263\n",
            "      4        0.1553        \u001b[32m0.2077\u001b[0m  1.2315\n",
            "      5        0.1535        \u001b[32m0.2061\u001b[0m  1.2366\n",
            "      6        \u001b[36m0.1509\u001b[0m        \u001b[32m0.2060\u001b[0m  1.2240\n",
            "      7        0.1510        \u001b[32m0.2058\u001b[0m  1.2253\n",
            "      8        \u001b[36m0.1463\u001b[0m        0.2073  1.2390\n",
            "      9        \u001b[36m0.1453\u001b[0m        0.2059  1.2204\n",
            "     10        \u001b[36m0.1427\u001b[0m        \u001b[32m0.2038\u001b[0m  1.2425\n",
            "     11        \u001b[36m0.1406\u001b[0m        0.2048  1.5969\n",
            "     12        \u001b[36m0.1371\u001b[0m        0.2051  1.6357\n",
            "     13        \u001b[36m0.1366\u001b[0m        \u001b[32m0.1991\u001b[0m  1.3028\n",
            "     14        0.1376        \u001b[32m0.1922\u001b[0m  1.2192\n",
            "     15        0.1395        \u001b[32m0.1885\u001b[0m  1.2232\n",
            "     16        0.1388        \u001b[32m0.1829\u001b[0m  1.2095\n",
            "     17        0.1380        \u001b[32m0.1708\u001b[0m  1.2262\n",
            "     18        \u001b[36m0.1362\u001b[0m        \u001b[32m0.1578\u001b[0m  1.2592\n",
            "     19        \u001b[36m0.1353\u001b[0m        0.1640  1.2295\n",
            "     20        \u001b[36m0.1333\u001b[0m        \u001b[32m0.1521\u001b[0m  1.2209\n",
            "     21        0.1348        0.1578  1.5108\n",
            "     22        \u001b[36m0.1329\u001b[0m        \u001b[32m0.1443\u001b[0m  1.6206\n",
            "     23        \u001b[36m0.1297\u001b[0m        0.1455  1.3897\n",
            "     24        \u001b[36m0.1286\u001b[0m        \u001b[32m0.1326\u001b[0m  1.2078\n",
            "     25        \u001b[36m0.1248\u001b[0m        0.1329  1.2236\n",
            "     26        \u001b[36m0.1237\u001b[0m        0.1352  1.2238\n",
            "     27        \u001b[36m0.1199\u001b[0m        \u001b[32m0.1274\u001b[0m  1.2280\n",
            "     28        \u001b[36m0.1132\u001b[0m        0.1391  1.2293\n",
            "     29        \u001b[36m0.1124\u001b[0m        0.1409  1.2311\n",
            "     30        \u001b[36m0.1081\u001b[0m        0.1512  1.2202\n",
            "     31        \u001b[36m0.1074\u001b[0m        0.1456  1.4285\n",
            "     32        \u001b[36m0.1054\u001b[0m        0.1633  1.5967\n",
            "     33        \u001b[36m0.1047\u001b[0m        0.1582  1.5366\n",
            "     34        \u001b[36m0.1028\u001b[0m        0.1630  1.2129\n",
            "     35        \u001b[36m0.1024\u001b[0m        0.1723  1.2253\n",
            "     36        \u001b[36m0.1016\u001b[0m        0.1593  1.2276\n",
            "     37        \u001b[36m0.1011\u001b[0m        0.1789  1.2121\n",
            "     38        \u001b[36m0.1005\u001b[0m        0.1860  1.2162\n",
            "     39        0.1032        0.2210  1.2178\n",
            "     40        0.1070        0.1504  1.2358\n",
            "     41        \u001b[36m0.0999\u001b[0m        0.1579  1.3455\n",
            "     42        \u001b[36m0.0983\u001b[0m        0.1793  1.7125\n",
            "     43        0.0983        0.1771  1.9922\n",
            "     44        \u001b[36m0.0973\u001b[0m        0.1679  1.5829\n",
            "     45        \u001b[36m0.0959\u001b[0m        0.1614  1.5264\n",
            "     46        \u001b[36m0.0949\u001b[0m        0.1649  1.2360\n",
            "     47        0.0952        0.1903  1.2088\n",
            "     48        \u001b[36m0.0946\u001b[0m        0.1663  1.2266\n",
            "     49        \u001b[36m0.0933\u001b[0m        0.1962  1.2428\n",
            "     50        \u001b[36m0.0931\u001b[0m        0.1936  1.2355\n",
            "     51        \u001b[36m0.0921\u001b[0m        0.1675  1.4993\n",
            "     52        0.0922        0.1733  1.6227\n",
            "     53        0.0923        0.1684  1.4101\n",
            "     54        0.0925        0.2052  1.2248\n",
            "     55        0.0927        0.1723  1.2274\n",
            "     56        \u001b[36m0.0908\u001b[0m        0.1905  1.2325\n",
            "     57        0.0910        0.1647  1.2308\n",
            "     58        0.0915        0.2158  1.2153\n",
            "     59        0.0959        0.1394  1.2172\n",
            "     60        0.0911        0.1928  1.2234\n",
            "     61        0.0920        0.1693  1.4128\n",
            "     62        \u001b[36m0.0908\u001b[0m        0.1748  1.5947\n",
            "     63        \u001b[36m0.0905\u001b[0m        0.1686  1.5958\n",
            "     64        \u001b[36m0.0893\u001b[0m        0.1779  1.2276\n",
            "     65        0.0898        0.1816  1.2348\n",
            "     66        \u001b[36m0.0881\u001b[0m        0.1731  1.2236\n",
            "     67        0.0884        0.1663  1.2224\n",
            "     68        0.0894        0.1677  1.2287\n",
            "     69        0.0892        0.1856  1.2207\n",
            "     70        \u001b[36m0.0880\u001b[0m        0.1812  1.2262\n",
            "     71        0.0885        0.1735  1.2929\n",
            "     72        \u001b[36m0.0877\u001b[0m        0.2062  1.6070\n",
            "     73        0.0881        0.1789  1.6589\n",
            "     74        \u001b[36m0.0877\u001b[0m        0.2035  1.2397\n",
            "     75        0.0882        0.1687  1.2328\n",
            "     76        \u001b[36m0.0869\u001b[0m        0.1825  1.2218\n",
            "     77        \u001b[36m0.0864\u001b[0m        0.1860  1.2212\n",
            "     78        0.0873        0.1716  1.2028\n",
            "     79        \u001b[36m0.0863\u001b[0m        0.1886  1.2467\n",
            "     80        \u001b[36m0.0857\u001b[0m        0.1931  1.2213\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1603\u001b[0m        \u001b[32m0.2151\u001b[0m  1.2555\n",
            "      2        \u001b[36m0.1514\u001b[0m        \u001b[32m0.2103\u001b[0m  1.6038\n",
            "      3        0.1534        0.2108  1.6530\n",
            "      4        0.1564        0.2103  1.2701\n",
            "      5        0.1551        0.2112  1.2549\n",
            "      6        0.1558        0.2127  1.2298\n",
            "      7        0.1558        0.2145  1.3567\n",
            "      8        0.1550        0.2153  1.5446\n",
            "      9        0.1516        0.2172  1.5808\n",
            "     10        0.1521        0.2205  1.5459\n",
            "     11        0.1526        0.2259  1.5210\n",
            "     12        \u001b[36m0.1500\u001b[0m        0.2283  1.6196\n",
            "     13        0.1506        0.2347  1.4012\n",
            "     14        0.1501        0.2418  1.2273\n",
            "     15        \u001b[36m0.1476\u001b[0m        0.2493  1.2117\n",
            "     16        0.1485        0.2530  1.2253\n",
            "     17        \u001b[36m0.1471\u001b[0m        0.2507  1.2173\n",
            "     18        0.1486        0.2577  1.2124\n",
            "     19        0.1488        0.2669  1.2346\n",
            "     20        0.1481        0.2718  1.2200\n",
            "     21        0.1481        0.2720  1.4415\n",
            "     22        0.1482        0.2650  1.6130\n",
            "     23        0.1484        0.2579  1.4596\n",
            "     24        0.1491        0.2775  1.2254\n",
            "     25        0.1480        0.2676  1.2172\n",
            "     26        0.1488        0.2671  1.2385\n",
            "     27        0.1474        0.2588  1.2478\n",
            "     28        0.1473        0.2587  1.2215\n",
            "     29        \u001b[36m0.1460\u001b[0m        0.2573  1.2235\n",
            "     30        \u001b[36m0.1456\u001b[0m        0.2647  1.2315\n",
            "     31        \u001b[36m0.1447\u001b[0m        0.2460  1.3879\n",
            "     32        \u001b[36m0.1435\u001b[0m        0.2541  1.6035\n",
            "     33        0.1444        0.2446  1.5505\n",
            "     34        0.1439        0.2431  1.2217\n",
            "     35        \u001b[36m0.1433\u001b[0m        0.2382  1.2221\n",
            "     36        \u001b[36m0.1426\u001b[0m        0.2661  1.2268\n",
            "     37        \u001b[36m0.1421\u001b[0m        0.2632  1.2339\n",
            "     38        0.1422        0.2636  1.2370\n",
            "     39        \u001b[36m0.1417\u001b[0m        0.2644  1.2228\n",
            "     40        0.1426        0.2524  1.2262\n",
            "     41        0.1418        0.2415  1.2987\n",
            "     42        0.1423        0.2270  1.5837\n",
            "     43        0.1424        0.2252  1.6484\n",
            "     44        0.1426        0.2209  1.2340\n",
            "     45        0.1426        0.2166  1.2128\n",
            "     46        \u001b[36m0.1415\u001b[0m        0.2908  1.2074\n",
            "     47        0.1424        0.2290  1.2164\n",
            "     48        \u001b[36m0.1400\u001b[0m        0.2908  1.2293\n",
            "     49        0.1414        0.2824  1.2312\n",
            "     50        \u001b[36m0.1379\u001b[0m        0.2359  1.2527\n",
            "     51        0.1393        0.2519  1.2269\n",
            "     52        \u001b[36m0.1343\u001b[0m        0.2333  1.9766\n",
            "     53        \u001b[36m0.1338\u001b[0m        0.2315  2.1514\n",
            "     54        \u001b[36m0.1280\u001b[0m        0.2612  1.6211\n",
            "     55        0.1294        0.2504  1.2280\n",
            "     56        \u001b[36m0.1240\u001b[0m        0.2443  1.2186\n",
            "     57        \u001b[36m0.1224\u001b[0m        0.2121  1.2269\n",
            "     58        \u001b[36m0.1205\u001b[0m        0.2221  1.2400\n",
            "     59        0.1222        0.2108  1.2312\n",
            "     60        0.1233        0.2333  1.2278\n",
            "     61        \u001b[36m0.1190\u001b[0m        \u001b[32m0.2095\u001b[0m  1.2557\n",
            "     62        0.1231        \u001b[32m0.2033\u001b[0m  1.6099\n",
            "     63        \u001b[36m0.1153\u001b[0m        \u001b[32m0.1799\u001b[0m  1.6472\n",
            "     64        \u001b[36m0.1150\u001b[0m        \u001b[32m0.1766\u001b[0m  1.2289\n",
            "     65        \u001b[36m0.1132\u001b[0m        \u001b[32m0.1724\u001b[0m  1.2335\n",
            "     66        \u001b[36m0.1121\u001b[0m        0.1854  1.2382\n",
            "     67        \u001b[36m0.1082\u001b[0m        \u001b[32m0.1558\u001b[0m  1.2275\n",
            "     68        \u001b[36m0.1066\u001b[0m        \u001b[32m0.1519\u001b[0m  1.2243\n",
            "     69        \u001b[36m0.1049\u001b[0m        0.1595  1.2230\n",
            "     70        \u001b[36m0.1028\u001b[0m        0.1570  1.2249\n",
            "     71        \u001b[36m0.1022\u001b[0m        0.1614  1.2332\n",
            "     72        \u001b[36m0.1015\u001b[0m        \u001b[32m0.1511\u001b[0m  1.6001\n",
            "     73        0.1022        0.1620  1.6330\n",
            "     74        \u001b[36m0.1010\u001b[0m        0.1595  1.3635\n",
            "     75        \u001b[36m0.1006\u001b[0m        0.1628  1.2579\n",
            "     76        \u001b[36m0.0997\u001b[0m        0.1538  1.2451\n",
            "     77        0.1004        0.1614  1.2476\n",
            "     78        \u001b[36m0.0989\u001b[0m        \u001b[32m0.1476\u001b[0m  1.2403\n",
            "     79        0.0990        0.1607  1.2595\n",
            "     80        \u001b[36m0.0984\u001b[0m        \u001b[32m0.1458\u001b[0m  1.2473\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1612\u001b[0m        \u001b[32m0.2066\u001b[0m  1.1736\n",
            "      2        \u001b[36m0.1509\u001b[0m        0.2087  1.4572\n",
            "      3        0.1524        \u001b[32m0.2044\u001b[0m  1.4761\n",
            "      4        0.1531        \u001b[32m0.1969\u001b[0m  1.2968\n",
            "      5        \u001b[36m0.1468\u001b[0m        \u001b[32m0.1944\u001b[0m  1.1364\n",
            "      6        \u001b[36m0.1451\u001b[0m        \u001b[32m0.1891\u001b[0m  1.1550\n",
            "      7        \u001b[36m0.1411\u001b[0m        \u001b[32m0.1873\u001b[0m  1.1412\n",
            "      8        \u001b[36m0.1394\u001b[0m        \u001b[32m0.1841\u001b[0m  1.1408\n",
            "      9        \u001b[36m0.1382\u001b[0m        \u001b[32m0.1800\u001b[0m  1.1402\n",
            "     10        \u001b[36m0.1362\u001b[0m        \u001b[32m0.1725\u001b[0m  1.1562\n",
            "     11        \u001b[36m0.1344\u001b[0m        \u001b[32m0.1702\u001b[0m  1.1394\n",
            "     12        \u001b[36m0.1335\u001b[0m        \u001b[32m0.1661\u001b[0m  1.1363\n",
            "     13        \u001b[36m0.1328\u001b[0m        \u001b[32m0.1623\u001b[0m  1.4747\n",
            "     14        \u001b[36m0.1322\u001b[0m        \u001b[32m0.1590\u001b[0m  1.4772\n",
            "     15        \u001b[36m0.1317\u001b[0m        \u001b[32m0.1574\u001b[0m  1.2627\n",
            "     16        \u001b[36m0.1313\u001b[0m        \u001b[32m0.1569\u001b[0m  1.1712\n",
            "     17        \u001b[36m0.1309\u001b[0m        \u001b[32m0.1543\u001b[0m  1.2562\n",
            "     18        \u001b[36m0.1306\u001b[0m        0.1558  1.4240\n",
            "     19        \u001b[36m0.1304\u001b[0m        0.1564  1.3957\n",
            "     20        \u001b[36m0.1303\u001b[0m        \u001b[32m0.1508\u001b[0m  1.4771\n",
            "     21        \u001b[36m0.1300\u001b[0m        0.1664  1.2504\n",
            "     22        0.1312        0.1611  1.1374\n",
            "     23        0.1304        0.1614  1.4853\n",
            "     24        0.1308        0.1599  1.4989\n",
            "     25        \u001b[36m0.1299\u001b[0m        0.1560  1.2926\n",
            "     26        \u001b[36m0.1286\u001b[0m        0.1572  1.1390\n",
            "     27        \u001b[36m0.1284\u001b[0m        0.1517  1.1446\n",
            "     28        \u001b[36m0.1272\u001b[0m        0.1690  1.1458\n",
            "     29        0.1274        0.1548  1.1361\n",
            "     30        0.1278        0.1558  1.1493\n",
            "     31        \u001b[36m0.1271\u001b[0m        0.1609  1.1441\n",
            "     32        0.1273        0.1541  1.1429\n",
            "     33        \u001b[36m0.1268\u001b[0m        0.1651  1.1442\n",
            "     34        \u001b[36m0.1253\u001b[0m        0.1698  1.4770\n",
            "     35        0.1273        0.1725  1.4760\n",
            "     36        0.1264        0.1658  1.2893\n",
            "     37        \u001b[36m0.1241\u001b[0m        0.1602  1.1346\n",
            "     38        \u001b[36m0.1203\u001b[0m        \u001b[32m0.1361\u001b[0m  1.1306\n",
            "     39        \u001b[36m0.1168\u001b[0m        0.1648  1.1510\n",
            "     40        \u001b[36m0.1150\u001b[0m        0.1662  1.1645\n",
            "     41        0.1169        0.1594  1.1549\n",
            "     42        \u001b[36m0.1113\u001b[0m        \u001b[32m0.1353\u001b[0m  1.1431\n",
            "     43        \u001b[36m0.1069\u001b[0m        \u001b[32m0.1288\u001b[0m  1.1350\n",
            "     44        \u001b[36m0.1015\u001b[0m        0.1530  1.1406\n",
            "     45        0.1087        0.1536  1.4689\n",
            "     46        0.1019        0.1559  1.4861\n",
            "     47        \u001b[36m0.1007\u001b[0m        0.1508  1.2466\n",
            "     48        \u001b[36m0.0926\u001b[0m        0.1381  1.1433\n",
            "     49        \u001b[36m0.0901\u001b[0m        0.1317  1.1589\n",
            "     50        0.0937        \u001b[32m0.1284\u001b[0m  1.1450\n",
            "     51        \u001b[36m0.0878\u001b[0m        \u001b[32m0.1280\u001b[0m  1.1454\n",
            "     52        \u001b[36m0.0870\u001b[0m        \u001b[32m0.1209\u001b[0m  1.1418\n",
            "     53        \u001b[36m0.0856\u001b[0m        0.1375  1.1412\n",
            "     54        0.0872        0.1328  1.1386\n",
            "     55        0.0864        0.1239  1.1676\n",
            "     56        0.0896        0.1408  1.4868\n",
            "     57        0.0889        0.1353  1.4952\n",
            "     58        0.0863        0.1699  1.2620\n",
            "     59        0.0865        0.1688  1.1402\n",
            "     60        0.0860        0.1446  1.1453\n",
            "     61        \u001b[36m0.0853\u001b[0m        0.1325  1.1320\n",
            "     62        \u001b[36m0.0847\u001b[0m        0.1567  1.1372\n",
            "     63        0.0849        0.1647  1.1354\n",
            "     64        0.0848        0.1627  1.1779\n",
            "     65        0.0857        0.1279  1.2183\n",
            "     66        0.0853        0.1251  1.5730\n",
            "     67        \u001b[36m0.0835\u001b[0m        0.1482  1.9132\n",
            "     68        \u001b[36m0.0821\u001b[0m        0.1484  1.7992\n",
            "     69        0.0847        0.1433  1.1230\n",
            "     70        0.0838        0.1409  1.1426\n",
            "     71        0.0835        0.1337  1.1495\n",
            "     72        0.0825        0.1480  1.1450\n",
            "     73        0.0833        0.1440  1.1728\n",
            "     74        0.0826        0.1310  1.1447\n",
            "     75        0.0829        0.1511  1.1421\n",
            "     76        \u001b[36m0.0819\u001b[0m        0.1367  1.1383\n",
            "     77        \u001b[36m0.0801\u001b[0m        0.1541  1.3653\n",
            "     78        0.0839        0.1774  1.4673\n",
            "     79        0.0822        0.1357  1.4358\n",
            "     80        0.0824        0.1549  1.1242\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.7min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1632\u001b[0m        \u001b[32m0.2048\u001b[0m  1.1511\n",
            "      2        \u001b[36m0.1504\u001b[0m        \u001b[32m0.1985\u001b[0m  1.1323\n",
            "      3        0.1505        \u001b[32m0.1935\u001b[0m  1.1434\n",
            "      4        \u001b[36m0.1486\u001b[0m        \u001b[32m0.1912\u001b[0m  1.1397\n",
            "      5        \u001b[36m0.1440\u001b[0m        \u001b[32m0.1907\u001b[0m  1.1417\n",
            "      6        \u001b[36m0.1435\u001b[0m        0.1913  1.1372\n",
            "      7        \u001b[36m0.1419\u001b[0m        \u001b[32m0.1862\u001b[0m  1.1463\n",
            "      8        \u001b[36m0.1406\u001b[0m        \u001b[32m0.1848\u001b[0m  1.4116\n",
            "      9        \u001b[36m0.1400\u001b[0m        0.1853  1.4828\n",
            "     10        \u001b[36m0.1392\u001b[0m        \u001b[32m0.1836\u001b[0m  1.3907\n",
            "     11        \u001b[36m0.1382\u001b[0m        \u001b[32m0.1829\u001b[0m  1.1417\n",
            "     12        \u001b[36m0.1358\u001b[0m        \u001b[32m0.1785\u001b[0m  1.1427\n",
            "     13        0.1368        \u001b[32m0.1725\u001b[0m  1.1393\n",
            "     14        \u001b[36m0.1336\u001b[0m        \u001b[32m0.1719\u001b[0m  1.1408\n",
            "     15        \u001b[36m0.1304\u001b[0m        \u001b[32m0.1666\u001b[0m  1.1309\n",
            "     16        \u001b[36m0.1298\u001b[0m        \u001b[32m0.1661\u001b[0m  1.1358\n",
            "     17        \u001b[36m0.1283\u001b[0m        \u001b[32m0.1657\u001b[0m  1.1289\n",
            "     18        0.1308        0.1723  1.1465\n",
            "     19        0.1309        \u001b[32m0.1519\u001b[0m  1.3576\n",
            "     20        0.1301        0.1692  1.4641\n",
            "     21        \u001b[36m0.1272\u001b[0m        0.1591  1.4246\n",
            "     22        0.1289        0.1654  1.1150\n",
            "     23        0.1336        0.1590  1.1243\n",
            "     24        \u001b[36m0.1268\u001b[0m        0.1627  1.1389\n",
            "     25        0.1272        0.1641  1.1372\n",
            "     26        0.1338        0.1632  1.1435\n",
            "     27        \u001b[36m0.1254\u001b[0m        0.1630  1.1358\n",
            "     28        0.1316        \u001b[32m0.1472\u001b[0m  1.1485\n",
            "     29        \u001b[36m0.1168\u001b[0m        0.1565  1.1457\n",
            "     30        0.1173        0.1501  1.3073\n",
            "     31        0.1217        \u001b[32m0.1430\u001b[0m  1.4636\n",
            "     32        \u001b[36m0.1108\u001b[0m        \u001b[32m0.1318\u001b[0m  1.5144\n",
            "     33        \u001b[36m0.1099\u001b[0m        \u001b[32m0.1301\u001b[0m  1.2147\n",
            "     34        \u001b[36m0.1055\u001b[0m        \u001b[32m0.1301\u001b[0m  1.4192\n",
            "     35        \u001b[36m0.1012\u001b[0m        0.1362  1.4062\n",
            "     36        \u001b[36m0.0985\u001b[0m        0.1481  1.4770\n",
            "     37        \u001b[36m0.0966\u001b[0m        0.1463  1.2714\n",
            "     38        \u001b[36m0.0956\u001b[0m        0.1470  1.1363\n",
            "     39        \u001b[36m0.0938\u001b[0m        0.1509  1.1450\n",
            "     40        \u001b[36m0.0929\u001b[0m        0.1562  1.2857\n",
            "     41        \u001b[36m0.0929\u001b[0m        0.1616  1.5021\n",
            "     42        \u001b[36m0.0927\u001b[0m        0.1797  1.5391\n",
            "     43        \u001b[36m0.0922\u001b[0m        0.1736  1.1336\n",
            "     44        \u001b[36m0.0920\u001b[0m        0.1754  1.1344\n",
            "     45        0.0923        0.1764  1.1333\n",
            "     46        0.0921        0.1810  1.1408\n",
            "     47        \u001b[36m0.0914\u001b[0m        0.1837  1.1357\n",
            "     48        \u001b[36m0.0911\u001b[0m        0.1899  1.1499\n",
            "     49        0.0915        0.1898  1.1385\n",
            "     50        0.0912        0.2002  1.1515\n",
            "     51        \u001b[36m0.0910\u001b[0m        0.1980  1.2438\n",
            "     52        \u001b[36m0.0907\u001b[0m        0.2015  1.4778\n",
            "     53        0.0914        0.1996  1.5262\n",
            "     54        0.0908        0.2110  1.1736\n",
            "     55        \u001b[36m0.0906\u001b[0m        0.2041  1.1438\n",
            "     56        \u001b[36m0.0903\u001b[0m        0.2147  1.1590\n",
            "     57        \u001b[36m0.0903\u001b[0m        0.2112  1.1479\n",
            "     58        \u001b[36m0.0902\u001b[0m        0.2158  1.1434\n",
            "     59        0.0905        0.2094  1.1295\n",
            "     60        \u001b[36m0.0901\u001b[0m        0.2196  1.1491\n",
            "     61        \u001b[36m0.0901\u001b[0m        0.2133  1.1408\n",
            "     62        \u001b[36m0.0899\u001b[0m        0.2207  1.2134\n",
            "     63        \u001b[36m0.0899\u001b[0m        0.2196  1.4573\n",
            "     64        \u001b[36m0.0897\u001b[0m        0.2231  1.5157\n",
            "     65        \u001b[36m0.0896\u001b[0m        0.2207  1.2134\n",
            "     66        0.0898        0.2198  1.1518\n",
            "     67        \u001b[36m0.0895\u001b[0m        0.2251  1.1422\n",
            "     68        \u001b[36m0.0894\u001b[0m        0.2180  1.1395\n",
            "     69        \u001b[36m0.0893\u001b[0m        0.2228  1.1387\n",
            "     70        \u001b[36m0.0892\u001b[0m        0.2226  1.1478\n",
            "     71        \u001b[36m0.0892\u001b[0m        0.2202  1.1387\n",
            "     72        \u001b[36m0.0891\u001b[0m        0.2233  1.1392\n",
            "     73        \u001b[36m0.0890\u001b[0m        0.2227  1.2026\n",
            "     74        \u001b[36m0.0888\u001b[0m        0.2217  1.4567\n",
            "     75        \u001b[36m0.0888\u001b[0m        0.2248  1.5357\n",
            "     76        \u001b[36m0.0887\u001b[0m        0.2263  1.2142\n",
            "     77        0.0890        0.2249  1.1500\n",
            "     78        0.0893        0.2305  1.1199\n",
            "     79        0.0890        0.2395  1.1468\n",
            "     80        0.0888        0.2353  1.1602\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.6min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1635\u001b[0m        \u001b[32m0.2073\u001b[0m  1.4304\n",
            "      2        \u001b[36m0.1533\u001b[0m        \u001b[32m0.2065\u001b[0m  1.5722\n",
            "      3        \u001b[36m0.1515\u001b[0m        \u001b[32m0.2061\u001b[0m  1.7570\n",
            "      4        \u001b[36m0.1515\u001b[0m        \u001b[32m0.2047\u001b[0m  1.8567\n",
            "      5        0.1519        \u001b[32m0.2038\u001b[0m  1.5945\n",
            "      6        \u001b[36m0.1485\u001b[0m        \u001b[32m0.2038\u001b[0m  1.2546\n",
            "      7        \u001b[36m0.1480\u001b[0m        0.2066  1.2574\n",
            "      8        \u001b[36m0.1452\u001b[0m        0.2076  1.2829\n",
            "      9        \u001b[36m0.1449\u001b[0m        0.2045  1.2732\n",
            "     10        \u001b[36m0.1424\u001b[0m        \u001b[32m0.2032\u001b[0m  1.2596\n",
            "     11        \u001b[36m0.1408\u001b[0m        \u001b[32m0.2020\u001b[0m  1.2638\n",
            "     12        \u001b[36m0.1384\u001b[0m        \u001b[32m0.1959\u001b[0m  1.2556\n",
            "     13        0.1385        \u001b[32m0.1903\u001b[0m  1.3921\n",
            "     14        \u001b[36m0.1370\u001b[0m        \u001b[32m0.1836\u001b[0m  1.6030\n",
            "     15        0.1377        \u001b[32m0.1785\u001b[0m  1.6152\n",
            "     16        0.1378        \u001b[32m0.1665\u001b[0m  1.2572\n",
            "     17        \u001b[36m0.1358\u001b[0m        0.1721  1.2563\n",
            "     18        \u001b[36m0.1349\u001b[0m        0.1768  1.2625\n",
            "     19        \u001b[36m0.1343\u001b[0m        0.1724  1.2822\n",
            "     20        \u001b[36m0.1332\u001b[0m        \u001b[32m0.1534\u001b[0m  1.2556\n",
            "     21        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1486\u001b[0m  1.2694\n",
            "     22        \u001b[36m0.1278\u001b[0m        0.1545  1.2760\n",
            "     23        0.1306        \u001b[32m0.1430\u001b[0m  1.3920\n",
            "     24        \u001b[36m0.1256\u001b[0m        \u001b[32m0.1395\u001b[0m  1.5987\n",
            "     25        \u001b[36m0.1215\u001b[0m        \u001b[32m0.1363\u001b[0m  1.6005\n",
            "     26        \u001b[36m0.1205\u001b[0m        0.1373  1.2492\n",
            "     27        \u001b[36m0.1135\u001b[0m        0.1616  1.2521\n",
            "     28        \u001b[36m0.1096\u001b[0m        0.1701  1.2524\n",
            "     29        0.1124        0.1364  1.2612\n",
            "     30        \u001b[36m0.1060\u001b[0m        0.1889  1.2834\n",
            "     31        \u001b[36m0.1049\u001b[0m        0.1800  1.2632\n",
            "     32        \u001b[36m0.1034\u001b[0m        \u001b[32m0.1349\u001b[0m  1.2570\n",
            "     33        \u001b[36m0.1011\u001b[0m        0.1504  1.3752\n",
            "     34        \u001b[36m0.1000\u001b[0m        0.1586  1.6004\n",
            "     35        0.1002        0.1800  1.6254\n",
            "     36        \u001b[36m0.0991\u001b[0m        0.1441  1.2604\n",
            "     37        0.0999        0.1389  1.2631\n",
            "     38        \u001b[36m0.0976\u001b[0m        0.2122  1.2687\n",
            "     39        \u001b[36m0.0956\u001b[0m        0.2106  1.2676\n",
            "     40        \u001b[36m0.0955\u001b[0m        0.1758  1.2632\n",
            "     41        \u001b[36m0.0951\u001b[0m        0.1938  1.2609\n",
            "     42        \u001b[36m0.0948\u001b[0m        0.1956  1.2742\n",
            "     43        0.0949        0.1888  1.3734\n",
            "     44        \u001b[36m0.0933\u001b[0m        0.1913  1.6248\n",
            "     45        \u001b[36m0.0925\u001b[0m        0.2010  2.0079\n",
            "     46        0.0931        0.1785  1.5089\n",
            "     47        \u001b[36m0.0924\u001b[0m        0.1484  1.5997\n",
            "     48        0.0937        0.1635  1.4006\n",
            "     49        \u001b[36m0.0905\u001b[0m        0.2369  1.2498\n",
            "     50        0.0934        0.1771  1.2627\n",
            "     51        0.0919        0.1828  1.2743\n",
            "     52        0.0916        0.2015  1.2894\n",
            "     53        0.0921        0.1974  1.5722\n",
            "     54        0.0910        0.1615  1.6347\n",
            "     55        \u001b[36m0.0899\u001b[0m        0.2071  1.3530\n",
            "     56        0.0907        0.1976  1.2606\n",
            "     57        0.0918        0.1892  1.2628\n",
            "     58        0.0920        0.1401  1.2765\n",
            "     59        0.0903        0.1799  1.2513\n",
            "     60        \u001b[36m0.0896\u001b[0m        0.1679  1.2646\n",
            "     61        \u001b[36m0.0895\u001b[0m        0.1784  1.2464\n",
            "     62        \u001b[36m0.0894\u001b[0m        0.1953  1.2621\n",
            "     63        0.0901        0.1539  1.6194\n",
            "     64        \u001b[36m0.0887\u001b[0m        0.1780  1.6789\n",
            "     65        0.0894        0.1769  1.2604\n",
            "     66        \u001b[36m0.0881\u001b[0m        0.1984  1.2782\n",
            "     67        0.0881        0.1646  1.2680\n",
            "     68        0.0893        0.1497  1.2599\n",
            "     69        \u001b[36m0.0874\u001b[0m        0.1605  1.2612\n",
            "     70        0.0878        0.1620  1.2575\n",
            "     71        \u001b[36m0.0871\u001b[0m        0.1753  1.2587\n",
            "     72        0.0880        0.1778  1.3365\n",
            "     73        \u001b[36m0.0860\u001b[0m        0.1526  1.6200\n",
            "     74        0.0866        0.1721  1.7038\n",
            "     75        0.0874        0.1798  1.2693\n",
            "     76        0.0867        0.1831  1.2638\n",
            "     77        0.0870        0.1471  1.2730\n",
            "     78        \u001b[36m0.0860\u001b[0m        0.1760  1.2683\n",
            "     79        0.0864        0.1524  1.2669\n",
            "     80        0.0869        0.1451  1.2625\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.8min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1680\u001b[0m        \u001b[32m0.2106\u001b[0m  1.2975\n",
            "      2        \u001b[36m0.1545\u001b[0m        \u001b[32m0.2094\u001b[0m  1.4234\n",
            "      3        \u001b[36m0.1541\u001b[0m        0.2096  1.6175\n",
            "      4        0.1553        \u001b[32m0.2093\u001b[0m  1.5772\n",
            "      5        0.1554        0.2096  1.2486\n",
            "      6        0.1550        \u001b[32m0.2082\u001b[0m  1.2664\n",
            "      7        0.1551        0.2099  1.2574\n",
            "      8        0.1554        0.2087  1.4665\n",
            "      9        0.1573        0.2097  1.5307\n",
            "     10        0.1543        0.2092  1.6067\n",
            "     11        0.1564        0.2117  1.5298\n",
            "     12        \u001b[36m0.1537\u001b[0m        0.2108  1.6059\n",
            "     13        0.1560        0.2140  1.6367\n",
            "     14        0.1543        0.2124  1.2563\n",
            "     15        0.1554        0.2124  1.2877\n",
            "     16        0.1545        0.2128  1.2533\n",
            "     17        0.1554        0.2120  1.2512\n",
            "     18        0.1547        0.2133  1.2605\n",
            "     19        \u001b[36m0.1534\u001b[0m        0.2138  1.2545\n",
            "     20        0.1544        0.2137  1.2553\n",
            "     21        0.1541        0.2136  1.3225\n",
            "     22        \u001b[36m0.1534\u001b[0m        0.2129  1.6088\n",
            "     23        0.1538        0.2132  1.6620\n",
            "     24        0.1539        0.2120  1.2495\n",
            "     25        0.1537        0.2129  1.2579\n",
            "     26        0.1564        0.2123  1.2521\n",
            "     27        0.1537        0.2133  1.2519\n",
            "     28        \u001b[36m0.1522\u001b[0m        0.2135  1.2745\n",
            "     29        0.1550        0.2129  1.2616\n",
            "     30        0.1523        0.2130  1.2805\n",
            "     31        \u001b[36m0.1518\u001b[0m        0.2134  1.3217\n",
            "     32        \u001b[36m0.1515\u001b[0m        0.2150  1.6075\n",
            "     33        \u001b[36m0.1508\u001b[0m        0.2156  1.6077\n",
            "     34        \u001b[36m0.1499\u001b[0m        0.2175  1.2596\n",
            "     35        \u001b[36m0.1489\u001b[0m        0.2172  1.2687\n",
            "     36        \u001b[36m0.1488\u001b[0m        0.2184  1.2570\n",
            "     37        \u001b[36m0.1481\u001b[0m        0.2186  1.2590\n",
            "     38        0.1489        0.2263  1.2636\n",
            "     39        \u001b[36m0.1478\u001b[0m        0.2249  1.2698\n",
            "     40        0.1482        0.2286  1.2571\n",
            "     41        0.1483        0.2327  1.3704\n",
            "     42        \u001b[36m0.1477\u001b[0m        0.2360  1.6233\n",
            "     43        \u001b[36m0.1472\u001b[0m        0.2417  1.6122\n",
            "     44        \u001b[36m0.1462\u001b[0m        0.2436  1.2608\n",
            "     45        0.1463        0.2472  1.2679\n",
            "     46        \u001b[36m0.1461\u001b[0m        0.2449  1.2442\n",
            "     47        0.1462        0.2363  1.2569\n",
            "     48        0.1463        0.2661  1.2625\n",
            "     49        \u001b[36m0.1457\u001b[0m        0.2595  1.2580\n",
            "     50        0.1477        0.2654  1.2723\n",
            "     51        0.1504        0.2476  1.3872\n",
            "     52        0.1462        0.2276  1.8518\n",
            "     53        0.1489        0.2500  2.0661\n",
            "     54        0.1465        0.2704  1.6109\n",
            "     55        0.1465        0.2694  1.4068\n",
            "     56        0.1473        0.2731  1.2662\n",
            "     57        \u001b[36m0.1455\u001b[0m        0.2622  1.2534\n",
            "     58        0.1467        0.2390  1.2708\n",
            "     59        0.1462        0.2576  1.2822\n",
            "     60        0.1463        0.2653  1.2612\n",
            "     61        0.1464        0.2777  1.5550\n",
            "     62        0.1460        0.2678  1.6408\n",
            "     63        0.1463        0.2670  1.3210\n",
            "     64        0.1460        0.2261  1.2543\n",
            "     65        0.1460        0.2461  1.2538\n",
            "     66        \u001b[36m0.1451\u001b[0m        0.2759  1.2470\n",
            "     67        0.1453        0.2723  1.2530\n",
            "     68        \u001b[36m0.1443\u001b[0m        0.2751  1.2776\n",
            "     69        \u001b[36m0.1437\u001b[0m        0.2821  1.2580\n",
            "     70        0.1451        0.2814  1.2584\n",
            "     71        \u001b[36m0.1435\u001b[0m        0.2818  1.6048\n",
            "     72        0.1458        0.2707  1.6489\n",
            "     73        0.1456        0.2195  1.3630\n",
            "     74        0.1441        0.2865  1.2603\n",
            "     75        0.1435        0.2754  1.2544\n",
            "     76        0.1453        0.2699  1.2513\n",
            "     77        0.1450        0.2793  1.2551\n",
            "     78        0.1447        0.2784  1.2597\n",
            "     79        0.1450        0.2912  1.2550\n",
            "     80        0.1449        0.3060  1.2534\n",
            "[CV] END batch_size=32, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.8min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.2969\u001b[0m        \u001b[32m0.2420\u001b[0m  0.8386\n",
            "      2        \u001b[36m0.1648\u001b[0m        0.2420  0.8566\n",
            "      3        \u001b[36m0.1573\u001b[0m        \u001b[32m0.2223\u001b[0m  0.8553\n",
            "      4        \u001b[36m0.1498\u001b[0m        0.2351  0.8387\n",
            "      5        0.1529        \u001b[32m0.2197\u001b[0m  0.6272\n",
            "      6        \u001b[36m0.1484\u001b[0m        \u001b[32m0.2045\u001b[0m  0.6387\n",
            "      7        \u001b[36m0.1430\u001b[0m        \u001b[32m0.2000\u001b[0m  0.6386\n",
            "      8        \u001b[36m0.1426\u001b[0m        0.2186  0.6302\n",
            "      9        0.1427        \u001b[32m0.1978\u001b[0m  0.6340\n",
            "     10        0.1432        \u001b[32m0.1792\u001b[0m  0.6308\n",
            "     11        \u001b[36m0.1326\u001b[0m        0.1915  0.6349\n",
            "     12        0.1337        \u001b[32m0.1669\u001b[0m  0.6353\n",
            "     13        \u001b[36m0.1302\u001b[0m        0.2073  0.6297\n",
            "     14        \u001b[36m0.1279\u001b[0m        \u001b[32m0.1542\u001b[0m  0.6375\n",
            "     15        0.1292        0.1950  0.6369\n",
            "     16        \u001b[36m0.1199\u001b[0m        0.1597  0.6319\n",
            "     17        \u001b[36m0.1165\u001b[0m        0.2033  0.6403\n",
            "     18        \u001b[36m0.1134\u001b[0m        0.1644  0.6292\n",
            "     19        \u001b[36m0.1127\u001b[0m        0.1803  0.6385\n",
            "     20        \u001b[36m0.1094\u001b[0m        0.1710  0.7450\n",
            "     21        \u001b[36m0.1088\u001b[0m        0.1703  0.8340\n",
            "     22        \u001b[36m0.1076\u001b[0m        0.1668  0.8216\n",
            "     23        0.1168        0.2434  0.8678\n",
            "     24        0.1129        0.1758  0.7934\n",
            "     25        0.1084        0.1863  0.6390\n",
            "     26        0.1148        0.1664  0.6294\n",
            "     27        \u001b[36m0.1067\u001b[0m        0.2062  0.6448\n",
            "     28        \u001b[36m0.1043\u001b[0m        0.1931  0.6304\n",
            "     29        0.1059        \u001b[32m0.1428\u001b[0m  0.7408\n",
            "     30        0.1062        0.1846  0.8463\n",
            "     31        \u001b[36m0.1036\u001b[0m        0.1583  0.7854\n",
            "     32        \u001b[36m0.0998\u001b[0m        0.1672  0.7974\n",
            "     33        \u001b[36m0.0995\u001b[0m        0.1436  0.8456\n",
            "     34        0.1008        0.1657  0.7938\n",
            "     35        0.1017        0.1765  0.7886\n",
            "     36        0.1005        \u001b[32m0.1313\u001b[0m  0.6257\n",
            "     37        \u001b[36m0.0989\u001b[0m        0.1504  0.6341\n",
            "     38        \u001b[36m0.0977\u001b[0m        0.1603  0.7511\n",
            "     39        \u001b[36m0.0975\u001b[0m        0.1428  0.8270\n",
            "     40        \u001b[36m0.0961\u001b[0m        0.1771  0.8182\n",
            "     41        \u001b[36m0.0950\u001b[0m        \u001b[32m0.1245\u001b[0m  0.8681\n",
            "     42        0.0962        0.2220  0.7736\n",
            "     43        0.1006        0.1726  0.6578\n",
            "     44        0.1017        0.1531  0.6464\n",
            "     45        0.0954        0.1631  0.6420\n",
            "     46        \u001b[36m0.0944\u001b[0m        0.1298  0.6425\n",
            "     47        \u001b[36m0.0943\u001b[0m        0.1900  0.6311\n",
            "     48        \u001b[36m0.0939\u001b[0m        0.1432  0.6398\n",
            "     49        0.0965        0.1777  0.6271\n",
            "     50        0.0978        0.1801  0.6392\n",
            "     51        0.0958        0.1295  0.6394\n",
            "     52        \u001b[36m0.0934\u001b[0m        0.1993  0.6294\n",
            "     53        \u001b[36m0.0926\u001b[0m        0.1321  0.6368\n",
            "     54        0.0934        0.2070  0.6382\n",
            "     55        0.0939        0.1559  0.6275\n",
            "     56        0.0944        0.1395  0.6387\n",
            "     57        \u001b[36m0.0925\u001b[0m        0.1937  0.6310\n",
            "     58        \u001b[36m0.0910\u001b[0m        0.1276  0.8103\n",
            "     59        0.0921        0.2171  0.8572\n",
            "     60        0.0942        0.1683  0.8164\n",
            "     61        0.0944        0.1563  0.8950\n",
            "     62        0.0928        0.1871  0.7434\n",
            "     63        \u001b[36m0.0907\u001b[0m        0.1325  0.6349\n",
            "     64        \u001b[36m0.0897\u001b[0m        0.1912  0.6398\n",
            "     65        \u001b[36m0.0891\u001b[0m        0.1418  0.6287\n",
            "     66        0.0892        0.1852  0.6376\n",
            "     67        \u001b[36m0.0891\u001b[0m        0.1496  0.6293\n",
            "     68        0.0908        0.2098  0.6353\n",
            "     69        0.0895        0.1451  0.6449\n",
            "     70        0.0906        0.2168  0.6312\n",
            "     71        0.0903        0.1622  0.6370\n",
            "     72        0.0903        0.2007  0.6402\n",
            "     73        \u001b[36m0.0889\u001b[0m        0.1653  0.6550\n",
            "     74        0.0894        0.1855  0.6366\n",
            "     75        \u001b[36m0.0875\u001b[0m        0.1547  0.6297\n",
            "     76        0.0877        0.1704  0.6470\n",
            "     77        \u001b[36m0.0865\u001b[0m        0.1453  0.6740\n",
            "     78        0.0868        0.1755  0.8406\n",
            "     79        0.0870        0.1668  0.8432\n",
            "     80        \u001b[36m0.0854\u001b[0m        0.1541  0.8596\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  56.8s\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.3230\u001b[0m        \u001b[32m0.2351\u001b[0m  0.7099\n",
            "      2        \u001b[36m0.1589\u001b[0m        0.2383  0.6296\n",
            "      3        \u001b[36m0.1512\u001b[0m        \u001b[32m0.2277\u001b[0m  0.6283\n",
            "      4        0.1539        \u001b[32m0.2216\u001b[0m  0.6333\n",
            "      5        \u001b[36m0.1483\u001b[0m        \u001b[32m0.2141\u001b[0m  0.6338\n",
            "      6        0.1538        \u001b[32m0.2066\u001b[0m  0.6282\n",
            "      7        \u001b[36m0.1391\u001b[0m        \u001b[32m0.1945\u001b[0m  0.6373\n",
            "      8        0.1463        \u001b[32m0.1882\u001b[0m  0.6293\n",
            "      9        0.1399        \u001b[32m0.1817\u001b[0m  0.6349\n",
            "     10        0.1470        \u001b[32m0.1729\u001b[0m  0.6326\n",
            "     11        \u001b[36m0.1298\u001b[0m        \u001b[32m0.1525\u001b[0m  0.6269\n",
            "     12        0.1397        \u001b[32m0.1502\u001b[0m  0.6583\n",
            "     13        0.1346        \u001b[32m0.1330\u001b[0m  0.6362\n",
            "     14        0.1299        0.1475  0.6307\n",
            "     15        0.1313        0.1879  0.6343\n",
            "     16        0.1321        0.1914  0.6255\n",
            "     17        \u001b[36m0.1294\u001b[0m        0.1879  0.8392\n",
            "     18        \u001b[36m0.1247\u001b[0m        0.1772  0.8450\n",
            "     19        \u001b[36m0.1197\u001b[0m        0.1604  0.8351\n",
            "     20        \u001b[36m0.1192\u001b[0m        0.1547  0.8753\n",
            "     21        \u001b[36m0.1149\u001b[0m        0.1656  0.6597\n",
            "     22        \u001b[36m0.1133\u001b[0m        0.1575  0.6419\n",
            "     23        0.1268        0.2378  0.6379\n",
            "     24        0.1186        0.1624  0.6262\n",
            "     25        0.1197        0.1481  0.6379\n",
            "     26        \u001b[36m0.1116\u001b[0m        \u001b[32m0.1295\u001b[0m  0.6280\n",
            "     27        \u001b[36m0.1097\u001b[0m        0.1831  0.6372\n",
            "     28        0.1122        0.1581  0.6390\n",
            "     29        \u001b[36m0.1088\u001b[0m        0.1476  0.6262\n",
            "     30        0.1105        0.1298  0.6373\n",
            "     31        \u001b[36m0.1063\u001b[0m        0.1466  0.6408\n",
            "     32        0.1096        0.1357  0.6275\n",
            "     33        \u001b[36m0.1042\u001b[0m        0.1649  0.6429\n",
            "     34        0.1104        0.1630  0.6355\n",
            "     35        0.1080        0.1353  0.6347\n",
            "     36        0.1078        0.1438  0.8207\n",
            "     37        0.1044        0.2134  1.1403\n",
            "     38        0.1089        0.1295  1.0437\n",
            "     39        0.1093        \u001b[32m0.1268\u001b[0m  1.1880\n",
            "     40        \u001b[36m0.1040\u001b[0m        0.1842  0.9471\n",
            "     41        \u001b[36m0.1010\u001b[0m        0.1688  0.7340\n",
            "     42        \u001b[36m0.0972\u001b[0m        0.1834  0.6292\n",
            "     43        \u001b[36m0.0949\u001b[0m        0.1272  0.6408\n",
            "     44        0.0972        0.1405  0.6492\n",
            "     45        \u001b[36m0.0925\u001b[0m        0.1788  0.6311\n",
            "     46        0.0945        \u001b[32m0.1203\u001b[0m  0.6382\n",
            "     47        \u001b[36m0.0903\u001b[0m        0.1233  0.6321\n",
            "     48        \u001b[36m0.0878\u001b[0m        0.1220  0.6380\n",
            "     49        \u001b[36m0.0869\u001b[0m        0.1241  0.6260\n",
            "     50        \u001b[36m0.0863\u001b[0m        0.1238  0.6637\n",
            "     51        \u001b[36m0.0860\u001b[0m        0.1219  0.6370\n",
            "     52        \u001b[36m0.0855\u001b[0m        0.1249  0.6367\n",
            "     53        \u001b[36m0.0850\u001b[0m        0.1273  0.6328\n",
            "     54        0.0851        0.1228  0.6357\n",
            "     55        \u001b[36m0.0845\u001b[0m        0.1283  0.7165\n",
            "     56        0.0846        0.1322  0.8194\n",
            "     57        0.0867        0.1545  0.8458\n",
            "     58        0.0880        0.1233  0.8715\n",
            "     59        0.0849        0.1448  0.7236\n",
            "     60        0.0853        0.1454  0.6304\n",
            "     61        0.0871        \u001b[32m0.1168\u001b[0m  0.6381\n",
            "     62        0.0859        0.1412  0.6363\n",
            "     63        0.0863        0.1548  0.6300\n",
            "     64        0.0880        \u001b[32m0.1077\u001b[0m  0.6409\n",
            "     65        0.0849        0.1188  0.6317\n",
            "     66        \u001b[36m0.0834\u001b[0m        0.1579  0.6377\n",
            "     67        0.0868        0.1104  0.6339\n",
            "     68        0.0855        0.1173  0.6302\n",
            "     69        \u001b[36m0.0830\u001b[0m        0.1463  0.6372\n",
            "     70        0.0852        \u001b[32m0.1069\u001b[0m  0.6374\n",
            "     71        0.0832        0.1163  0.6301\n",
            "     72        \u001b[36m0.0815\u001b[0m        0.1325  0.6309\n",
            "     73        0.0831        0.1094  0.6216\n",
            "     74        0.0822        0.1140  0.6408\n",
            "     75        \u001b[36m0.0814\u001b[0m        0.1161  0.8470\n",
            "     76        0.0821        0.1234  0.8497\n",
            "     77        0.0816        0.1078  0.8293\n",
            "     78        \u001b[36m0.0812\u001b[0m        0.1186  0.9125\n",
            "     79        0.0823        0.1322  0.6835\n",
            "     80        0.0845        \u001b[32m0.1055\u001b[0m  0.6383\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  56.0s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1735\u001b[0m        \u001b[32m0.2064\u001b[0m  0.6679\n",
            "      2        0.1832        \u001b[32m0.2061\u001b[0m  0.6721\n",
            "      3        0.1757        \u001b[32m0.2037\u001b[0m  0.6922\n",
            "      4        \u001b[36m0.1692\u001b[0m        0.2096  0.6705\n",
            "      5        \u001b[36m0.1643\u001b[0m        0.2073  0.6736\n",
            "      6        \u001b[36m0.1596\u001b[0m        0.2087  0.6781\n",
            "      7        \u001b[36m0.1560\u001b[0m        0.2042  0.6618\n",
            "      8        \u001b[36m0.1533\u001b[0m        \u001b[32m0.1925\u001b[0m  0.6699\n",
            "      9        \u001b[36m0.1487\u001b[0m        0.2161  0.6760\n",
            "     10        \u001b[36m0.1478\u001b[0m        \u001b[32m0.1849\u001b[0m  0.6638\n",
            "     11        \u001b[36m0.1469\u001b[0m        0.2269  0.6778\n",
            "     12        \u001b[36m0.1466\u001b[0m        \u001b[32m0.1773\u001b[0m  0.6774\n",
            "     13        \u001b[36m0.1456\u001b[0m        0.1971  0.7497\n",
            "     14        \u001b[36m0.1428\u001b[0m        \u001b[32m0.1679\u001b[0m  0.8990\n",
            "     15        \u001b[36m0.1425\u001b[0m        0.1732  0.9065\n",
            "     16        \u001b[36m0.1413\u001b[0m        \u001b[32m0.1642\u001b[0m  0.9371\n",
            "     17        0.1418        0.2019  0.8311\n",
            "     18        0.1477        0.1921  0.6726\n",
            "     19        0.1450        0.2025  0.6751\n",
            "     20        0.1420        \u001b[32m0.1626\u001b[0m  0.6782\n",
            "     21        \u001b[36m0.1338\u001b[0m        0.2278  0.6728\n",
            "     22        0.1385        \u001b[32m0.1568\u001b[0m  0.6804\n",
            "     23        0.1346        0.2428  0.6752\n",
            "     24        \u001b[36m0.1299\u001b[0m        \u001b[32m0.1412\u001b[0m  0.6705\n",
            "     25        \u001b[36m0.1263\u001b[0m        0.2201  0.6760\n",
            "     26        \u001b[36m0.1239\u001b[0m        \u001b[32m0.1208\u001b[0m  0.6766\n",
            "     27        \u001b[36m0.1170\u001b[0m        0.1950  0.6731\n",
            "     28        \u001b[36m0.1139\u001b[0m        0.1443  0.6807\n",
            "     29        \u001b[36m0.1080\u001b[0m        0.1444  0.6762\n",
            "     30        \u001b[36m0.1045\u001b[0m        0.1427  0.6743\n",
            "     31        \u001b[36m0.1011\u001b[0m        0.1663  0.6774\n",
            "     32        \u001b[36m0.0994\u001b[0m        0.1400  0.8564\n",
            "     33        \u001b[36m0.0986\u001b[0m        0.1704  0.9114\n",
            "     34        0.0992        0.1457  0.9043\n",
            "     35        0.0992        0.1700  0.9525\n",
            "     36        0.1011        0.1385  0.6965\n",
            "     37        \u001b[36m0.0971\u001b[0m        0.1426  0.6749\n",
            "     38        \u001b[36m0.0968\u001b[0m        0.1537  0.6766\n",
            "     39        \u001b[36m0.0944\u001b[0m        0.1455  0.7479\n",
            "     40        0.0954        0.1555  0.8622\n",
            "     41        0.0948        0.1616  0.8374\n",
            "     42        \u001b[36m0.0940\u001b[0m        0.1580  0.8288\n",
            "     43        0.0953        0.1640  0.8967\n",
            "     44        0.0960        0.1624  0.8684\n",
            "     45        \u001b[36m0.0939\u001b[0m        0.1504  0.8326\n",
            "     46        \u001b[36m0.0938\u001b[0m        0.1689  0.6823\n",
            "     47        \u001b[36m0.0923\u001b[0m        0.1516  0.6748\n",
            "     48        0.0929        0.1681  0.6711\n",
            "     49        0.0934        0.1562  0.8798\n",
            "     50        0.0924        0.1747  0.9048\n",
            "     51        \u001b[36m0.0916\u001b[0m        0.1578  0.9078\n",
            "     52        0.0926        0.1794  0.8658\n",
            "     53        0.0923        0.1722  0.6755\n",
            "     54        0.0920        0.1554  0.6788\n",
            "     55        0.0928        0.1601  0.6763\n",
            "     56        0.0935        0.1654  0.6699\n",
            "     57        \u001b[36m0.0915\u001b[0m        0.1462  0.6803\n",
            "     58        \u001b[36m0.0906\u001b[0m        0.1864  0.6779\n",
            "     59        \u001b[36m0.0905\u001b[0m        0.1502  0.6721\n",
            "     60        \u001b[36m0.0884\u001b[0m        0.1772  0.6791\n",
            "     61        0.0884        0.1524  0.6762\n",
            "     62        \u001b[36m0.0876\u001b[0m        0.1694  0.6685\n",
            "     63        \u001b[36m0.0870\u001b[0m        0.1603  0.6778\n",
            "     64        \u001b[36m0.0867\u001b[0m        0.1686  0.6791\n",
            "     65        0.0879        0.1513  0.6689\n",
            "     66        0.0880        0.1839  0.6819\n",
            "     67        0.0890        0.1333  0.8528\n",
            "     68        0.0885        0.1781  0.9050\n",
            "     69        0.0899        0.1478  0.8993\n",
            "     70        0.0897        0.1567  0.8899\n",
            "     71        0.0893        0.1731  0.6797\n",
            "     72        0.0875        0.1441  0.6798\n",
            "     73        \u001b[36m0.0852\u001b[0m        0.1700  0.6695\n",
            "     74        0.0869        0.1523  0.6784\n",
            "     75        \u001b[36m0.0849\u001b[0m        0.1560  0.6813\n",
            "     76        \u001b[36m0.0841\u001b[0m        0.1660  0.6808\n",
            "     77        \u001b[36m0.0838\u001b[0m        0.1482  0.6759\n",
            "     78        0.0843        0.1736  0.6811\n",
            "     79        0.0839        0.1464  0.6697\n",
            "     80        \u001b[36m0.0838\u001b[0m        0.1969  0.7008\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  59.2s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1680\u001b[0m        \u001b[32m0.2222\u001b[0m  0.6748\n",
            "      2        0.1775        0.2262  0.6747\n",
            "      3        0.1747        \u001b[32m0.2180\u001b[0m  0.6669\n",
            "      4        0.1762        \u001b[32m0.2162\u001b[0m  0.6754\n",
            "      5        0.1714        0.2178  0.8769\n",
            "      6        0.1685        0.2235  0.9156\n",
            "      7        \u001b[36m0.1655\u001b[0m        0.2270  0.9057\n",
            "      8        \u001b[36m0.1636\u001b[0m        0.2219  0.9267\n",
            "      9        \u001b[36m0.1604\u001b[0m        0.2177  0.6751\n",
            "     10        \u001b[36m0.1581\u001b[0m        \u001b[32m0.2127\u001b[0m  0.6749\n",
            "     11        \u001b[36m0.1566\u001b[0m        \u001b[32m0.2092\u001b[0m  0.6714\n",
            "     12        \u001b[36m0.1526\u001b[0m        0.2150  0.6749\n",
            "     13        0.1536        \u001b[32m0.2028\u001b[0m  0.6815\n",
            "     14        \u001b[36m0.1503\u001b[0m        0.2032  0.6707\n",
            "     15        \u001b[36m0.1488\u001b[0m        \u001b[32m0.1932\u001b[0m  0.6771\n",
            "     16        \u001b[36m0.1473\u001b[0m        0.1942  0.6744\n",
            "     17        \u001b[36m0.1466\u001b[0m        \u001b[32m0.1835\u001b[0m  0.6706\n",
            "     18        \u001b[36m0.1459\u001b[0m        \u001b[32m0.1794\u001b[0m  0.6776\n",
            "     19        \u001b[36m0.1453\u001b[0m        \u001b[32m0.1760\u001b[0m  0.6751\n",
            "     20        \u001b[36m0.1450\u001b[0m        0.1905  0.6714\n",
            "     21        0.1507        0.2061  0.6775\n",
            "     22        \u001b[36m0.1441\u001b[0m        0.1967  0.6742\n",
            "     23        \u001b[36m0.1435\u001b[0m        0.2224  0.7479\n",
            "     24        0.1458        \u001b[32m0.1683\u001b[0m  0.8949\n",
            "     25        0.1452        0.1893  0.9000\n",
            "     26        \u001b[36m0.1397\u001b[0m        0.2049  0.9452\n",
            "     27        0.1461        \u001b[32m0.1515\u001b[0m  0.8271\n",
            "     28        0.1404        0.2312  0.6784\n",
            "     29        0.1469        0.1854  0.6863\n",
            "     30        0.1460        0.1760  0.6827\n",
            "     31        0.1444        \u001b[32m0.1494\u001b[0m  0.6743\n",
            "     32        0.1400        0.2212  0.6835\n",
            "     33        0.1442        0.1625  0.6834\n",
            "     34        0.1441        0.1611  0.6791\n",
            "     35        \u001b[36m0.1395\u001b[0m        0.1930  0.6852\n",
            "     36        0.1420        \u001b[32m0.1434\u001b[0m  0.6856\n",
            "     37        0.1434        0.1669  0.6832\n",
            "     38        0.1416        0.1547  0.6833\n",
            "     39        0.1429        0.1539  0.7334\n",
            "     40        \u001b[36m0.1389\u001b[0m        0.2137  0.8317\n",
            "     41        0.1418        \u001b[32m0.1389\u001b[0m  1.0545\n",
            "     42        0.1411        0.2343  1.1929\n",
            "     43        0.1463        \u001b[32m0.1388\u001b[0m  1.2342\n",
            "     44        0.1459        0.1701  1.1483\n",
            "     45        0.1418        \u001b[32m0.1335\u001b[0m  0.6953\n",
            "     46        0.1425        0.1589  0.6681\n",
            "     47        0.1406        0.1368  0.6709\n",
            "     48        0.1418        0.1451  0.6761\n",
            "     49        \u001b[36m0.1389\u001b[0m        0.1520  0.6695\n",
            "     50        0.1395        0.1554  0.6685\n",
            "     51        0.1396        0.1703  0.6864\n",
            "     52        \u001b[36m0.1387\u001b[0m        0.1625  0.6723\n",
            "     53        0.1408        0.2130  0.6974\n",
            "     54        0.1396        0.2741  0.6671\n",
            "     55        0.1487        0.1718  0.6687\n",
            "     56        0.1448        0.2245  0.6783\n",
            "     57        0.1458        0.1684  0.6789\n",
            "     58        0.1465        0.2088  0.6728\n",
            "     59        0.1413        0.1341  0.7762\n",
            "     60        0.1398        0.1611  0.8924\n",
            "     61        \u001b[36m0.1383\u001b[0m        0.1585  0.8912\n",
            "     62        \u001b[36m0.1368\u001b[0m        0.1789  0.9394\n",
            "     63        0.1393        0.1407  0.8046\n",
            "     64        0.1378        \u001b[32m0.1331\u001b[0m  0.6688\n",
            "     65        \u001b[36m0.1328\u001b[0m        0.1611  0.6720\n",
            "     66        0.1360        0.1482  0.6793\n",
            "     67        0.1366        0.1455  0.6657\n",
            "     68        0.1394        \u001b[32m0.1299\u001b[0m  0.6691\n",
            "     69        \u001b[36m0.1327\u001b[0m        0.2132  0.6782\n",
            "     70        0.1388        0.1767  0.6664\n",
            "     71        0.1438        \u001b[32m0.1270\u001b[0m  0.6704\n",
            "     72        0.1409        0.1606  0.6680\n",
            "     73        0.1384        0.1297  0.6747\n",
            "     74        0.1346        0.2525  0.6996\n",
            "     75        0.1350        0.1949  0.6742\n",
            "     76        0.1364        0.1507  0.6856\n",
            "     77        0.1365        0.1635  0.6895\n",
            "     78        0.1413        \u001b[32m0.1221\u001b[0m  0.8646\n",
            "     79        \u001b[36m0.1312\u001b[0m        0.2014  0.9056\n",
            "     80        0.1418        \u001b[32m0.1214\u001b[0m  0.9176\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=16, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.0min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1649\u001b[0m        \u001b[32m0.2477\u001b[0m  0.7768\n",
            "      2        \u001b[36m0.1561\u001b[0m        \u001b[32m0.2276\u001b[0m  0.6370\n",
            "      3        \u001b[36m0.1444\u001b[0m        \u001b[32m0.2174\u001b[0m  0.6353\n",
            "      4        \u001b[36m0.1417\u001b[0m        \u001b[32m0.2045\u001b[0m  0.6259\n",
            "      5        0.1431        0.2124  0.6345\n",
            "      6        \u001b[36m0.1369\u001b[0m        \u001b[32m0.1832\u001b[0m  0.6336\n",
            "      7        \u001b[36m0.1339\u001b[0m        \u001b[32m0.1798\u001b[0m  0.6286\n",
            "      8        \u001b[36m0.1299\u001b[0m        \u001b[32m0.1792\u001b[0m  0.6474\n",
            "      9        \u001b[36m0.1267\u001b[0m        \u001b[32m0.1672\u001b[0m  0.6264\n",
            "     10        0.1277        \u001b[32m0.1506\u001b[0m  0.6367\n",
            "     11        \u001b[36m0.1251\u001b[0m        0.1780  0.6382\n",
            "     12        \u001b[36m0.1234\u001b[0m        0.1957  0.6273\n",
            "     13        \u001b[36m0.1220\u001b[0m        0.1755  0.6414\n",
            "     14        0.1223        0.1520  0.6393\n",
            "     15        \u001b[36m0.1191\u001b[0m        0.1512  0.6300\n",
            "     16        \u001b[36m0.1145\u001b[0m        0.2149  0.6406\n",
            "     17        \u001b[36m0.1144\u001b[0m        0.1507  0.7926\n",
            "     18        \u001b[36m0.1120\u001b[0m        \u001b[32m0.1331\u001b[0m  0.8291\n",
            "     19        \u001b[36m0.1099\u001b[0m        0.2676  0.8266\n",
            "     20        0.1116        0.1498  0.8831\n",
            "     21        \u001b[36m0.1072\u001b[0m        \u001b[32m0.1304\u001b[0m  0.6387\n",
            "     22        \u001b[36m0.1030\u001b[0m        0.1679  0.6260\n",
            "     23        \u001b[36m0.1004\u001b[0m        0.1441  0.6371\n",
            "     24        \u001b[36m0.0971\u001b[0m        \u001b[32m0.1260\u001b[0m  0.6300\n",
            "     25        \u001b[36m0.0930\u001b[0m        \u001b[32m0.1167\u001b[0m  0.6330\n",
            "     26        \u001b[36m0.0905\u001b[0m        0.1215  0.6366\n",
            "     27        \u001b[36m0.0887\u001b[0m        0.1186  0.6306\n",
            "     28        \u001b[36m0.0875\u001b[0m        0.1359  0.6342\n",
            "     29        \u001b[36m0.0874\u001b[0m        0.1299  0.6357\n",
            "     30        0.0878        0.1192  0.6307\n",
            "     31        \u001b[36m0.0862\u001b[0m        0.1484  0.6389\n",
            "     32        0.0879        0.1343  0.6303\n",
            "     33        0.0880        \u001b[32m0.1166\u001b[0m  0.6332\n",
            "     34        \u001b[36m0.0839\u001b[0m        0.1317  0.6417\n",
            "     35        \u001b[36m0.0839\u001b[0m        0.1302  0.6284\n",
            "     36        \u001b[36m0.0838\u001b[0m        0.1197  0.7320\n",
            "     37        \u001b[36m0.0829\u001b[0m        0.1369  0.8347\n",
            "     38        0.0831        0.1370  0.8403\n",
            "     39        0.0843        0.1215  0.8705\n",
            "     40        0.0836        0.1491  0.7853\n",
            "     41        0.0844        0.1358  0.6385\n",
            "     42        0.0845        0.1193  0.6842\n",
            "     43        \u001b[36m0.0819\u001b[0m        0.1401  0.7445\n",
            "     44        0.0822        0.1399  0.8619\n",
            "     45        0.0823        0.1262  0.7731\n",
            "     46        \u001b[36m0.0819\u001b[0m        0.1479  0.8086\n",
            "     47        0.0830        0.1420  0.8320\n",
            "     48        0.0831        0.1288  0.8215\n",
            "     49        0.0822        0.1535  0.6920\n",
            "     50        0.0840        0.1483  0.6461\n",
            "     51        0.0828        0.1261  0.6381\n",
            "     52        0.0824        0.1428  0.6404\n",
            "     53        0.0835        0.1331  0.6289\n",
            "     54        \u001b[36m0.0810\u001b[0m        0.1250  0.7694\n",
            "     55        \u001b[36m0.0809\u001b[0m        0.1337  0.8291\n",
            "     56        \u001b[36m0.0801\u001b[0m        0.1316  0.8195\n",
            "     57        \u001b[36m0.0791\u001b[0m        0.1243  0.8994\n",
            "     58        \u001b[36m0.0791\u001b[0m        0.1394  0.7445\n",
            "     59        \u001b[36m0.0788\u001b[0m        0.1293  0.6391\n",
            "     60        \u001b[36m0.0786\u001b[0m        0.1238  0.6398\n",
            "     61        \u001b[36m0.0775\u001b[0m        0.1383  0.6300\n",
            "     62        0.0775        0.1275  0.6395\n",
            "     63        \u001b[36m0.0774\u001b[0m        0.1285  0.6290\n",
            "     64        \u001b[36m0.0762\u001b[0m        0.1370  0.6365\n",
            "     65        \u001b[36m0.0759\u001b[0m        0.1293  0.6397\n",
            "     66        \u001b[36m0.0758\u001b[0m        0.1343  0.6276\n",
            "     67        \u001b[36m0.0752\u001b[0m        0.1390  0.6395\n",
            "     68        0.0757        0.1280  0.6360\n",
            "     69        0.0760        0.1465  0.6291\n",
            "     70        0.0762        0.1262  0.6296\n",
            "     71        0.0753        0.1461  0.6305\n",
            "     72        \u001b[36m0.0751\u001b[0m        0.1455  0.6435\n",
            "     73        0.0754        0.1330  0.6376\n",
            "     74        \u001b[36m0.0746\u001b[0m        0.1415  0.8111\n",
            "     75        \u001b[36m0.0746\u001b[0m        0.1453  0.8520\n",
            "     76        0.0751        0.1323  0.8323\n",
            "     77        \u001b[36m0.0735\u001b[0m        0.1407  0.8796\n",
            "     78        0.0740        0.1387  0.6901\n",
            "     79        0.0738        0.1378  0.6294\n",
            "     80        \u001b[36m0.0730\u001b[0m        0.1484  0.6387\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  56.0s\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.2031\u001b[0m        \u001b[32m0.2137\u001b[0m  0.6470\n",
            "      2        \u001b[36m0.1613\u001b[0m        0.2183  0.6283\n",
            "      3        \u001b[36m0.1564\u001b[0m        \u001b[32m0.2093\u001b[0m  0.6346\n",
            "      4        \u001b[36m0.1493\u001b[0m        \u001b[32m0.2062\u001b[0m  0.6270\n",
            "      5        \u001b[36m0.1408\u001b[0m        0.2115  0.6337\n",
            "      6        \u001b[36m0.1360\u001b[0m        0.2119  0.6378\n",
            "      7        \u001b[36m0.1340\u001b[0m        \u001b[32m0.1804\u001b[0m  0.6551\n",
            "      8        \u001b[36m0.1291\u001b[0m        \u001b[32m0.1799\u001b[0m  0.6327\n",
            "      9        \u001b[36m0.1261\u001b[0m        \u001b[32m0.1581\u001b[0m  0.6372\n",
            "     10        \u001b[36m0.1239\u001b[0m        0.2257  0.6299\n",
            "     11        0.1242        0.2210  0.6318\n",
            "     12        0.1241        0.1858  0.6298\n",
            "     13        \u001b[36m0.1227\u001b[0m        0.2059  0.7436\n",
            "     14        \u001b[36m0.1189\u001b[0m        0.1813  0.8350\n",
            "     15        \u001b[36m0.1150\u001b[0m        0.1839  0.8319\n",
            "     16        \u001b[36m0.1113\u001b[0m        \u001b[32m0.1314\u001b[0m  0.8908\n",
            "     17        \u001b[36m0.1093\u001b[0m        0.1337  0.7428\n",
            "     18        \u001b[36m0.1080\u001b[0m        0.1441  0.6356\n",
            "     19        0.1087        \u001b[32m0.1298\u001b[0m  0.6317\n",
            "     20        \u001b[36m0.1073\u001b[0m        \u001b[32m0.1277\u001b[0m  0.6319\n",
            "     21        \u001b[36m0.1051\u001b[0m        0.1318  0.6330\n",
            "     22        \u001b[36m0.1046\u001b[0m        0.1300  0.6238\n",
            "     23        0.1049        \u001b[32m0.1170\u001b[0m  0.6300\n",
            "     24        \u001b[36m0.1037\u001b[0m        0.1243  0.6268\n",
            "     25        \u001b[36m0.1032\u001b[0m        0.1364  0.6324\n",
            "     26        0.1056        0.1184  0.6374\n",
            "     27        0.1043        0.1178  0.6226\n",
            "     28        \u001b[36m0.1032\u001b[0m        0.1403  0.6336\n",
            "     29        0.1042        0.1248  0.6517\n",
            "     30        0.1047        \u001b[32m0.1096\u001b[0m  0.6309\n",
            "     31        \u001b[36m0.1028\u001b[0m        0.1472  0.6363\n",
            "     32        0.1060        0.1394  0.6362\n",
            "     33        0.1048        0.1122  0.8115\n",
            "     34        \u001b[36m0.1027\u001b[0m        0.1318  0.8439\n",
            "     35        0.1031        0.1320  0.8328\n",
            "     36        0.1029        0.1150  0.8907\n",
            "     37        \u001b[36m0.1018\u001b[0m        0.1202  0.6419\n",
            "     38        0.1021        0.1436  0.6372\n",
            "     39        0.1020        0.1223  0.6364\n",
            "     40        0.1020        0.1203  0.6279\n",
            "     41        \u001b[36m0.1017\u001b[0m        0.1562  0.6366\n",
            "     42        0.1033        0.1271  0.6389\n",
            "     43        0.1029        0.1240  0.6269\n",
            "     44        0.1019        0.1505  0.6357\n",
            "     45        0.1036        0.1273  0.6322\n",
            "     46        0.1027        0.1224  0.6410\n",
            "     47        0.1019        0.1415  0.6427\n",
            "     48        0.1030        0.1370  0.7264\n",
            "     49        0.1024        0.1225  0.7972\n",
            "     50        0.1019        0.1483  0.8144\n",
            "     51        0.1036        0.1230  0.8501\n",
            "     52        0.1020        0.1226  1.1685\n",
            "     53        \u001b[36m0.1001\u001b[0m        0.1391  1.1153\n",
            "     54        0.1012        0.1180  0.9003\n",
            "     55        0.1005        0.1355  0.7447\n",
            "     56        0.1002        0.1311  0.6313\n",
            "     57        0.1012        0.1282  0.6348\n",
            "     58        \u001b[36m0.0997\u001b[0m        0.1196  0.6393\n",
            "     59        0.1001        0.1339  0.6423\n",
            "     60        \u001b[36m0.0997\u001b[0m        0.1213  0.6347\n",
            "     61        \u001b[36m0.0992\u001b[0m        0.1205  0.6407\n",
            "     62        0.0992        0.1388  0.6435\n",
            "     63        0.1001        0.1184  0.6654\n",
            "     64        0.0995        0.1176  0.6387\n",
            "     65        \u001b[36m0.0991\u001b[0m        0.1397  0.6360\n",
            "     66        0.1018        0.1326  0.6300\n",
            "     67        \u001b[36m0.0962\u001b[0m        0.1607  0.6409\n",
            "     68        \u001b[36m0.0927\u001b[0m        0.1551  0.6370\n",
            "     69        \u001b[36m0.0895\u001b[0m        0.1225  0.6297\n",
            "     70        \u001b[36m0.0845\u001b[0m        0.1236  0.6375\n",
            "     71        \u001b[36m0.0824\u001b[0m        0.1262  0.8517\n",
            "     72        \u001b[36m0.0821\u001b[0m        \u001b[32m0.1092\u001b[0m  0.8499\n",
            "     73        \u001b[36m0.0806\u001b[0m        0.1381  0.8336\n",
            "     74        0.0824        0.1246  0.8754\n",
            "     75        0.0831        0.1119  0.6532\n",
            "     76        \u001b[36m0.0806\u001b[0m        0.1379  0.6297\n",
            "     77        0.0816        0.1170  0.6385\n",
            "     78        0.0816        0.1188  0.6434\n",
            "     79        0.0813        0.1409  0.6263\n",
            "     80        0.0827        0.1122  0.6407\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  55.9s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1820\u001b[0m        \u001b[32m0.2160\u001b[0m  0.6809\n",
            "      2        \u001b[36m0.1644\u001b[0m        0.2195  0.6698\n",
            "      3        \u001b[36m0.1623\u001b[0m        0.2213  0.6792\n",
            "      4        \u001b[36m0.1594\u001b[0m        \u001b[32m0.2153\u001b[0m  0.6787\n",
            "      5        \u001b[36m0.1517\u001b[0m        0.2298  0.6685\n",
            "      6        \u001b[36m0.1468\u001b[0m        0.2255  0.6795\n",
            "      7        \u001b[36m0.1399\u001b[0m        0.2227  0.6889\n",
            "      8        \u001b[36m0.1345\u001b[0m        0.2222  0.6749\n",
            "      9        \u001b[36m0.1293\u001b[0m        \u001b[32m0.1644\u001b[0m  0.7290\n",
            "     10        \u001b[36m0.1259\u001b[0m        0.1924  0.8881\n",
            "     11        \u001b[36m0.1254\u001b[0m        0.2309  0.9100\n",
            "     12        0.1264        0.1976  0.9480\n",
            "     13        \u001b[36m0.1252\u001b[0m        \u001b[32m0.1642\u001b[0m  0.8077\n",
            "     14        \u001b[36m0.1213\u001b[0m        0.2036  0.6682\n",
            "     15        0.1220        0.2001  0.6694\n",
            "     16        0.1231        0.1737  0.6808\n",
            "     17        \u001b[36m0.1187\u001b[0m        0.2091  0.6709\n",
            "     18        0.1209        0.2117  0.6768\n",
            "     19        0.1223        0.2053  0.6804\n",
            "     20        0.1218        0.2159  0.6699\n",
            "     21        \u001b[36m0.1186\u001b[0m        0.2076  0.6786\n",
            "     22        0.1205        0.2484  0.6855\n",
            "     23        \u001b[36m0.1177\u001b[0m        0.1915  0.6735\n",
            "     24        \u001b[36m0.1165\u001b[0m        0.1833  0.6792\n",
            "     25        0.1169        \u001b[32m0.1571\u001b[0m  0.6923\n",
            "     26        \u001b[36m0.1151\u001b[0m        0.2171  0.6717\n",
            "     27        \u001b[36m0.1141\u001b[0m        0.2212  0.6785\n",
            "     28        0.1143        0.1588  0.8769\n",
            "     29        \u001b[36m0.1129\u001b[0m        0.1656  0.9185\n",
            "     30        0.1131        0.1779  0.9029\n",
            "     31        0.1130        0.1977  0.9083\n",
            "     32        \u001b[36m0.1115\u001b[0m        0.1900  0.6810\n",
            "     33        \u001b[36m0.1112\u001b[0m        0.1767  0.6783\n",
            "     34        \u001b[36m0.1095\u001b[0m        \u001b[32m0.1403\u001b[0m  0.6716\n",
            "     35        \u001b[36m0.1066\u001b[0m        0.1737  0.6782\n",
            "     36        0.1102        0.2182  0.6843\n",
            "     37        0.1126        0.1773  0.6738\n",
            "     38        0.1096        0.1486  0.6840\n",
            "     39        0.1103        0.1816  0.6891\n",
            "     40        0.1102        0.2148  0.6765\n",
            "     41        0.1086        0.1706  0.6782\n",
            "     42        \u001b[36m0.1044\u001b[0m        0.1544  0.6797\n",
            "     43        0.1056        0.2267  0.6701\n",
            "     44        0.1103        0.1864  0.6788\n",
            "     45        \u001b[36m0.1039\u001b[0m        0.1460  0.6798\n",
            "     46        0.1058        0.1584  0.7917\n",
            "     47        0.1068        0.1922  0.9188\n",
            "     48        0.1043        \u001b[32m0.1363\u001b[0m  0.9022\n",
            "     49        \u001b[36m0.0972\u001b[0m        0.1556  0.9470\n",
            "     50        0.0999        0.1506  0.8217\n",
            "     51        0.1007        0.1684  0.8171\n",
            "     52        0.0993        \u001b[32m0.1347\u001b[0m  0.9029\n",
            "     53        \u001b[36m0.0967\u001b[0m        0.1608  0.8170\n",
            "     54        \u001b[36m0.0952\u001b[0m        0.1638  0.8947\n",
            "     55        \u001b[36m0.0941\u001b[0m        0.1578  0.8684\n",
            "     56        0.0962        0.1574  0.8379\n",
            "     57        \u001b[36m0.0933\u001b[0m        0.1588  0.6789\n",
            "     58        \u001b[36m0.0920\u001b[0m        0.1506  0.6717\n",
            "     59        \u001b[36m0.0910\u001b[0m        0.1681  0.6761\n",
            "     60        0.0927        0.1765  0.6955\n",
            "     61        \u001b[36m0.0903\u001b[0m        0.1705  0.6706\n",
            "     62        0.0908        0.1366  0.6898\n",
            "     63        \u001b[36m0.0897\u001b[0m        0.2035  0.8563\n",
            "     64        0.0935        0.1661  0.9331\n",
            "     65        0.0924        0.1464  0.9013\n",
            "     66        \u001b[36m0.0895\u001b[0m        0.1414  0.9425\n",
            "     67        \u001b[36m0.0877\u001b[0m        0.1675  0.7057\n",
            "     68        \u001b[36m0.0866\u001b[0m        0.1901  0.6791\n",
            "     69        0.0885        0.1596  0.6724\n",
            "     70        0.0880        0.1558  0.6855\n",
            "     71        0.0885        0.1393  0.6790\n",
            "     72        0.0871        0.1569  0.6733\n",
            "     73        0.0869        0.1791  0.6807\n",
            "     74        \u001b[36m0.0857\u001b[0m        0.1620  0.6812\n",
            "     75        \u001b[36m0.0855\u001b[0m        0.1843  0.6722\n",
            "     76        0.0869        0.1381  0.6829\n",
            "     77        0.0859        0.1691  0.6814\n",
            "     78        \u001b[36m0.0837\u001b[0m        0.1367  0.6754\n",
            "     79        0.0841        0.1521  0.6865\n",
            "     80        \u001b[36m0.0835\u001b[0m        0.1571  0.6800\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  59.5s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1623\u001b[0m        \u001b[32m0.2093\u001b[0m  0.8173\n",
            "      2        \u001b[36m0.1599\u001b[0m        0.2135  0.9114\n",
            "      3        0.1602        \u001b[32m0.2092\u001b[0m  0.8866\n",
            "      4        0.1622        \u001b[32m0.2075\u001b[0m  0.9369\n",
            "      5        0.1618        \u001b[32m0.2051\u001b[0m  0.7559\n",
            "      6        \u001b[36m0.1583\u001b[0m        0.2106  0.6782\n",
            "      7        \u001b[36m0.1497\u001b[0m        \u001b[32m0.2049\u001b[0m  0.6769\n",
            "      8        \u001b[36m0.1446\u001b[0m        0.2263  0.6775\n",
            "      9        \u001b[36m0.1377\u001b[0m        0.2160  0.6800\n",
            "     10        \u001b[36m0.1343\u001b[0m        0.2075  0.6695\n",
            "     11        \u001b[36m0.1316\u001b[0m        \u001b[32m0.1931\u001b[0m  0.6778\n",
            "     12        \u001b[36m0.1284\u001b[0m        \u001b[32m0.1839\u001b[0m  0.7025\n",
            "     13        \u001b[36m0.1268\u001b[0m        \u001b[32m0.1760\u001b[0m  0.6735\n",
            "     14        \u001b[36m0.1250\u001b[0m        \u001b[32m0.1741\u001b[0m  0.6792\n",
            "     15        \u001b[36m0.1229\u001b[0m        \u001b[32m0.1731\u001b[0m  0.6785\n",
            "     16        \u001b[36m0.1218\u001b[0m        \u001b[32m0.1654\u001b[0m  0.6712\n",
            "     17        \u001b[36m0.1200\u001b[0m        \u001b[32m0.1608\u001b[0m  0.6778\n",
            "     18        \u001b[36m0.1197\u001b[0m        \u001b[32m0.1534\u001b[0m  0.6843\n",
            "     19        \u001b[36m0.1180\u001b[0m        0.1648  0.6925\n",
            "     20        \u001b[36m0.1168\u001b[0m        \u001b[32m0.1476\u001b[0m  0.9040\n",
            "     21        0.1175        \u001b[32m0.1426\u001b[0m  0.9062\n",
            "     22        0.1179        0.1473  0.9213\n",
            "     23        0.1168        \u001b[32m0.1414\u001b[0m  0.8023\n",
            "     24        \u001b[36m0.1152\u001b[0m        0.1467  0.6884\n",
            "     25        \u001b[36m0.1148\u001b[0m        0.1530  0.6732\n",
            "     26        0.1155        \u001b[32m0.1368\u001b[0m  0.6778\n",
            "     27        \u001b[36m0.1144\u001b[0m        0.1530  0.6800\n",
            "     28        0.1152        0.1483  0.6718\n",
            "     29        0.1151        \u001b[32m0.1246\u001b[0m  0.6783\n",
            "     30        \u001b[36m0.1121\u001b[0m        0.1673  0.6765\n",
            "     31        0.1160        0.1296  0.6724\n",
            "     32        0.1139        0.1413  0.6774\n",
            "     33        0.1130        0.1303  0.6718\n",
            "     34        \u001b[36m0.1112\u001b[0m        0.1328  0.6774\n",
            "     35        \u001b[36m0.1105\u001b[0m        0.1374  0.6752\n",
            "     36        0.1111        0.1331  0.6711\n",
            "     37        0.1113        0.1355  0.6838\n",
            "     38        0.1111        0.1425  0.8964\n",
            "     39        0.1119        0.1355  0.9249\n",
            "     40        0.1113        0.1430  0.9164\n",
            "     41        0.1111        0.1447  0.8988\n",
            "     42        0.1110        0.1341  0.6635\n",
            "     43        \u001b[36m0.1098\u001b[0m        0.1468  0.6773\n",
            "     44        0.1113        0.1426  0.6774\n",
            "     45        0.1126        0.1285  0.6757\n",
            "     46        0.1103        0.1559  0.6793\n",
            "     47        0.1124        0.1385  0.6803\n",
            "     48        0.1120        0.1429  0.6830\n",
            "     49        0.1106        0.1326  0.6856\n",
            "     50        0.1100        0.1349  0.7538\n",
            "     51        \u001b[36m0.1091\u001b[0m        0.1405  0.8114\n",
            "     52        0.1097        0.1349  0.8613\n",
            "     53        \u001b[36m0.1087\u001b[0m        0.1341  0.8631\n",
            "     54        \u001b[36m0.1079\u001b[0m        0.1432  0.8994\n",
            "     55        0.1089        0.1366  1.1425\n",
            "     56        0.1085        0.1324  1.0086\n",
            "     57        \u001b[36m0.1078\u001b[0m        0.1479  0.9156\n",
            "     58        0.1095        0.1318  0.9161\n",
            "     59        0.1086        0.1431  0.6744\n",
            "     60        0.1081        0.1429  0.6784\n",
            "     61        0.1091        0.1322  0.6840\n",
            "     62        0.1079        0.1478  0.6736\n",
            "     63        0.1094        0.1403  0.6906\n",
            "     64        0.1105        0.1292  0.7058\n",
            "     65        0.1092        0.1633  0.6753\n",
            "     66        0.1097        0.1383  0.6774\n",
            "     67        0.1092        0.1307  0.6805\n",
            "     68        0.1084        0.1459  0.6657\n",
            "     69        0.1092        0.1422  0.6725\n",
            "     70        0.1098        0.1339  0.6783\n",
            "     71        \u001b[36m0.1074\u001b[0m        0.1383  0.6752\n",
            "     72        0.1077        0.1545  0.6816\n",
            "     73        \u001b[36m0.1073\u001b[0m        0.1438  0.7920\n",
            "     74        \u001b[36m0.1064\u001b[0m        0.1351  0.9089\n",
            "     75        \u001b[36m0.1062\u001b[0m        0.1565  0.9004\n",
            "     76        0.1075        0.1405  0.9681\n",
            "     77        0.1084        0.1346  0.6784\n",
            "     78        0.1069        0.1531  0.6806\n",
            "     79        0.1075        0.1326  0.6763\n",
            "     80        0.1070        0.1365  0.6689\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=32, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.0min\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.2020\u001b[0m        \u001b[32m0.2816\u001b[0m  0.6412\n",
            "      2        \u001b[36m0.1468\u001b[0m        \u001b[32m0.2714\u001b[0m  0.6357\n",
            "      3        0.1474        \u001b[32m0.2257\u001b[0m  0.6286\n",
            "      4        \u001b[36m0.1425\u001b[0m        \u001b[32m0.2004\u001b[0m  0.6373\n",
            "      5        \u001b[36m0.1294\u001b[0m        0.2460  0.6367\n",
            "      6        \u001b[36m0.1254\u001b[0m        0.2070  0.6359\n",
            "      7        0.1291        \u001b[32m0.1978\u001b[0m  0.6381\n",
            "      8        \u001b[36m0.1252\u001b[0m        0.2263  0.6394\n",
            "      9        \u001b[36m0.1217\u001b[0m        \u001b[32m0.1850\u001b[0m  0.6502\n",
            "     10        0.1243        \u001b[32m0.1832\u001b[0m  0.6387\n",
            "     11        0.1218        0.2087  0.6375\n",
            "     12        \u001b[36m0.1203\u001b[0m        0.1891  0.8282\n",
            "     13        0.1231        0.1863  0.8442\n",
            "     14        0.1206        \u001b[32m0.1824\u001b[0m  0.8838\n",
            "     15        \u001b[36m0.1174\u001b[0m        0.2079  0.8726\n",
            "     16        0.1189        \u001b[32m0.1713\u001b[0m  0.6812\n",
            "     17        0.1216        0.1823  0.6415\n",
            "     18        0.1184        0.1942  0.6334\n",
            "     19        \u001b[36m0.1163\u001b[0m        0.1843  0.6389\n",
            "     20        \u001b[36m0.1123\u001b[0m        0.1876  0.6402\n",
            "     21        0.1146        0.2100  0.6335\n",
            "     22        0.1162        0.2033  0.6406\n",
            "     23        0.1194        0.1804  0.6412\n",
            "     24        \u001b[36m0.1112\u001b[0m        0.1991  0.6315\n",
            "     25        0.1129        \u001b[32m0.1657\u001b[0m  0.6454\n",
            "     26        \u001b[36m0.1096\u001b[0m        0.1790  0.6362\n",
            "     27        0.1099        0.2018  0.6367\n",
            "     28        0.1142        0.2019  0.6443\n",
            "     29        0.1117        \u001b[32m0.1605\u001b[0m  0.6312\n",
            "     30        0.1165        \u001b[32m0.1375\u001b[0m  0.6413\n",
            "     31        0.1125        \u001b[32m0.1360\u001b[0m  0.6986\n",
            "     32        0.1125        \u001b[32m0.1297\u001b[0m  0.8233\n",
            "     33        \u001b[36m0.1069\u001b[0m        0.2232  0.8526\n",
            "     34        0.1128        0.1563  0.8621\n",
            "     35        0.1144        0.1612  0.7962\n",
            "     36        0.1071        0.1358  0.6352\n",
            "     37        \u001b[36m0.1033\u001b[0m        0.1577  0.6409\n",
            "     38        0.1084        0.1524  0.6409\n",
            "     39        0.1067        0.1413  0.6346\n",
            "     40        \u001b[36m0.1024\u001b[0m        0.1472  0.6375\n",
            "     41        \u001b[36m0.1003\u001b[0m        0.1541  0.6411\n",
            "     42        \u001b[36m0.0977\u001b[0m        \u001b[32m0.1293\u001b[0m  0.6345\n",
            "     43        \u001b[36m0.0963\u001b[0m        \u001b[32m0.1262\u001b[0m  0.6387\n",
            "     44        \u001b[36m0.0926\u001b[0m        \u001b[32m0.1219\u001b[0m  0.6325\n",
            "     45        \u001b[36m0.0907\u001b[0m        0.1274  0.6426\n",
            "     46        \u001b[36m0.0896\u001b[0m        0.1298  0.6425\n",
            "     47        \u001b[36m0.0886\u001b[0m        \u001b[32m0.1183\u001b[0m  0.6355\n",
            "     48        \u001b[36m0.0873\u001b[0m        0.1253  0.6396\n",
            "     49        \u001b[36m0.0872\u001b[0m        \u001b[32m0.1121\u001b[0m  0.6403\n",
            "     50        \u001b[36m0.0869\u001b[0m        0.1320  0.6342\n",
            "     51        0.0875        0.1348  0.8100\n",
            "     52        0.0910        \u001b[32m0.1104\u001b[0m  0.8631\n",
            "     53        0.0870        0.1311  0.8431\n",
            "     54        \u001b[36m0.0865\u001b[0m        0.1241  1.0456\n",
            "     55        0.0878        0.1313  0.9472\n",
            "     56        0.0906        0.1320  0.7759\n",
            "     57        0.0900        0.1215  0.8658\n",
            "     58        0.0899        \u001b[32m0.1090\u001b[0m  0.8740\n",
            "     59        \u001b[36m0.0857\u001b[0m        0.1259  0.8489\n",
            "     60        0.0866        0.1296  0.7036\n",
            "     61        \u001b[36m0.0848\u001b[0m        0.1248  0.6629\n",
            "     62        \u001b[36m0.0846\u001b[0m        0.1266  0.6501\n",
            "     63        0.0848        0.1382  0.6444\n",
            "     64        0.0854        0.1226  0.6568\n",
            "     65        \u001b[36m0.0843\u001b[0m        0.1440  0.6538\n",
            "     66        0.0852        0.1270  0.6444\n",
            "     67        0.0857        0.1359  0.6694\n",
            "     68        0.0864        0.1465  0.6788\n",
            "     69        0.0918        0.1155  0.8723\n",
            "     70        \u001b[36m0.0839\u001b[0m        0.1204  0.8738\n",
            "     71        \u001b[36m0.0826\u001b[0m        0.1101  0.8478\n",
            "     72        \u001b[36m0.0820\u001b[0m        0.1117  0.8955\n",
            "     73        \u001b[36m0.0816\u001b[0m        0.1154  0.6772\n",
            "     74        \u001b[36m0.0810\u001b[0m        0.1106  0.6510\n",
            "     75        0.0811        0.1123  0.6509\n",
            "     76        \u001b[36m0.0803\u001b[0m        0.1293  0.6557\n",
            "     77        0.0808        0.1200  0.6394\n",
            "     78        0.0813        0.1308  0.6446\n",
            "     79        0.0828        0.1106  0.6434\n",
            "     80        0.0813        0.1264  0.6456\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time=  56.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1515\u001b[0m        \u001b[32m0.2334\u001b[0m  0.6469\n",
            "      2        \u001b[36m0.1442\u001b[0m        0.2872  0.6542\n",
            "      3        0.1558        \u001b[32m0.2158\u001b[0m  0.6408\n",
            "      4        0.1469        \u001b[32m0.2045\u001b[0m  0.6438\n",
            "      5        \u001b[36m0.1342\u001b[0m        0.2147  0.6413\n",
            "      6        \u001b[36m0.1303\u001b[0m        0.2141  0.6466\n",
            "      7        \u001b[36m0.1301\u001b[0m        0.2089  0.6472\n",
            "      8        \u001b[36m0.1255\u001b[0m        0.2154  0.8106\n",
            "      9        \u001b[36m0.1237\u001b[0m        0.2139  0.8597\n",
            "     10        0.1241        \u001b[32m0.1970\u001b[0m  0.8382\n",
            "     11        0.1239        0.2128  0.9075\n",
            "     12        0.1250        0.2073  0.6537\n",
            "     13        \u001b[36m0.1221\u001b[0m        \u001b[32m0.1737\u001b[0m  0.6398\n",
            "     14        0.1246        0.1911  0.6529\n",
            "     15        \u001b[36m0.1177\u001b[0m        0.2145  0.6391\n",
            "     16        0.1212        0.2055  0.6490\n",
            "     17        0.1220        0.1956  0.6493\n",
            "     18        0.1210        0.1966  0.6396\n",
            "     19        0.1236        0.2191  0.6694\n",
            "     20        0.1234        0.2024  0.6515\n",
            "     21        0.1205        0.1970  0.6406\n",
            "     22        \u001b[36m0.1151\u001b[0m        0.2241  0.6481\n",
            "     23        0.1178        0.2022  0.6504\n",
            "     24        0.1179        0.2160  0.6437\n",
            "     25        0.1151        0.2560  0.6532\n",
            "     26        0.1255        0.2083  0.6421\n",
            "     27        \u001b[36m0.1134\u001b[0m        0.1917  0.7934\n",
            "     28        \u001b[36m0.1085\u001b[0m        0.2268  0.8462\n",
            "     29        \u001b[36m0.1084\u001b[0m        0.1983  0.8371\n",
            "     30        \u001b[36m0.1025\u001b[0m        0.1838  0.8969\n",
            "     31        \u001b[36m0.0996\u001b[0m        \u001b[32m0.1569\u001b[0m  0.6842\n",
            "     32        0.1001        0.1620  0.6500\n",
            "     33        \u001b[36m0.0981\u001b[0m        \u001b[32m0.1355\u001b[0m  0.6542\n",
            "     34        \u001b[36m0.0947\u001b[0m        \u001b[32m0.1304\u001b[0m  0.6355\n",
            "     35        0.0952        \u001b[32m0.1284\u001b[0m  0.6447\n",
            "     36        0.0973        0.1319  0.6496\n",
            "     37        \u001b[36m0.0927\u001b[0m        0.1501  0.6358\n",
            "     38        0.0935        0.1470  0.6451\n",
            "     39        0.1000        0.1395  0.6365\n",
            "     40        \u001b[36m0.0918\u001b[0m        0.1545  0.6480\n",
            "     41        0.0924        0.1628  0.6444\n",
            "     42        0.0998        0.1355  0.6389\n",
            "     43        \u001b[36m0.0915\u001b[0m        0.1335  0.6545\n",
            "     44        0.0916        0.1321  0.6492\n",
            "     45        \u001b[36m0.0896\u001b[0m        0.1299  0.6450\n",
            "     46        \u001b[36m0.0887\u001b[0m        0.1301  0.7462\n",
            "     47        0.0900        \u001b[32m0.1220\u001b[0m  0.8402\n",
            "     48        \u001b[36m0.0880\u001b[0m        0.1241  0.8411\n",
            "     49        \u001b[36m0.0871\u001b[0m        0.1272  0.8649\n",
            "     50        0.0883        \u001b[32m0.1190\u001b[0m  0.8265\n",
            "     51        \u001b[36m0.0870\u001b[0m        0.1225  0.6516\n",
            "     52        \u001b[36m0.0863\u001b[0m        0.1254  0.6295\n",
            "     53        0.0877        0.1237  0.6446\n",
            "     54        0.0866        0.1204  0.6509\n",
            "     55        \u001b[36m0.0860\u001b[0m        0.1263  0.6459\n",
            "     56        \u001b[36m0.0858\u001b[0m        0.1245  0.6550\n",
            "     57        0.0878        0.1201  0.7057\n",
            "     58        0.0864        0.1262  0.7639\n",
            "     59        \u001b[36m0.0850\u001b[0m        0.1241  0.8390\n",
            "     60        0.0879        \u001b[32m0.1189\u001b[0m  0.7863\n",
            "     61        0.0860        0.1301  0.8604\n",
            "     62        0.0850        0.1223  0.8405\n",
            "     63        0.0879        \u001b[32m0.1189\u001b[0m  0.8255\n",
            "     64        0.0857        0.1304  0.8132\n",
            "     65        0.0850        0.1286  0.8545\n",
            "     66        0.0879        \u001b[32m0.1141\u001b[0m  0.8393\n",
            "     67        0.0853        0.1392  0.8853\n",
            "     68        \u001b[36m0.0847\u001b[0m        0.1305  0.7701\n",
            "     69        0.0882        \u001b[32m0.1141\u001b[0m  0.6497\n",
            "     70        0.0858        0.1482  0.6502\n",
            "     71        0.0863        0.1378  0.6412\n",
            "     72        0.0904        \u001b[32m0.1067\u001b[0m  0.6624\n",
            "     73        0.0848        0.1496  0.6572\n",
            "     74        0.0860        0.1329  0.6448\n",
            "     75        0.0879        0.1086  0.6561\n",
            "     76        \u001b[36m0.0844\u001b[0m        0.1285  0.6549\n",
            "     77        \u001b[36m0.0834\u001b[0m        0.1317  0.6407\n",
            "     78        0.0864        0.1122  0.6501\n",
            "     79        0.0840        0.1210  0.6419\n",
            "     80        \u001b[36m0.0827\u001b[0m        0.1252  0.6503\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=1, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time=  56.7s\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1745\u001b[0m        \u001b[32m0.2345\u001b[0m  0.7427\n",
            "      2        \u001b[36m0.1588\u001b[0m        \u001b[32m0.2095\u001b[0m  0.7476\n",
            "      3        0.1595        \u001b[32m0.2057\u001b[0m  0.9100\n",
            "      4        \u001b[36m0.1577\u001b[0m        0.2062  0.9705\n",
            "      5        0.1593        0.2068  0.9488\n",
            "      6        \u001b[36m0.1552\u001b[0m        0.2083  0.9709\n",
            "      7        \u001b[36m0.1517\u001b[0m        \u001b[32m0.2004\u001b[0m  0.7489\n",
            "      8        \u001b[36m0.1460\u001b[0m        0.2005  0.7389\n",
            "      9        \u001b[36m0.1426\u001b[0m        0.2069  0.7430\n",
            "     10        \u001b[36m0.1362\u001b[0m        0.2035  0.7384\n",
            "     11        \u001b[36m0.1311\u001b[0m        \u001b[32m0.1859\u001b[0m  0.7421\n",
            "     12        \u001b[36m0.1270\u001b[0m        \u001b[32m0.1388\u001b[0m  0.7421\n",
            "     13        \u001b[36m0.1228\u001b[0m        \u001b[32m0.1268\u001b[0m  0.7400\n",
            "     14        \u001b[36m0.1202\u001b[0m        0.1363  0.7402\n",
            "     15        \u001b[36m0.1184\u001b[0m        0.1415  0.7579\n",
            "     16        0.1189        \u001b[32m0.1230\u001b[0m  0.7408\n",
            "     17        \u001b[36m0.1164\u001b[0m        0.1420  0.7431\n",
            "     18        0.1182        0.1317  0.7420\n",
            "     19        0.1205        0.1274  0.7481\n",
            "     20        \u001b[36m0.1163\u001b[0m        0.1337  0.8948\n",
            "     21        \u001b[36m0.1142\u001b[0m        0.1378  0.9650\n",
            "     22        \u001b[36m0.1135\u001b[0m        0.1254  0.9607\n",
            "     23        0.1136        0.1240  0.9872\n",
            "     24        \u001b[36m0.1121\u001b[0m        0.1339  0.7508\n",
            "     25        \u001b[36m0.1118\u001b[0m        0.1272  0.7443\n",
            "     26        \u001b[36m0.1097\u001b[0m        0.1280  0.7473\n",
            "     27        \u001b[36m0.1091\u001b[0m        0.1316  0.7348\n",
            "     28        0.1105        0.1364  0.7416\n",
            "     29        0.1108        0.1413  0.7492\n",
            "     30        0.1104        0.1288  0.7439\n",
            "     31        \u001b[36m0.1076\u001b[0m        0.1496  0.7375\n",
            "     32        0.1077        0.1396  0.7477\n",
            "     33        \u001b[36m0.1068\u001b[0m        0.1414  0.7422\n",
            "     34        0.1076        0.1435  0.7420\n",
            "     35        0.1076        0.1446  0.7447\n",
            "     36        0.1077        0.1374  0.7502\n",
            "     37        \u001b[36m0.1055\u001b[0m        0.1508  0.8728\n",
            "     38        0.1065        0.1238  0.9507\n",
            "     39        0.1078        0.1590  0.9402\n",
            "     40        0.1089        0.1256  1.0017\n",
            "     41        \u001b[36m0.1044\u001b[0m        0.1587  0.7664\n",
            "     42        0.1074        0.1535  0.7440\n",
            "     43        0.1067        0.1534  0.7416\n",
            "     44        \u001b[36m0.1039\u001b[0m        0.1782  0.7350\n",
            "     45        0.1063        0.1463  0.7427\n",
            "     46        0.1067        0.1570  0.7460\n",
            "     47        \u001b[36m0.1023\u001b[0m        0.1397  0.7489\n",
            "     48        0.1051        0.1482  0.7401\n",
            "     49        0.1029        0.1346  0.7382\n",
            "     50        0.1024        0.1578  0.7305\n",
            "     51        \u001b[36m0.1022\u001b[0m        0.1319  0.7362\n",
            "     52        0.1026        0.1602  0.7292\n",
            "     53        0.1058        0.1642  0.7484\n",
            "     54        \u001b[36m0.1019\u001b[0m        0.1914  0.8659\n",
            "     55        0.1046        0.1555  1.1545\n",
            "     56        0.1045        0.1544  1.2335\n",
            "     57        0.1028        0.1477  1.3091\n",
            "     58        \u001b[36m0.1016\u001b[0m        0.1540  0.9589\n",
            "     59        0.1040        0.1615  0.9224\n",
            "     60        \u001b[36m0.0983\u001b[0m        0.1716  0.7438\n",
            "     61        0.1023        0.2307  0.7486\n",
            "     62        0.1017        0.1936  0.7331\n",
            "     63        \u001b[36m0.0965\u001b[0m        0.2065  0.7453\n",
            "     64        0.1089        0.1685  0.7421\n",
            "     65        0.1066        0.1963  0.7405\n",
            "     66        0.1085        0.1331  0.7501\n",
            "     67        0.1003        0.1412  0.7518\n",
            "     68        \u001b[36m0.0958\u001b[0m        0.1667  0.7645\n",
            "     69        \u001b[36m0.0932\u001b[0m        0.1503  0.7373\n",
            "     70        \u001b[36m0.0909\u001b[0m        0.1619  0.8002\n",
            "     71        \u001b[36m0.0879\u001b[0m        0.1872  0.9630\n",
            "     72        0.0957        0.1440  0.9588\n",
            "     73        0.0910        0.1724  0.9912\n",
            "     74        0.0916        0.1450  0.8397\n",
            "     75        0.0882        0.1745  0.7445\n",
            "     76        \u001b[36m0.0865\u001b[0m        0.1553  0.7386\n",
            "     77        0.0882        0.1870  0.7476\n",
            "     78        0.0886        0.1548  0.7464\n",
            "     79        0.0881        0.2481  0.7492\n",
            "     80        0.0937        0.1567  0.7373\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.0001; total time= 1.1min\n",
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.1696\u001b[0m        \u001b[32m0.2199\u001b[0m  0.7439\n",
            "      2        \u001b[36m0.1543\u001b[0m        \u001b[32m0.2146\u001b[0m  0.7425\n",
            "      3        0.1579        \u001b[32m0.2104\u001b[0m  0.7443\n",
            "      4        0.1599        0.2130  0.7370\n",
            "      5        0.1590        0.2116  0.7429\n",
            "      6        0.1587        0.2115  0.7431\n",
            "      7        0.1577        \u001b[32m0.2084\u001b[0m  0.8783\n",
            "      8        0.1552        \u001b[32m0.2072\u001b[0m  0.9740\n",
            "      9        \u001b[36m0.1509\u001b[0m        0.2074  0.9380\n",
            "     10        \u001b[36m0.1479\u001b[0m        \u001b[32m0.2067\u001b[0m  0.9883\n",
            "     11        \u001b[36m0.1464\u001b[0m        \u001b[32m0.2067\u001b[0m  0.7408\n",
            "     12        \u001b[36m0.1420\u001b[0m        0.2177  0.7481\n",
            "     13        \u001b[36m0.1377\u001b[0m        0.2104  0.7465\n",
            "     14        \u001b[36m0.1364\u001b[0m        \u001b[32m0.1940\u001b[0m  0.7388\n",
            "     15        \u001b[36m0.1358\u001b[0m        0.1967  0.7619\n",
            "     16        \u001b[36m0.1320\u001b[0m        \u001b[32m0.1895\u001b[0m  0.7481\n",
            "     17        \u001b[36m0.1296\u001b[0m        \u001b[32m0.1785\u001b[0m  0.7475\n",
            "     18        \u001b[36m0.1278\u001b[0m        0.1866  0.7444\n",
            "     19        \u001b[36m0.1271\u001b[0m        0.1971  0.7432\n",
            "     20        \u001b[36m0.1260\u001b[0m        0.1792  0.7406\n",
            "     21        \u001b[36m0.1259\u001b[0m        \u001b[32m0.1778\u001b[0m  0.7261\n",
            "     22        \u001b[36m0.1244\u001b[0m        0.2042  0.7502\n",
            "     23        \u001b[36m0.1233\u001b[0m        0.1833  0.7453\n",
            "     24        0.1254        \u001b[32m0.1565\u001b[0m  0.8887\n",
            "     25        0.1234        0.1676  0.9526\n",
            "     26        \u001b[36m0.1194\u001b[0m        0.1736  0.9561\n",
            "     27        0.1195        \u001b[32m0.1354\u001b[0m  0.9243\n",
            "     28        \u001b[36m0.1187\u001b[0m        0.1430  0.7461\n",
            "     29        \u001b[36m0.1171\u001b[0m        0.1449  0.7458\n",
            "     30        \u001b[36m0.1167\u001b[0m        0.1393  0.7463\n",
            "     31        \u001b[36m0.1166\u001b[0m        0.1510  0.7399\n",
            "     32        0.1197        0.1354  0.7601\n",
            "     33        \u001b[36m0.1155\u001b[0m        0.1419  0.7510\n",
            "     34        0.1175        0.1671  0.7401\n",
            "     35        0.1180        0.1703  0.7366\n",
            "     36        0.1217        0.1423  0.7466\n",
            "     37        0.1161        0.1527  0.7444\n",
            "     38        \u001b[36m0.1143\u001b[0m        0.1437  0.7401\n",
            "     39        0.1149        \u001b[32m0.1332\u001b[0m  0.7366\n",
            "     40        \u001b[36m0.1138\u001b[0m        0.1550  0.7496\n",
            "     41        0.1138        0.1404  0.9584\n",
            "     42        0.1138        \u001b[32m0.1324\u001b[0m  0.9568\n",
            "     43        \u001b[36m0.1124\u001b[0m        0.1581  0.9690\n",
            "     44        0.1130        0.1434  0.9115\n",
            "     45        0.1130        0.1404  0.7450\n",
            "     46        \u001b[36m0.1106\u001b[0m        0.1516  0.7538\n",
            "     47        0.1123        0.1347  0.7476\n",
            "     48        0.1114        0.1428  0.7507\n",
            "     49        \u001b[36m0.1104\u001b[0m        0.1464  0.8812\n",
            "     50        0.1129        0.1341  0.9529\n",
            "     51        0.1113        0.1372  0.8903\n",
            "     52        0.1121        0.1467  0.9723\n",
            "     53        0.1115        0.1406  0.9423\n",
            "     54        0.1124        0.1519  0.8827\n",
            "     55        0.1117        0.1363  0.7481\n",
            "     56        0.1111        0.1395  0.8287\n",
            "     57        \u001b[36m0.1095\u001b[0m        0.1511  0.9462\n",
            "     58        0.1110        \u001b[32m0.1295\u001b[0m  0.9510\n",
            "     59        0.1103        0.1389  0.9882\n",
            "     60        0.1101        0.1454  0.8439\n",
            "     61        0.1115        0.1337  0.7523\n",
            "     62        0.1106        0.1440  0.7465\n",
            "     63        0.1108        0.1452  0.7459\n",
            "     64        0.1118        0.1475  0.7496\n",
            "     65        0.1099        0.1798  0.7447\n",
            "     66        0.1112        0.1501  0.7407\n",
            "     67        0.1123        0.1379  0.7453\n",
            "     68        0.1115        0.1430  0.7474\n",
            "     69        0.1103        0.1355  0.7424\n",
            "     70        \u001b[36m0.1083\u001b[0m        \u001b[32m0.1218\u001b[0m  0.7466\n",
            "     71        0.1084        0.1416  0.7448\n",
            "     72        0.1093        0.1357  0.7450\n",
            "     73        0.1100        0.1312  0.8165\n",
            "     74        0.1100        0.1454  0.9555\n",
            "     75        0.1094        0.1321  0.9498\n",
            "     76        0.1091        0.1301  0.9963\n",
            "     77        0.1083        0.1424  0.8235\n",
            "     78        0.1111        0.1415  0.7350\n",
            "     79        0.1105        0.1419  0.7406\n",
            "     80        0.1115        0.1571  0.7303\n",
            "[CV] END batch_size=64, iterator_train__shuffle=False, max_epochs=80, module__drop_prob=0.5, module__hidden_dim=64, module__n_layers=2, optimizer__lr=0.001, optimizer__weight_decay=0.001; total time= 1.1min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
              "  module=<class 'GRU_model.GRUNet'>,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={'batch_size': [16, 32, 64],\n",
              "                         'iterator_train__shuffle': [False], 'max_epochs': [80],\n",
              "                         'module__drop_prob': [0.5],\n",
              "                         'module__hidden_dim': [16, 32, 64],\n",
              "                         'module__n_layers': [1, 2], 'optimizer__lr': [0.001],\n",
              "                         'optimizer__weight_decay': [0.0001, 0.001]},\n",
              "             refit=False, scoring='neg_mean_squared_error', verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={&#x27;batch_size&#x27;: [16, 32, 64],\n",
              "                         &#x27;iterator_train__shuffle&#x27;: [False], &#x27;max_epochs&#x27;: [80],\n",
              "                         &#x27;module__drop_prob&#x27;: [0.5],\n",
              "                         &#x27;module__hidden_dim&#x27;: [16, 32, 64],\n",
              "                         &#x27;module__n_layers&#x27;: [1, 2], &#x27;optimizer__lr&#x27;: [0.001],\n",
              "                         &#x27;optimizer__weight_decay&#x27;: [0.0001, 0.001]},\n",
              "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             estimator=&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              "),\n",
              "             param_grid={&#x27;batch_size&#x27;: [16, 32, 64],\n",
              "                         &#x27;iterator_train__shuffle&#x27;: [False], &#x27;max_epochs&#x27;: [80],\n",
              "                         &#x27;module__drop_prob&#x27;: [0.5],\n",
              "                         &#x27;module__hidden_dim&#x27;: [16, 32, 64],\n",
              "                         &#x27;module__n_layers&#x27;: [1, 2], &#x27;optimizer__lr&#x27;: [0.001],\n",
              "                         &#x27;optimizer__weight_decay&#x27;: [0.0001, 0.001]},\n",
              "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NeuralNetRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NeuralNetRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.regressor.NeuralNetRegressor&#x27;&gt;[uninitialized](\n",
              "  module=&lt;class &#x27;GRU_model.GRUNet&#x27;&gt;,\n",
              "  module__input_dim=11,\n",
              "  module__output_dim=1,\n",
              ")</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters_wind_24 = gs_wind_24.best_params_\n",
        "best_score_wind_24 = gs_wind_24.best_score_\n",
        "print(\"Best parameters:\", best_parameters_wind_24)\n",
        "print(\"Best score (negative MSE):\", best_score_wind_24)"
      ],
      "metadata": {
        "id": "CUymeWw5Xa3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6444072f-4abb-43df-9ae0-1cf262b7a5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'batch_size': 64, 'iterator_train__shuffle': False, 'max_epochs': 80, 'module__drop_prob': 0.5, 'module__hidden_dim': 16, 'module__n_layers': 1, 'optimizer__lr': 0.001, 'optimizer__weight_decay': 0.001}\n",
            "Best score (negative MSE): -0.09689153730869293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gs_results = pd.DataFrame(gs_wind_24.cv_results_)\n",
        "gs_results\n",
        "gs_results.to_csv('/content/drive/MyDrive/solar_data/gird_search_results_wind_24.csv')"
      ],
      "metadata": {
        "id": "wxo_WE84XgF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}